{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZJaM-35yxb8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a25f325-a758-41f6-c156-e3ddba7dfa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PkgvaFRu-rD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMmCQDIDtEKk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I6y5vBMAsBx7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report, precision_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_VS49msUjYNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd58fb0-8d84-487f-ca69-6e3056971ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZALYW7Z0tJvU"
      },
      "outputs": [],
      "source": [
        "## paths where datasets are located\n",
        "enron1_train_ham_path = \"//content//drive//MyDrive//project1_datasets//enron1_train//train//ham\"\n",
        "enron1_train_spam_path = \"//content//drive//MyDrive//project1_datasets//enron1_train//train//spam\"\n",
        "enron1_test_ham_path = \"//content//drive//MyDrive//project1_datasets//enron1_test//test//ham\"\n",
        "enron1_test_spam_path = \"//content//drive//MyDrive//project1_datasets//enron1_test//test//spam\"\n",
        "\n",
        "enron2_train_ham_path = \"//content//drive//MyDrive//project1_datasets//enron2_train//train//ham\"\n",
        "enron2_train_spam_path = \"//content//drive//MyDrive//project1_datasets//enron2_train//train//spam\"\n",
        "enron2_test_ham_path = \"//content//drive//MyDrive//project1_datasets//enron2_test//test//ham\"\n",
        "enron2_test_spam_path = \"//content//drive//MyDrive//project1_datasets//enron2_test//test//spam\"\n",
        "\n",
        "enron4_train_ham_path = \"//content//drive//MyDrive//project1_datasets//enron4_train//train//ham\"\n",
        "enron4_train_spam_path = \"//content//drive//MyDrive//project1_datasets//enron4_train//train//spam\"\n",
        "enron4_test_ham_path = \"//content//drive//MyDrive//project1_datasets//enron4_test//test//ham\"\n",
        "enron4_test_spam_path = \"//content//drive//MyDrive//project1_datasets//enron4_test//test//spam\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX611S6dtba4"
      },
      "source": [
        "## Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PmTgMjTltbAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e457d06e-2a26-4107-83a3-aae70e614004"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2925"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "all_data = []\n",
        "\n",
        "def create_dataset(path):\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            with open(path+\"//\"+filename, \"rb\") as f:\n",
        "                all_data.append([f.read(), path.split(\"//\")[-1]])\n",
        "\n",
        "paths = [enron1_train_ham_path, enron1_train_spam_path, enron1_test_ham_path, enron1_test_spam_path, enron2_train_ham_path, enron2_train_spam_path, enron2_test_ham_path, enron2_test_spam_path, enron4_train_ham_path, enron4_train_spam_path, enron4_test_ham_path, enron4_test_spam_path]\n",
        "\n",
        "for path in paths:\n",
        "    create_dataset(path)\n",
        "\n",
        "len(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f5opri2mtiQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e417d913-b8b2-4064-aefd-8349f714a59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "319 131\n",
            "340 123\n",
            "133 402\n",
            "307 149\n",
            "348 130\n",
            "152 391\n"
          ]
        }
      ],
      "source": [
        "# creating each enron dataset\n",
        "def create_individual_dataset(path, dataset):\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            with open(path+\"//\"+filename, \"rb\") as f:\n",
        "                dataset.append([f.read(), path.split(\"//\")[-1]])\n",
        "    return dataset\n",
        "\n",
        "\"\"\"For training\"\"\"\n",
        "enron1_train_ham_df = []\n",
        "enron1_train_ham_df = create_individual_dataset(enron1_train_ham_path, enron1_train_ham_df)\n",
        "enron1_train_spam_df = []\n",
        "enron1_train_spam_df = create_individual_dataset(enron1_train_spam_path, enron1_train_spam_df)\n",
        "\n",
        "print(len(enron1_train_ham_df), len(enron1_train_spam_df))\n",
        "\n",
        "enron2_train_ham_df = []\n",
        "enron2_train_ham_df = create_individual_dataset(enron2_train_ham_path, enron2_train_ham_df)\n",
        "enron2_train_spam_df = []\n",
        "enron2_train_spam_df = create_individual_dataset(enron2_train_spam_path, enron2_train_spam_df)\n",
        "\n",
        "print(len(enron2_train_ham_df), len(enron2_train_spam_df))\n",
        "\n",
        "enron4_train_ham_df = []\n",
        "enron4_train_ham_df = create_individual_dataset(enron4_train_ham_path, enron4_train_ham_df)\n",
        "enron4_train_spam_df = []\n",
        "enron4_train_spam_df = create_individual_dataset(enron4_train_spam_path, enron4_train_spam_df)\n",
        "\n",
        "print(len(enron4_train_ham_df), len(enron4_train_spam_df))\n",
        "\n",
        "\"\"\"For testing\"\"\"\n",
        "enron1_test_ham_df = []\n",
        "enron1_test_ham_df = create_individual_dataset(enron1_test_ham_path, enron1_test_ham_df)\n",
        "enron1_test_spam_df = []\n",
        "enron1_test_spam_df = create_individual_dataset(enron1_test_spam_path, enron1_test_spam_df)\n",
        "\n",
        "print(len(enron1_test_ham_df), len(enron1_test_spam_df))\n",
        "\n",
        "enron2_test_ham_df = []\n",
        "enron2_test_ham_df = create_individual_dataset(enron2_test_ham_path, enron2_test_ham_df)\n",
        "enron2_test_spam_df = []\n",
        "enron2_test_spam_df = create_individual_dataset(enron2_test_spam_path, enron2_test_spam_df)\n",
        "\n",
        "print(len(enron2_test_ham_df), len(enron2_test_spam_df))\n",
        "\n",
        "enron4_test_ham_df = []\n",
        "enron4_test_ham_df = create_individual_dataset(enron4_test_ham_path, enron4_test_ham_df)\n",
        "enron4_test_spam_df = []\n",
        "enron4_test_spam_df = create_individual_dataset(enron4_test_spam_path, enron4_test_spam_df)\n",
        "\n",
        "print(len(enron4_test_ham_df), len(enron4_test_spam_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IMYZGxyOuejA"
      },
      "outputs": [],
      "source": [
        "\"\"\"Converting string to byte string\"\"\"\n",
        "\n",
        "def convert_to_str(data):\n",
        "    converted_data = []\n",
        "    for item in data:\n",
        "        if isinstance(item[0], bytes):\n",
        "            # If the item is in bytes, decode it to a string\n",
        "            converted_data.append([item[0].decode('utf-8', errors='ignore'), item[1]])\n",
        "        else:\n",
        "            # If the item is already a string, just append it\n",
        "                 converted_data.append([item[0], item[1]])\n",
        "    return converted_data\n",
        "\n",
        "\n",
        "all_data = convert_to_str(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A1XXWT8Lukij"
      },
      "outputs": [],
      "source": [
        "enron1_train_ham_df = convert_to_str(enron1_train_ham_df)\n",
        "enron1_train_spam_df = convert_to_str(enron1_train_spam_df)\n",
        "enron2_train_ham_df = convert_to_str(enron2_train_ham_df)\n",
        "enron2_train_spam_df = convert_to_str(enron2_train_spam_df)\n",
        "enron4_train_ham_df = convert_to_str(enron4_train_ham_df)\n",
        "enron4_train_spam_df = convert_to_str(enron4_train_spam_df)\n",
        "\n",
        "enron1_test_ham_df = convert_to_str(enron1_test_ham_df)\n",
        "enron1_test_spam_df = convert_to_str(enron1_test_spam_df)\n",
        "enron2_test_ham_df = convert_to_str(enron2_test_ham_df)\n",
        "enron2_test_spam_df = convert_to_str(enron2_test_spam_df)\n",
        "enron4_test_ham_df = convert_to_str(enron4_test_ham_df)\n",
        "enron4_test_spam_df = convert_to_str(enron4_test_spam_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RC_Yss6HumPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76667a76-48f2-42f3-b75d-73f94945d678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "450\n",
            "463\n",
            "535\n",
            "456\n",
            "478\n",
            "543\n"
          ]
        }
      ],
      "source": [
        "enron1_train_spam_df.extend(enron1_train_ham_df)\n",
        "enron1_train = enron1_train_spam_df\n",
        "print(len(enron1_train))\n",
        "\n",
        "enron2_train_spam_df.extend(enron2_train_ham_df)\n",
        "enron2_train = enron2_train_spam_df\n",
        "print(len(enron2_train))\n",
        "\n",
        "enron4_train_spam_df.extend(enron4_train_ham_df)\n",
        "enron4_train = enron4_train_spam_df\n",
        "print(len(enron4_train))\n",
        "\n",
        "enron1_test_spam_df.extend(enron1_test_ham_df)\n",
        "enron1_test = enron1_test_spam_df\n",
        "print(len(enron1_test))\n",
        "\n",
        "enron2_test_spam_df.extend(enron2_test_ham_df)\n",
        "enron2_test = enron2_test_spam_df\n",
        "print(len(enron2_test))\n",
        "\n",
        "enron4_test_spam_df.extend(enron4_test_ham_df)\n",
        "enron4_test = enron4_test_spam_df\n",
        "print(len(enron4_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBZFwRrkux8C"
      },
      "source": [
        "## Convert list to Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FXN-9jJ-uv46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d88179-6f9c-49ef-8dcd-d290d50b23c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2925, 2)\n",
            "(450, 2)\n",
            "(463, 2)\n",
            "(535, 2)\n",
            "(456, 2)\n",
            "(478, 2)\n",
            "(543, 2)\n"
          ]
        }
      ],
      "source": [
        "def convert_to_df(data):\n",
        "    df = pd.DataFrame(data, columns=['Email', 'Type'])\n",
        "    print(df.shape)\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "all_data_df = convert_to_df(all_data)\n",
        "\n",
        "enron1_train_df = convert_to_df(enron1_train)\n",
        "enron2_train_df = convert_to_df(enron2_train)\n",
        "enron4_train_df = convert_to_df(enron4_train)\n",
        "\n",
        "enron1_test_df = convert_to_df(enron1_test)\n",
        "enron2_test_df = convert_to_df(enron2_test)\n",
        "enron4_test_df = convert_to_df(enron4_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GEV3XJjFidh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebbb642-6f5c-49b4-aee7-c258dccfec95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 2)\n",
            "(463, 2)\n",
            "(535, 2)\n",
            "(456, 2)\n",
            "(478, 2)\n",
            "(543, 2)\n"
          ]
        }
      ],
      "source": [
        "copy_of_enron1_train_df = convert_to_df(enron1_train)\n",
        "copy_of_enron2_train_df = convert_to_df(enron2_train)\n",
        "copy_of_enron4_train_df = convert_to_df(enron4_train)\n",
        "\n",
        "copy_of_enron1_test_df = convert_to_df(enron1_test)\n",
        "copy_of_enron2_test_df = convert_to_df(enron2_test)\n",
        "copy_of_enron4_test_df = convert_to_df(enron4_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9kyv7l1u3jD"
      },
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "For preprocessing the data, I have performed the following operations -\n",
        "\n",
        "- Converting text to lower case\n",
        "\n",
        "- Rmoving punctuations\n",
        "\n",
        "- Creating word tokens\n",
        "\n",
        "- Removing stop words\n",
        "\n",
        "- Lemmatization\n",
        "\n",
        "- Removing non alphabetic character\n",
        "\n",
        "**For this project I have used the Natural Language Toolkit (NLTK) library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4m0WcXYJu1FY"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    # conver to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove punctuations\n",
        "    text = text.translate(str.maketrans('','', string.punctuation))\n",
        "\n",
        "    # tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # stop word removal\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # lemmatization\n",
        "    lemma = WordNetLemmatizer()\n",
        "    filtered = [lemma.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # remove non-alphabetic characters\n",
        "    filtered = [word for word in filtered if word.isalpha()]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "## Applying the function to Email column\n",
        "all_data_df['Email_tok'] = all_data_df['Email'].apply(preprocess)\n",
        "\n",
        "enron1_train_df['Email_tok'] = enron1_train_df['Email'].apply(preprocess)\n",
        "enron2_train_df['Email_tok'] = enron2_train_df['Email'].apply(preprocess)\n",
        "enron4_train_df['Email_tok'] = enron4_train_df['Email'].apply(preprocess)\n",
        "\n",
        "enron1_test_df['Email_tok'] = enron1_test_df['Email'].apply(preprocess)\n",
        "enron2_test_df['Email_tok'] = enron2_test_df['Email'].apply(preprocess)\n",
        "enron4_test_df['Email_tok'] = enron4_test_df['Email'].apply(preprocess)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1FyyTDVSu8Dr"
      },
      "outputs": [],
      "source": [
        "all_data_df['Email_str'] = all_data_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "enron1_train_df['Email_str'] = enron1_train_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "enron2_train_df['Email_str'] = enron2_train_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "enron4_train_df['Email_str'] = enron4_train_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "enron1_test_df['Email_str'] = enron1_test_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "enron2_test_df['Email_str'] = enron2_test_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "enron4_test_df['Email_str'] = enron4_test_df['Email_tok'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "37FkcZOAvJkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "2d2da35e-ed8e-4563-c8c0-6bca9f642903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Email  Type  \\\n",
              "0    Subject: enron actuals for july 14 thru 16 , 2...   ham   \n",
              "1    Subject: fw : pymt for 03 / 01 sales adjustmen...   ham   \n",
              "2    Subject: re : automate load of scheduled quant...   ham   \n",
              "3    Subject: i congratulate you , my friend , with...  spam   \n",
              "4    Subject: re : hi suzette\\r\\nthe secret on how ...  spam   \n",
              "..                                                 ...   ...   \n",
              "445  Subject: proccess credit cards online , hispan...  spam   \n",
              "446  Subject: re : volume increase - hpl meter 68 -...   ham   \n",
              "447  Subject: viewsonic airpanel vl 50 15 - inch sm...  spam   \n",
              "448             Subject: private vl @ gra\\r\\nremove me  spam   \n",
              "449  Subject: cornhusker - lone star payments\\r\\nse...   ham   \n",
              "\n",
              "                                             Email_tok  \\\n",
              "0    [subject, enron, actuals, for, july, 14, thru,...   \n",
              "1    [subject, fw, pymt, for, 03, 01, sales, adjust...   \n",
              "2    [subject, re, automate, load, of, scheduled, q...   \n",
              "3    [subject, i, congratulate, you, my, friend, wi...   \n",
              "4    [subject, re, hi, suzette, the, secret, on, ho...   \n",
              "..                                                 ...   \n",
              "445  [subject, proccess, credit, cards, online, his...   \n",
              "446  [subject, re, volume, increase, hpl, meter, 68...   \n",
              "447  [subject, viewsonic, airpanel, vl, 50, 15, inc...   \n",
              "448            [subject, private, vl, gra, remove, me]   \n",
              "449  [subject, cornhusker, lone, star, payments, se...   \n",
              "\n",
              "                                             Email_str  \n",
              "0    subject enron actuals for july 14 thru 16 2000...  \n",
              "1    subject fw pymt for 03 01 sales adjustment for...  \n",
              "2    subject re automate load of scheduled quantity...  \n",
              "3    subject i congratulate you my friend with sain...  \n",
              "4    subject re hi suzette the secret on how porn s...  \n",
              "..                                                 ...  \n",
              "445  subject proccess credit cards online hispanic ...  \n",
              "446  subject re volume increase hpl meter 68 6296 s...  \n",
              "447  subject viewsonic airpanel vl 50 15 inch smart...  \n",
              "448                   subject private vl gra remove me  \n",
              "449  subject cornhusker lone star payments section ...  \n",
              "\n",
              "[450 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8731520-e882-47b9-9386-6429a4cbd163\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Email</th>\n",
              "      <th>Type</th>\n",
              "      <th>Email_tok</th>\n",
              "      <th>Email_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: enron actuals for july 14 thru 16 , 2...</td>\n",
              "      <td>ham</td>\n",
              "      <td>[subject, enron, actuals, for, july, 14, thru,...</td>\n",
              "      <td>subject enron actuals for july 14 thru 16 2000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: fw : pymt for 03 / 01 sales adjustmen...</td>\n",
              "      <td>ham</td>\n",
              "      <td>[subject, fw, pymt, for, 03, 01, sales, adjust...</td>\n",
              "      <td>subject fw pymt for 03 01 sales adjustment for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: re : automate load of scheduled quant...</td>\n",
              "      <td>ham</td>\n",
              "      <td>[subject, re, automate, load, of, scheduled, q...</td>\n",
              "      <td>subject re automate load of scheduled quantity...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: i congratulate you , my friend , with...</td>\n",
              "      <td>spam</td>\n",
              "      <td>[subject, i, congratulate, you, my, friend, wi...</td>\n",
              "      <td>subject i congratulate you my friend with sain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: re : hi suzette\\r\\nthe secret on how ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>[subject, re, hi, suzette, the, secret, on, ho...</td>\n",
              "      <td>subject re hi suzette the secret on how porn s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>Subject: proccess credit cards online , hispan...</td>\n",
              "      <td>spam</td>\n",
              "      <td>[subject, proccess, credit, cards, online, his...</td>\n",
              "      <td>subject proccess credit cards online hispanic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>Subject: re : volume increase - hpl meter 68 -...</td>\n",
              "      <td>ham</td>\n",
              "      <td>[subject, re, volume, increase, hpl, meter, 68...</td>\n",
              "      <td>subject re volume increase hpl meter 68 6296 s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>Subject: viewsonic airpanel vl 50 15 - inch sm...</td>\n",
              "      <td>spam</td>\n",
              "      <td>[subject, viewsonic, airpanel, vl, 50, 15, inc...</td>\n",
              "      <td>subject viewsonic airpanel vl 50 15 inch smart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>Subject: private vl @ gra\\r\\nremove me</td>\n",
              "      <td>spam</td>\n",
              "      <td>[subject, private, vl, gra, remove, me]</td>\n",
              "      <td>subject private vl gra remove me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>Subject: cornhusker - lone star payments\\r\\nse...</td>\n",
              "      <td>ham</td>\n",
              "      <td>[subject, cornhusker, lone, star, payments, se...</td>\n",
              "      <td>subject cornhusker lone star payments section ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>450 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8731520-e882-47b9-9386-6429a4cbd163')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f8731520-e882-47b9-9386-6429a4cbd163 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f8731520-e882-47b9-9386-6429a4cbd163');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ea04e5f-73b6-43cd-bfac-cd66f31420d5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ea04e5f-73b6-43cd-bfac-cd66f31420d5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ea04e5f-73b6-43cd-bfac-cd66f31420d5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_13b3279b-4b7a-4f82-abbb-bdbfc1128ff8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('enron1_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_13b3279b-4b7a-4f82-abbb-bdbfc1128ff8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('enron1_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "enron1_train_df",
              "summary": "{\n  \"name\": \"enron1_train_df\",\n  \"rows\": 450,\n  \"fields\": [\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 448,\n        \"samples\": [\n          \"Subject: txu fuels / sds nomination for april 2001\\r\\nattached is the april 2001 nomination for our takes under the gas sales and\\r\\npurchase contract between txu fuel company ( previously tufco ) and sds . please\\r\\nadvise should you have any questions concerning the attached .\\r\\nthanks ,\\r\\nccs\\r\\n( see attached file : sdsnom . xls )\\r\\n- sdsnom . xls\",\n          \"Subject: enron / hpl actuals for november 10 - 12 , 2000\\r\\nnovember 10 , 2000\\r\\nteco tap 40 . 000 / enron ; 66 . 667 / hpl iferc\\r\\nnovember 11 , 2000\\r\\nteco tap 40 . 000 / enron ; 104 . 583 / hpl iferc\\r\\nnovember 12 , 2000\\r\\nteco tap 40 . 000 / enron ; 110 . 000 / hpl iferc\",\n          \"Subject: realize that now is the perfect time to start feeling better\\r\\n- - - - 9723437065620721\\r\\nhi varou ,\\r\\nwhat is the ideal dosage for the drug that treats impotence in men ? 25 mg begins to work in about 15 minutes to a half an hour and lasts for 3 hours or so , able to have an erection and intercourse more than once in a few hours . - age 65 , new york city\\r\\nwe are providing an online solution to finding and receiving prescription medications from the comfort of your own home , you will be able to save money , save time , save yourself from worry , and most of all , save your life !\\r\\nplay an active role and participate more fully in your own process of care . . visit us today .\\r\\nbest regards ,\\r\\nmargot halpin\\r\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_tok\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_str\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 447,\n        \"samples\": [\n          \"subject not as rough list i spoke to sabrae we are cumulatively balanced with pge cumulative and don t want to do anything volumetric at either meter the nng oba is being reconciled and they are really hesitant reluctant to balance current month florida can be paid back 5 m or more it is being reconciled but sabrae is okay with paying back this just in from mark mccoy the oasis oba is flat as of today s gas day that should hit the imb report next week mary\",\n          \"subject the permanent fix to penis enlargement limited time offer add atleast 3 inches or your money back visit us to learn more no more offers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "enron1_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XUdbkfbbwhtE"
      },
      "outputs": [],
      "source": [
        "all_results_accuracy = {}\n",
        "all_results_recall = {}\n",
        "all_results_f1 = {}\n",
        "all_results_precision = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0VaGCC-vWf1"
      },
      "source": [
        "## Multinomial Naive Bayes using Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZeL3JjKvZxU"
      },
      "source": [
        "### Scikit-learn approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XdL2OoxfvKE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccf604d-d1ed-4895-803d-091b644c02a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1\n",
            "accuracy - 0.9407894736842105\n",
            "recall - 0.8859060402684564\n",
            "f1 score - 0.9072164948453608\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      0.97      0.96       307\n",
            "        spam       0.93      0.89      0.91       149\n",
            "\n",
            "    accuracy                           0.94       456\n",
            "   macro avg       0.94      0.93      0.93       456\n",
            "weighted avg       0.94      0.94      0.94       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron2\n",
            "accuracy - 0.9435146443514645\n",
            "recall - 0.8615384615384616\n",
            "f1 score - 0.8924302788844622\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      0.97      0.96       348\n",
            "        spam       0.93      0.86      0.89       130\n",
            "\n",
            "    accuracy                           0.94       478\n",
            "   macro avg       0.94      0.92      0.93       478\n",
            "weighted avg       0.94      0.94      0.94       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron4\n",
            "accuracy - 0.9742173112338858\n",
            "recall - 0.989769820971867\n",
            "f1 score - 0.9822335025380711\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.93      0.95       152\n",
            "        spam       0.97      0.99      0.98       391\n",
            "\n",
            "    accuracy                           0.97       543\n",
            "   macro avg       0.97      0.96      0.97       543\n",
            "weighted avg       0.97      0.97      0.97       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "{'sklearn_mnb_bow_enron1': 0.9407894736842105, 'sklearn_mnb_bow_enron2': 0.9435146443514645, 'sklearn_mnb_bow_enron4': 0.9742173112338858}\n",
            "{'sklearn_mnb_bow_enron1': 0.8859060402684564, 'sklearn_mnb_bow_enron2': 0.8615384615384616, 'sklearn_mnb_bow_enron4': 0.989769820971867}\n",
            "{'sklearn_mnb_bow_enron1': 0.9072164948453608, 'sklearn_mnb_bow_enron2': 0.8924302788844622, 'sklearn_mnb_bow_enron4': 0.9822335025380711}\n",
            "{'sklearn_mnb_bow_enron1': 0.9295774647887324, 'sklearn_mnb_bow_enron2': 0.9256198347107438, 'sklearn_mnb_bow_enron4': 0.9748110831234257}\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "\n",
        "    model = MultinomialNB(alpha=1.0)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "\n",
        "all_results_accuracy['sklearn_mnb_bow_enron1'], all_results_recall['sklearn_mnb_bow_enron1'], all_results_f1['sklearn_mnb_bow_enron1'], all_results_precision['sklearn_mnb_bow_enron1'] = predict_type(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['sklearn_mnb_bow_enron2'], all_results_recall['sklearn_mnb_bow_enron2'], all_results_f1['sklearn_mnb_bow_enron2'], all_results_precision['sklearn_mnb_bow_enron2'] = predict_type(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['sklearn_mnb_bow_enron4'], all_results_recall['sklearn_mnb_bow_enron4'], all_results_f1['sklearn_mnb_bow_enron4'], all_results_precision['sklearn_mnb_bow_enron4'] = predict_type(enron4_train_df, enron4_test_df, 'enron4')\n",
        "\n",
        "print(all_results_accuracy)\n",
        "print(all_results_recall)\n",
        "print(all_results_f1)\n",
        "print(all_results_precision)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3kXSwpqw3Vz"
      },
      "source": [
        "### Step by step approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tJz79JdDvvH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1a1b6e-c0b5-4186-bde9-9660316949cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1\n",
            "Accuracy = 93.85964912280701%\n",
            "Recall =  0.8590604026845637\n",
            "F1 =  0.9014084507042253\n",
            "-----------------------------------------------------------\n",
            "enron2\n",
            "Accuracy = 94.35146443514645%\n",
            "Recall =  0.8461538461538461\n",
            "F1 =  0.8906882591093118\n",
            "-----------------------------------------------------------\n",
            "enron4\n",
            "Accuracy = 95.39594843462247%\n",
            "Recall =  0.979539641943734\n",
            "F1 =  0.9683944374209861\n",
            "-----------------------------------------------------------\n",
            "{'sklearn_mnb_bow_enron1': 0.9407894736842105, 'sklearn_mnb_bow_enron2': 0.9435146443514645, 'sklearn_mnb_bow_enron4': 0.9742173112338858, 'step_by_step_mnb_bow_enron1': 0.9385964912280702, 'step_by_step_mnb_bow_enron2': 0.9435146443514645, 'step_by_step_mnb_bow_enron4': 0.9539594843462247}\n",
            "{'sklearn_mnb_bow_enron1': 0.8859060402684564, 'sklearn_mnb_bow_enron2': 0.8615384615384616, 'sklearn_mnb_bow_enron4': 0.989769820971867, 'step_by_step_mnb_bow_enron1': 0.8590604026845637, 'step_by_step_mnb_bow_enron2': 0.8461538461538461, 'step_by_step_mnb_bow_enron4': 0.979539641943734}\n",
            "{'sklearn_mnb_bow_enron1': 0.9072164948453608, 'sklearn_mnb_bow_enron2': 0.8924302788844622, 'sklearn_mnb_bow_enron4': 0.9822335025380711, 'step_by_step_mnb_bow_enron1': 0.9014084507042253, 'step_by_step_mnb_bow_enron2': 0.8906882591093118, 'step_by_step_mnb_bow_enron4': 0.9683944374209861}\n",
            "{'sklearn_mnb_bow_enron1': 0.9295774647887324, 'sklearn_mnb_bow_enron2': 0.9256198347107438, 'sklearn_mnb_bow_enron4': 0.9748110831234257, 'step_by_step_mnb_bow_enron1': 0.9481481481481482, 'step_by_step_mnb_bow_enron2': 0.9401709401709402, 'step_by_step_mnb_bow_enron4': 0.9575}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to train Multinomial Naive Bayes\n",
        "def train_multinomial_naive_bayes(df):\n",
        "    cond_prob_ham = {}\n",
        "    cond_prob_spam = {}\n",
        "\n",
        "    \"\"\"Extracting vocabulary\"\"\"\n",
        "    vocabulary = set()\n",
        "    for email in df['Email_str']:  # Iterate over each email\n",
        "        tokens = email.split()  # Split each email into words\n",
        "        vocabulary.update(tokens)  # Add tokens to the vocabulary\n",
        "    vocabulary = list(vocabulary)\n",
        "\n",
        "    \"\"\"Counting the number of emails in each class\"\"\"\n",
        "    ham_docs = df[df['Type'] == 'ham'].shape[0]\n",
        "    spam_docs = df[df['Type'] == 'spam'].shape[0]\n",
        "    total_docs = df.shape[0]\n",
        "\n",
        "    \"\"\"Join the text of emails belonging to the same class\"\"\"\n",
        "    text_ham = \" \".join(df[df['Type'] == 'ham']['Email_str'].astype(str))\n",
        "    text_spam = \" \".join(df[df['Type'] == 'spam']['Email_str'].astype(str))\n",
        "\n",
        "    \"\"\"Calculating priors\"\"\"\n",
        "    prior_ham = ham_docs / total_docs\n",
        "    prior_spam = spam_docs / total_docs\n",
        "\n",
        "    \"\"\"Count tokens in ham and spam classes\"\"\"\n",
        "    tokens_in_ham = sum(text_ham.split().count(term) for term in vocabulary)\n",
        "    tokens_in_spam = sum(text_spam.split().count(term) for term in vocabulary)\n",
        "\n",
        "    \"\"\"Calculating conditional probabilities\"\"\"\n",
        "    for term in vocabulary:\n",
        "        tct_ham = text_ham.split().count(term)\n",
        "        cond_prob_ham[term] = (tct_ham + 1.0) / (tokens_in_ham + len(vocabulary))\n",
        "\n",
        "        tct_spam = text_spam.split().count(term)\n",
        "        cond_prob_spam[term] = (tct_spam + 1.0) / (tokens_in_spam + len(vocabulary))\n",
        "\n",
        "    return vocabulary, cond_prob_spam, cond_prob_ham, prior_ham, prior_spam\n",
        "\n",
        "# Function to classify emails using the trained model\n",
        "def apply_mnb(prior_ham, prior_spam, vocabulary, email, cond_prob_ham, cond_prob_spam):\n",
        "    tokens = email.split()\n",
        "    scores = {'ham': np.log(prior_ham), 'spam': np.log(prior_spam)}  # Log priors\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in cond_prob_ham:\n",
        "            scores['ham'] += np.log(cond_prob_ham[token])\n",
        "        else:\n",
        "            scores['ham'] += np.log(1 / (sum(cond_prob_ham.get(t, 0) for t in vocabulary) + len(vocabulary)))\n",
        "\n",
        "        if token in cond_prob_spam:\n",
        "            scores['spam'] += np.log(cond_prob_spam[token])\n",
        "        else:\n",
        "            scores['spam'] += np.log(1 / (sum(cond_prob_spam.get(t, 0) for t in vocabulary) + len(vocabulary)))\n",
        "\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "# Function to classify and evaluate test documents\n",
        "def funct(train_df, test_df, name):\n",
        "    # Train the Multinomial Naive Bayes model\n",
        "    vocabulary, cond_prob_spam, cond_prob_ham, prior_ham, prior_spam = train_multinomial_naive_bayes(train_df)\n",
        "\n",
        "    # Classify test documents\n",
        "    results = []\n",
        "    for email in test_df['Email_str']:\n",
        "        results.append(apply_mnb(prior_ham, prior_spam, vocabulary, email, cond_prob_ham, cond_prob_spam))\n",
        "\n",
        "    # Evaluate the predictions\n",
        "    correct_predictions = sum(pred == true for pred, true in zip(results, test_df['Type']))\n",
        "    accuracy = correct_predictions / len(results)\n",
        "    recall = recall_score(test_df['Type'], results, pos_label='spam')\n",
        "    f1 = f1_score(test_df['Type'], results, pos_label='spam')\n",
        "    precision = precision_score(test_df['Type'], results, pos_label='spam')\n",
        "\n",
        "    # Print the results\n",
        "    print(name)\n",
        "    print(f\"Accuracy = {accuracy * 100}%\")\n",
        "    print(\"Recall = \", recall)\n",
        "    print(\"F1 = \", f1)\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "# Example usage with datasets\n",
        "all_results_accuracy['step_by_step_mnb_bow_enron1'], all_results_recall['step_by_step_mnb_bow_enron1'], all_results_f1['step_by_step_mnb_bow_enron1'], all_results_precision['step_by_step_mnb_bow_enron1'] = funct(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['step_by_step_mnb_bow_enron2'], all_results_recall['step_by_step_mnb_bow_enron2'], all_results_f1['step_by_step_mnb_bow_enron2'], all_results_precision['step_by_step_mnb_bow_enron2'] = funct(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['step_by_step_mnb_bow_enron4'], all_results_recall['step_by_step_mnb_bow_enron4'], all_results_f1['step_by_step_mnb_bow_enron4'], all_results_precision['step_by_step_mnb_bow_enron4'] = funct(enron4_train_df, enron4_test_df, 'enron4')\n",
        "\n",
        "print(all_results_accuracy)\n",
        "print(all_results_recall)\n",
        "print(all_results_f1)\n",
        "print(all_results_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Ni8fP6DwycUZ"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['step_by_step_bow_enron1'] = accuracy_e1\n",
        "# all_results_accuracy['step_by_step_bow_enron2'] = accuracy_e2\n",
        "# all_results_accuracy['step_by_step_bow_enron4'] = accuracy_e4\n",
        "\n",
        "# all_results_recall['step_by_step_bow_enron1'] = recall_e1\n",
        "# all_results_recall['step_by_step_bow_enron2'] = recall_e2\n",
        "# all_results_recall['step_by_step_bow_enron4'] = recall_e4\n",
        "\n",
        "# all_results_f1['step_by_step_bow_enron1'] = f1_e1\n",
        "# all_results_f1['step_by_step_bow_enron2'] = f1_e2\n",
        "# all_results_f1['step_by_step_bow_enron4'] = f1_e4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_I2YItWK0jCx"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JskH97IB08Ar"
      },
      "source": [
        "## Bernoulli Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqvZP4sX0--L"
      },
      "source": [
        "### Scikit learn approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ef_azcU603Gr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed61e136-dfc7-4ce2-f3f6-e41a836f348a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1\n",
            "accuracy - 0.7302631578947368\n",
            "recall - 0.19463087248322147\n",
            "f1 score - 0.3204419889502762\n",
            "precision - 0.90625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.72      0.99      0.83       307\n",
            "        spam       0.91      0.19      0.32       149\n",
            "\n",
            "    accuracy                           0.73       456\n",
            "   macro avg       0.81      0.59      0.58       456\n",
            "weighted avg       0.78      0.73      0.66       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron2\n",
            "accuracy - 0.7740585774058577\n",
            "recall - 0.19230769230769232\n",
            "f1 score - 0.31645569620253167\n",
            "precision - 0.8928571428571429\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.77      0.99      0.86       348\n",
            "        spam       0.89      0.19      0.32       130\n",
            "\n",
            "    accuracy                           0.77       478\n",
            "   macro avg       0.83      0.59      0.59       478\n",
            "weighted avg       0.80      0.77      0.72       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron4\n",
            "accuracy - 0.9171270718232044\n",
            "recall - 1.0\n",
            "f1 score - 0.9455864570737605\n",
            "precision - 0.8967889908256881\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.70      0.83       152\n",
            "        spam       0.90      1.00      0.95       391\n",
            "\n",
            "    accuracy                           0.92       543\n",
            "   macro avg       0.95      0.85      0.89       543\n",
            "weighted avg       0.93      0.92      0.91       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def predict_type(X_train, X_test, name):\n",
        "    y_train = X_train['Type']\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "\n",
        "    X_train_text = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    model = BernoulliNB(alpha=1.0)\n",
        "    model.fit(X_train_text, y_train)\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    X_test_text = vectorizer.transform(X_test['Email_str'])\n",
        "\n",
        "    y_pred = model.predict(X_test_text)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "# Example usage with your dataframes\n",
        "all_results_accuracy['sklearn_bnb_bern_enron1'], all_results_recall['sklearn_bnb_bern_enron1'], all_results_f1['sklearn_bnb_bern_enron1'], all_results_precision['sklearn_bnb_bern_enron1'] = predict_type(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['sklearn_bnb_bern_enron2'], all_results_recall['sklearn_bnb_bern_enron2'], all_results_f1['sklearn_bnb_bern_enron2'], all_results_precision['sklearn_bnb_bern_enron2'] = predict_type(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['sklearn_bnb_bern_enron4'], all_results_recall['sklearn_bnb_bern_enron4'], all_results_f1['sklearn_bnb_bern_enron4'], all_results_precision['sklearn_bnb_bern_enron4'] = predict_type(enron4_train_df, enron4_test_df, 'enron4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5R5igZtB1wGa"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['sklearn_bern_enron1'] = sklearn_bern_acc_e1\n",
        "# all_results_accuracy['sklearn_bern_enron2'] = sklearn_bern_acc_e2\n",
        "# all_results_accuracy['sklearn_bern_enron4'] = sklearn_bern_acc_e4\n",
        "\n",
        "# all_results_recall['sklearn_bern_enron1'] = sklearn_bern_recall_e1\n",
        "# all_results_recall['sklearn_bern_enron2'] = sklearn_bern_recall_e2\n",
        "# all_results_recall['sklearn_bern_enron4'] = sklearn_bern_recall_e4\n",
        "\n",
        "# all_results_f1['sklearn_bern_enron1'] = sklearn_bern_f1_e1\n",
        "# all_results_f1['sklearn_bern_enron2'] = sklearn_bern_f1_e2\n",
        "# all_results_f1['sklearn_bern_enron4'] = sklearn_bern_f1_e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EowYDDp3TXA"
      },
      "source": [
        "### Step by step approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mpBdvUQz19DM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92523e6-f336-48b0-90d8-92cd025d5e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1\n",
            "Accuracy = 73.46491228070175%\n",
            "Recall =  0.2080536912751678\n",
            "F1 =  0.33879781420765026\n",
            "Precision =  0.9117647058823529\n",
            "-----------------------------------------------------------\n",
            "enron2\n",
            "Accuracy = 77.82426778242679%\n",
            "Recall =  0.2076923076923077\n",
            "F1 =  0.3375\n",
            "Precision =  0.9\n",
            "-----------------------------------------------------------\n",
            "enron4\n",
            "Accuracy = 91.71270718232044%\n",
            "Recall =  1.0\n",
            "F1 =  0.9455864570737605\n",
            "Precision =  0.8967889908256881\n",
            "-----------------------------------------------------------\n",
            "{'sklearn_mnb_bow_enron1': 0.9407894736842105, 'sklearn_mnb_bow_enron2': 0.9435146443514645, 'sklearn_mnb_bow_enron4': 0.9742173112338858, 'step_by_step_mnb_bow_enron1': 0.9385964912280702, 'step_by_step_mnb_bow_enron2': 0.9435146443514645, 'step_by_step_mnb_bow_enron4': 0.9539594843462247, 'sklearn_bnb_bern_enron1': 0.7302631578947368, 'sklearn_bnb_bern_enron2': 0.7740585774058577, 'sklearn_bnb_bern_enron4': 0.9171270718232044, 'step_by_step_bern_enron1': 0.7346491228070176, 'step_by_step_bern_enron2': 0.7782426778242678, 'step_by_step_bern_enron4': 0.9171270718232044}\n",
            "{'sklearn_mnb_bow_enron1': 0.8859060402684564, 'sklearn_mnb_bow_enron2': 0.8615384615384616, 'sklearn_mnb_bow_enron4': 0.989769820971867, 'step_by_step_mnb_bow_enron1': 0.8590604026845637, 'step_by_step_mnb_bow_enron2': 0.8461538461538461, 'step_by_step_mnb_bow_enron4': 0.979539641943734, 'sklearn_bnb_bern_enron1': 0.19463087248322147, 'sklearn_bnb_bern_enron2': 0.19230769230769232, 'sklearn_bnb_bern_enron4': 1.0, 'step_by_step_bern_enron1': 0.2080536912751678, 'step_by_step_bern_enron2': 0.2076923076923077, 'step_by_step_bern_enron4': 1.0}\n",
            "{'sklearn_mnb_bow_enron1': 0.9072164948453608, 'sklearn_mnb_bow_enron2': 0.8924302788844622, 'sklearn_mnb_bow_enron4': 0.9822335025380711, 'step_by_step_mnb_bow_enron1': 0.9014084507042253, 'step_by_step_mnb_bow_enron2': 0.8906882591093118, 'step_by_step_mnb_bow_enron4': 0.9683944374209861, 'sklearn_bnb_bern_enron1': 0.3204419889502762, 'sklearn_bnb_bern_enron2': 0.31645569620253167, 'sklearn_bnb_bern_enron4': 0.9455864570737605, 'step_by_step_bern_enron1': 0.33879781420765026, 'step_by_step_bern_enron2': 0.3375, 'step_by_step_bern_enron4': 0.9455864570737605}\n",
            "{'sklearn_mnb_bow_enron1': 0.9295774647887324, 'sklearn_mnb_bow_enron2': 0.9256198347107438, 'sklearn_mnb_bow_enron4': 0.9748110831234257, 'step_by_step_mnb_bow_enron1': 0.9481481481481482, 'step_by_step_mnb_bow_enron2': 0.9401709401709402, 'step_by_step_mnb_bow_enron4': 0.9575, 'sklearn_bnb_bern_enron1': 0.90625, 'sklearn_bnb_bern_enron2': 0.8928571428571429, 'sklearn_bnb_bern_enron4': 0.8967889908256881, 'step_by_step_bern_enron1': 0.9117647058823529, 'step_by_step_bern_enron2': 0.9, 'step_by_step_bern_enron4': 0.8967889908256881}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "\n",
        "# Function to train Bernoulli Naive Bayes\n",
        "def train_bernoulli_naive_bayes(df):\n",
        "    cond_prob_ham = {}\n",
        "    cond_prob_spam = {}\n",
        "\n",
        "    \"\"\"Extracting vocabulary\"\"\"\n",
        "    vocabulary = set()\n",
        "    for email in df['Email_str']:  # Iterate over each email\n",
        "        tokens = email.split()  # Split each email into words\n",
        "        vocabulary.update(tokens)  # Add tokens to the vocabulary\n",
        "    vocabulary = list(vocabulary)\n",
        "\n",
        "    \"\"\"Counting the number of emails in each class\"\"\"\n",
        "    ham_docs = df[df['Type'] == 'ham'].shape[0]\n",
        "    spam_docs = df[df['Type'] == 'spam'].shape[0]\n",
        "    total_docs = df.shape[0]\n",
        "\n",
        "    \"\"\"Join the text of emails belonging to the same class\"\"\"\n",
        "    ham_emails = df[df['Type'] == 'ham']['Email_str'].astype(str)\n",
        "    spam_emails = df[df['Type'] == 'spam']['Email_str'].astype(str)\n",
        "\n",
        "    \"\"\"Calculating priors\"\"\"\n",
        "    prior_ham = ham_docs / total_docs\n",
        "    prior_spam = spam_docs / total_docs\n",
        "\n",
        "    \"\"\"Calculating conditional probabilities (Bernoulli approach)\"\"\"\n",
        "    for term in vocabulary:\n",
        "        # For ham class: P(term|ham)\n",
        "        ham_with_term = sum(1 for email in ham_emails if term in email.split())\n",
        "        cond_prob_ham[term] = (ham_with_term + 1.0) / (ham_docs + 2.0)  # Laplace smoothing\n",
        "\n",
        "        # For spam class: P(term|spam)\n",
        "        spam_with_term = sum(1 for email in spam_emails if term in email.split())\n",
        "        cond_prob_spam[term] = (spam_with_term + 1.0) / (spam_docs + 2.0)  # Laplace smoothing\n",
        "\n",
        "    return vocabulary, cond_prob_spam, cond_prob_ham, prior_ham, prior_spam\n",
        "\n",
        "# Function to classify emails using the trained Bernoulli Naive Bayes model\n",
        "def apply_bnb(prior_ham, prior_spam, vocabulary, email, cond_prob_ham, cond_prob_spam):\n",
        "    tokens = set(email.split())  # Consider unique tokens for Bernoulli\n",
        "    scores = {'ham': np.log(prior_ham), 'spam': np.log(prior_spam)}  # Log priors\n",
        "\n",
        "    for term in vocabulary:\n",
        "        if term in tokens:\n",
        "            scores['ham'] += np.log(cond_prob_ham[term])\n",
        "            scores['spam'] += np.log(cond_prob_spam[term])\n",
        "        else:\n",
        "            scores['ham'] += np.log(1 - cond_prob_ham[term])\n",
        "            scores['spam'] += np.log(1 - cond_prob_spam[term])\n",
        "\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "# Function to classify and evaluate test documents\n",
        "def funct(train_df, test_df, name):\n",
        "    # Train the Bernoulli Naive Bayes model\n",
        "    vocabulary, cond_prob_spam, cond_prob_ham, prior_ham, prior_spam = train_bernoulli_naive_bayes(train_df)\n",
        "\n",
        "    # Classify test documents\n",
        "    results = []\n",
        "    for email in test_df['Email_str']:\n",
        "        results.append(apply_bnb(prior_ham, prior_spam, vocabulary, email, cond_prob_ham, cond_prob_spam))\n",
        "\n",
        "    # Evaluate the predictions\n",
        "    correct_predictions = sum(pred == true for pred, true in zip(results, test_df['Type']))\n",
        "    accuracy_score = correct_predictions / len(results)\n",
        "    recall = recall_score(test_df['Type'], results, pos_label='spam')\n",
        "    f1 = f1_score(test_df['Type'], results, pos_label='spam')\n",
        "    precision = precision_score(test_df['Type'], results, pos_label='spam')\n",
        "\n",
        "    # Print the results\n",
        "    print(name)\n",
        "    print(f\"Accuracy = {accuracy_score * 100}%\")\n",
        "    print(\"Recall = \", recall)\n",
        "    print(\"F1 = \", f1)\n",
        "    print(\"Precision = \", precision)\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    return accuracy_score, recall, f1, precision\n",
        "\n",
        "# Example usage with datasets\n",
        "all_results_accuracy['step_by_step_bern_enron1'], all_results_recall['step_by_step_bern_enron1'], all_results_f1['step_by_step_bern_enron1'], all_results_precision['step_by_step_bern_enron1'] = funct(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['step_by_step_bern_enron2'], all_results_recall['step_by_step_bern_enron2'], all_results_f1['step_by_step_bern_enron2'], all_results_precision['step_by_step_bern_enron2'] = funct(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['step_by_step_bern_enron4'], all_results_recall['step_by_step_bern_enron4'], all_results_f1['step_by_step_bern_enron4'], all_results_precision['step_by_step_bern_enron4'] = funct(enron4_train_df, enron4_test_df, 'enron4')\n",
        "\n",
        "print(all_results_accuracy)\n",
        "print(all_results_recall)\n",
        "print(all_results_f1)\n",
        "print(all_results_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hhiuaTUV3Xsz"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['step_by_step_bern_enron1'] = accuracy_e1\n",
        "# all_results_accuracy['step_by_step_bern_enron2'] = accuracy_e2\n",
        "# all_results_accuracy['step_by_step_bern_enron4'] = accuracy_e4\n",
        "\n",
        "# all_results_f1['step_by_step_bern_enron1'] = f1_e1\n",
        "# all_results_f1['step_by_step_bern_enron2'] = f1_e2\n",
        "# all_results_f1['step_by_step_bern_enron4'] = f1_e4\n",
        "\n",
        "# all_results_recall['step_by_step_bern_enron1'] = recall_e1\n",
        "# all_results_recall['step_by_step_bern_enron2'] = recall_e2\n",
        "# all_results_recall['step_by_step_bern_enron4'] = recall_e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FHHjHSkA8eS"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vhmHKLUBDWd"
      },
      "source": [
        "### Scikit learn approach BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zmY9T6aj4cDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcdd9c2-a031-4c92-a62b-b4a47bc1cc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1\n",
            "accuracy - 0.9649122807017544\n",
            "recall - 0.9865771812080537\n",
            "f1 score - 0.9483870967741935\n",
            "precision - 0.9130434782608695\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.95      0.97       307\n",
            "        spam       0.91      0.99      0.95       149\n",
            "\n",
            "    accuracy                           0.96       456\n",
            "   macro avg       0.95      0.97      0.96       456\n",
            "weighted avg       0.97      0.96      0.97       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron2\n",
            "accuracy - 0.9518828451882845\n",
            "recall - 0.9153846153846154\n",
            "f1 score - 0.9118773946360154\n",
            "precision - 0.9083969465648855\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.97      0.97       348\n",
            "        spam       0.91      0.92      0.91       130\n",
            "\n",
            "    accuracy                           0.95       478\n",
            "   macro avg       0.94      0.94      0.94       478\n",
            "weighted avg       0.95      0.95      0.95       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron4\n",
            "accuracy - 0.9465930018416207\n",
            "recall - 0.9974424552429667\n",
            "f1 score - 0.9641532756489494\n",
            "precision - 0.9330143540669856\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.82      0.90       152\n",
            "        spam       0.93      1.00      0.96       391\n",
            "\n",
            "    accuracy                           0.95       543\n",
            "   macro avg       0.96      0.91      0.93       543\n",
            "weighted avg       0.95      0.95      0.94       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "{'sklearn_mnb_bow_enron1': 0.9407894736842105, 'sklearn_mnb_bow_enron2': 0.9435146443514645, 'sklearn_mnb_bow_enron4': 0.9742173112338858, 'step_by_step_mnb_bow_enron1': 0.9385964912280702, 'step_by_step_mnb_bow_enron2': 0.9435146443514645, 'step_by_step_mnb_bow_enron4': 0.9539594843462247, 'sklearn_bnb_bern_enron1': 0.7302631578947368, 'sklearn_bnb_bern_enron2': 0.7740585774058577, 'sklearn_bnb_bern_enron4': 0.9171270718232044, 'step_by_step_bern_enron1': 0.7346491228070176, 'step_by_step_bern_enron2': 0.7782426778242678, 'step_by_step_bern_enron4': 0.9171270718232044, 'sklearn_lr_bow_enron1': 0.9649122807017544, 'sklearn_lr_bow_enron2': 0.9518828451882845, 'sklearn_lr_bow_enron4': 0.9465930018416207}\n",
            "{'sklearn_mnb_bow_enron1': 0.8859060402684564, 'sklearn_mnb_bow_enron2': 0.8615384615384616, 'sklearn_mnb_bow_enron4': 0.989769820971867, 'step_by_step_mnb_bow_enron1': 0.8590604026845637, 'step_by_step_mnb_bow_enron2': 0.8461538461538461, 'step_by_step_mnb_bow_enron4': 0.979539641943734, 'sklearn_bnb_bern_enron1': 0.19463087248322147, 'sklearn_bnb_bern_enron2': 0.19230769230769232, 'sklearn_bnb_bern_enron4': 1.0, 'step_by_step_bern_enron1': 0.2080536912751678, 'step_by_step_bern_enron2': 0.2076923076923077, 'step_by_step_bern_enron4': 1.0, 'sklearn_lr_bow_enron1': 0.9865771812080537, 'sklearn_lr_bow_enron2': 0.9153846153846154, 'sklearn_lr_bow_enron4': 0.9974424552429667}\n",
            "{'sklearn_mnb_bow_enron1': 0.9072164948453608, 'sklearn_mnb_bow_enron2': 0.8924302788844622, 'sklearn_mnb_bow_enron4': 0.9822335025380711, 'step_by_step_mnb_bow_enron1': 0.9014084507042253, 'step_by_step_mnb_bow_enron2': 0.8906882591093118, 'step_by_step_mnb_bow_enron4': 0.9683944374209861, 'sklearn_bnb_bern_enron1': 0.3204419889502762, 'sklearn_bnb_bern_enron2': 0.31645569620253167, 'sklearn_bnb_bern_enron4': 0.9455864570737605, 'step_by_step_bern_enron1': 0.33879781420765026, 'step_by_step_bern_enron2': 0.3375, 'step_by_step_bern_enron4': 0.9455864570737605, 'sklearn_lr_bow_enron1': 0.9483870967741935, 'sklearn_lr_bow_enron2': 0.9118773946360154, 'sklearn_lr_bow_enron4': 0.9641532756489494}\n",
            "{'sklearn_mnb_bow_enron1': 0.9295774647887324, 'sklearn_mnb_bow_enron2': 0.9256198347107438, 'sklearn_mnb_bow_enron4': 0.9748110831234257, 'step_by_step_mnb_bow_enron1': 0.9481481481481482, 'step_by_step_mnb_bow_enron2': 0.9401709401709402, 'step_by_step_mnb_bow_enron4': 0.9575, 'sklearn_bnb_bern_enron1': 0.90625, 'sklearn_bnb_bern_enron2': 0.8928571428571429, 'sklearn_bnb_bern_enron4': 0.8967889908256881, 'step_by_step_bern_enron1': 0.9117647058823529, 'step_by_step_bern_enron2': 0.9, 'step_by_step_bern_enron4': 0.8967889908256881, 'sklearn_lr_bow_enron1': 0.9130434782608695, 'sklearn_lr_bow_enron2': 0.9083969465648855, 'sklearn_lr_bow_enron4': 0.9330143540669856}\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    y = X_train['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "    X = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    lambda_values = {'C': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "    grid_search = GridSearchCV(model, lambda_values, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_C = grid_search.best_params_['C']\n",
        "    new_model = LogisticRegression(C=best_C)\n",
        "    new_model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    # X_test['Email_str'] = X_test['Email'].apply(lambda x: ' '.join(x))\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    y_pred = new_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "all_results_accuracy['sklearn_lr_bow_enron1'], all_results_recall['sklearn_lr_bow_enron1'], all_results_f1['sklearn_lr_bow_enron1'], all_results_precision['sklearn_lr_bow_enron1'] = predict_type(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['sklearn_lr_bow_enron2'], all_results_recall['sklearn_lr_bow_enron2'], all_results_f1['sklearn_lr_bow_enron2'], all_results_precision['sklearn_lr_bow_enron2'] = predict_type(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['sklearn_lr_bow_enron4'], all_results_recall['sklearn_lr_bow_enron4'], all_results_f1['sklearn_lr_bow_enron4'], all_results_precision['sklearn_lr_bow_enron4'] = predict_type(enron4_train_df, enron4_test_df, 'enron4')\n",
        "\n",
        "print(all_results_accuracy)\n",
        "print(all_results_recall)\n",
        "print(all_results_f1)\n",
        "print(all_results_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FY0SHb7e2W6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bbb92a-7b04-4995-b486-8b9b093ab69f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((315, 9762), (450, 9762))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "X = vectorizer.fit_transform(enron1_train_df['Email_str'])\n",
        "y = enron1_train_df['Type']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "X_train.shape, X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgW_XI_kkBk4"
      },
      "source": [
        "### Hyperparameter Tuning LR (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MHtMHQ1Di6Ra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "b9bfe93a-4fc5-4261-b04e-03663786efd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Dataset  \\\n",
              "0   enron1   \n",
              "1   enron2   \n",
              "2   enron4   \n",
              "3   enron1   \n",
              "4   enron2   \n",
              "5   enron4   \n",
              "6   enron1   \n",
              "7   enron2   \n",
              "8   enron4   \n",
              "9   enron1   \n",
              "10  enron2   \n",
              "11  enron4   \n",
              "\n",
              "                                                                    Parameters  \\\n",
              "0   {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "1   {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "2   {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "3       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "4       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "5       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "6    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "7    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "8    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "9        {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "10       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "11       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "\n",
              "    Accuracy    Recall  F1 Score  Precision Score  \n",
              "0   0.960526  0.966443  0.941176         0.917197  \n",
              "1   0.918410  0.761538  0.835443         0.925234  \n",
              "2   0.946593  1.000000  0.964242         0.930952  \n",
              "3   0.960526  0.966443  0.941176         0.917197  \n",
              "4   0.918410  0.761538  0.835443         0.925234  \n",
              "5   0.946593  1.000000  0.964242         0.930952  \n",
              "6   0.958333  0.966443  0.938111         0.911392  \n",
              "7   0.916318  0.753846  0.830508         0.924528  \n",
              "8   0.944751  1.000000  0.963054         0.928741  \n",
              "9   0.964912  0.986577  0.948387         0.913043  \n",
              "10  0.912134  0.738462  0.820513         0.923077  \n",
              "11  0.942910  1.000000  0.961870         0.926540  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c89651c-bbc3-4432-a769-9e39d01fdad4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Parameters</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Precision Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.966443</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.917197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.918410</td>\n",
              "      <td>0.761538</td>\n",
              "      <td>0.835443</td>\n",
              "      <td>0.925234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.946593</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.964242</td>\n",
              "      <td>0.930952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.960526</td>\n",
              "      <td>0.966443</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.917197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.918410</td>\n",
              "      <td>0.761538</td>\n",
              "      <td>0.835443</td>\n",
              "      <td>0.925234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.946593</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.964242</td>\n",
              "      <td>0.930952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.966443</td>\n",
              "      <td>0.938111</td>\n",
              "      <td>0.911392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.916318</td>\n",
              "      <td>0.753846</td>\n",
              "      <td>0.830508</td>\n",
              "      <td>0.924528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.944751</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963054</td>\n",
              "      <td>0.928741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.964912</td>\n",
              "      <td>0.986577</td>\n",
              "      <td>0.948387</td>\n",
              "      <td>0.913043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.912134</td>\n",
              "      <td>0.738462</td>\n",
              "      <td>0.820513</td>\n",
              "      <td>0.923077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.942910</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.961870</td>\n",
              "      <td>0.926540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c89651c-bbc3-4432-a769-9e39d01fdad4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c89651c-bbc3-4432-a769-9e39d01fdad4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c89651c-bbc3-4432-a769-9e39d01fdad4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7d1786eb-5afb-427b-babf-8884ecddb1ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d1786eb-5afb-427b-babf-8884ecddb1ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7d1786eb-5afb-427b-babf-8884ecddb1ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6f19a758-9621-4a7c-bc37-df9d37c31e28\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6f19a758-9621-4a7c-bc37-df9d37c31e28 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"enron1\",\n          \"enron2\",\n          \"enron4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parameters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01948877643959293,\n        \"min\": 0.9121338912133892,\n        \"max\": 0.9649122807017544,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.9121338912133892,\n          0.9184100418410042,\n          0.9447513812154696\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11508235778827783,\n        \"min\": 0.7384615384615385,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9664429530201343,\n          0.7615384615384615,\n          0.7384615384615385\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06104612767321872,\n        \"min\": 0.8205128205128206,\n        \"max\": 0.9642416769420469,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.8205128205128206,\n          0.8354430379746836,\n          0.9630541871921182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006641453610065824,\n        \"min\": 0.9113924050632911,\n        \"max\": 0.930952380952381,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.9230769230769231,\n          0.9252336448598131,\n          0.9287410926365796\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def predict_type(X_train, X_test, name, params):\n",
        "    y = X_train['Type']\n",
        "\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = LogisticRegression(**params)\n",
        "    model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "results = []\n",
        "\n",
        "parameters = [\n",
        "    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False},\n",
        "    {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False},\n",
        "    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True},\n",
        "    {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}\n",
        "]\n",
        "\n",
        "for params in parameters:\n",
        "    acc_e1, recall_e1, f1_e1, precision_e1 = predict_type(enron1_train_df, enron1_test_df, 'enron1', params)\n",
        "    results.append(['enron1', params, acc_e1, recall_e1, f1_e1, precision_e1])\n",
        "\n",
        "    acc_e2, recall_e2, f1_e2, precision_e2 = predict_type(enron2_train_df, enron2_test_df, 'enron2', params)\n",
        "    results.append(['enron2', params, acc_e2, recall_e2, f1_e2, precision_e2])\n",
        "\n",
        "    acc_e4, recall_e4, f1_e4, precision_e4 = predict_type(enron4_train_df, enron4_test_df, 'enron4', params)\n",
        "    results.append(['enron4', params, acc_e4, recall_e4, f1_e4, precision_e4])\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df_results = pd.DataFrame(results, columns=['Dataset', 'Parameters', 'Accuracy', 'Recall', 'F1 Score', 'Precision Score'])\n",
        "\n",
        "df_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F2WIaCcHwQX"
      },
      "source": [
        "## Hyperparameter Tuning LR (Bernoulli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "V-pT2NSfJgRi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "7ea199c5-77c0-4b3e-df4d-c14337394b27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Dataset  \\\n",
              "0   enron1   \n",
              "1   enron2   \n",
              "2   enron4   \n",
              "3   enron1   \n",
              "4   enron2   \n",
              "5   enron4   \n",
              "6   enron1   \n",
              "7   enron2   \n",
              "8   enron4   \n",
              "9   enron1   \n",
              "10  enron2   \n",
              "11  enron4   \n",
              "\n",
              "                                                                    Parameters  \\\n",
              "0   {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "1   {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "2   {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "3       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "4       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "5       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}   \n",
              "6    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "7    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "8    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "9        {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "10       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "11       {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}   \n",
              "\n",
              "    Accuracy    Recall  F1 Score  Precision Score  \n",
              "0   0.949561  0.879195  0.919298         0.963235  \n",
              "1   0.899582  0.715385  0.794872         0.894231  \n",
              "2   0.950276  1.000000  0.966625         0.935407  \n",
              "3   0.949561  0.879195  0.919298         0.963235  \n",
              "4   0.899582  0.715385  0.794872         0.894231  \n",
              "5   0.950276  1.000000  0.966625         0.935407  \n",
              "6   0.945175  0.865772  0.911661         0.962687  \n",
              "7   0.897490  0.707692  0.789700         0.893204  \n",
              "8   0.946593  1.000000  0.964242         0.930952  \n",
              "9   0.934211  0.832215  0.892086         0.961240  \n",
              "10  0.889121  0.676923  0.768559         0.888889  \n",
              "11  0.942910  1.000000  0.961870         0.926540  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8375ebd7-415c-4734-b48e-73d47d0746b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Parameters</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Precision Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.919298</td>\n",
              "      <td>0.963235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.899582</td>\n",
              "      <td>0.715385</td>\n",
              "      <td>0.794872</td>\n",
              "      <td>0.894231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.950276</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.966625</td>\n",
              "      <td>0.935407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.949561</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.919298</td>\n",
              "      <td>0.963235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.899582</td>\n",
              "      <td>0.715385</td>\n",
              "      <td>0.794872</td>\n",
              "      <td>0.894231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False}</td>\n",
              "      <td>0.950276</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.966625</td>\n",
              "      <td>0.935407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.945175</td>\n",
              "      <td>0.865772</td>\n",
              "      <td>0.911661</td>\n",
              "      <td>0.962687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.897490</td>\n",
              "      <td>0.707692</td>\n",
              "      <td>0.789700</td>\n",
              "      <td>0.893204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.946593</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.964242</td>\n",
              "      <td>0.930952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>enron1</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.934211</td>\n",
              "      <td>0.832215</td>\n",
              "      <td>0.892086</td>\n",
              "      <td>0.961240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>enron2</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.889121</td>\n",
              "      <td>0.676923</td>\n",
              "      <td>0.768559</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>enron4</td>\n",
              "      <td>{'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}</td>\n",
              "      <td>0.942910</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.961870</td>\n",
              "      <td>0.926540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8375ebd7-415c-4734-b48e-73d47d0746b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8375ebd7-415c-4734-b48e-73d47d0746b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8375ebd7-415c-4734-b48e-73d47d0746b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-08d4a1d1-da2d-421e-940c-cffe91819dd6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08d4a1d1-da2d-421e-940c-cffe91819dd6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-08d4a1d1-da2d-421e-940c-cffe91819dd6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a9410ee8-ca76-4054-ab32-f9ec01d0c2da\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a9410ee8-ca76-4054-ab32-f9ec01d0c2da button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"Dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"enron1\",\n          \"enron2\",\n          \"enron4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parameters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.024960776559285632,\n        \"min\": 0.8891213389121339,\n        \"max\": 0.9502762430939227,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.8891213389121339,\n          0.899581589958159,\n          0.9465930018416207\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.127311461845181,\n        \"min\": 0.676923076923077,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.8791946308724832,\n          0.7153846153846154,\n          0.8322147651006712\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07830052599739488,\n        \"min\": 0.7685589519650655,\n        \"max\": 0.9666254635352287,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.7685589519650655,\n          0.7948717948717949,\n          0.9642416769420469\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030027588609457926,\n        \"min\": 0.8888888888888888,\n        \"max\": 0.9632352941176471,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.8888888888888888,\n          0.8942307692307693,\n          0.930952380952381\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def predict_type(X_train, X_test, name, params):\n",
        "    y = X_train['Type']\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "    X = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = LogisticRegression(**params)\n",
        "    model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "results = []\n",
        "\n",
        "parameters = [\n",
        "    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': False},\n",
        "    {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': False},\n",
        "    {'C': 0.1, 'solver': 'liblinear', 'penalty': 'l2', 'fit_intercept': True},\n",
        "    {'C': 0.1, 'solver': 'lbfgs', 'penalty': 'l2', 'fit_intercept': True}\n",
        "]\n",
        "\n",
        "for params in parameters:\n",
        "    acc_e1, recall_e1, f1_e1, precision_e1 = predict_type(enron1_train_df, enron1_test_df, 'enron1', params)\n",
        "    results.append(['enron1', params, acc_e1, recall_e1, f1_e1, precision_e1])\n",
        "\n",
        "    acc_e2, recall_e2, f1_e2, precision_e2 = predict_type(enron2_train_df, enron2_test_df, 'enron2', params)\n",
        "    results.append(['enron2', params, acc_e2, recall_e2, f1_e2, precision_e2])\n",
        "\n",
        "    acc_e4, recall_e4, f1_e4, precision_e4 = predict_type(enron4_train_df, enron4_test_df, 'enron4', params)\n",
        "    results.append(['enron4', params, acc_e4, recall_e4, f1_e4, precision_e4])\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df_results = pd.DataFrame(results, columns=['Dataset', 'Parameters', 'Accuracy', 'Recall', 'F1 Score', 'Precision Score'])\n",
        "\n",
        "df_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1jpvmtdBA8h"
      },
      "source": [
        "### step by step approach BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM93XwqQ418R",
        "outputId": "31528e33-1ae0-4986-c243-e766d7471c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "λ: 1e-05, Validation Accuracy: 0.9333333333333333\n",
            "λ: 0.0001, Validation Accuracy: 0.9333333333333333\n",
            "λ: 0.001, Validation Accuracy: 0.9333333333333333\n",
            "λ: 0.01, Validation Accuracy: 0.9333333333333333\n",
            "λ: 0.1, Validation Accuracy: 0.9111111111111111\n",
            "Best λ: 1e-05\n",
            "enron1\n",
            "Accuracy = 94.95614035087719%\n",
            "Recall =  0.9194630872483222\n",
            "F1 =  0.9225589225589227\n",
            "Precision =  0.9256756756756757\n",
            "-----------------------------------------------------------\n",
            "λ: 1e-05, Validation Accuracy: 0.8920863309352518\n",
            "λ: 0.0001, Validation Accuracy: 0.8920863309352518\n",
            "λ: 0.001, Validation Accuracy: 0.8920863309352518\n",
            "λ: 0.01, Validation Accuracy: 0.8848920863309353\n",
            "λ: 0.1, Validation Accuracy: 0.8561151079136691\n",
            "Best λ: 1e-05\n",
            "enron2\n",
            "Accuracy = 90.3765690376569%\n",
            "Recall =  0.7153846153846154\n",
            "F1 =  0.8017241379310345\n",
            "Precision =  0.9117647058823529\n",
            "-----------------------------------------------------------\n",
            "λ: 1e-05, Validation Accuracy: 0.9503105590062112\n",
            "λ: 0.0001, Validation Accuracy: 0.9503105590062112\n",
            "λ: 0.001, Validation Accuracy: 0.9503105590062112\n",
            "λ: 0.01, Validation Accuracy: 0.9503105590062112\n",
            "λ: 0.1, Validation Accuracy: 0.9440993788819876\n",
            "Best λ: 1e-05\n",
            "enron4\n",
            "Accuracy = 94.47513812154696%\n",
            "Recall =  1.0\n",
            "F1 =  0.9630541871921182\n",
            "Precision =  0.9287410926365796\n",
            "-----------------------------------------------------------\n",
            "{'sklearn_mnb_bow_enron1': 0.9407894736842105, 'sklearn_mnb_bow_enron2': 0.9435146443514645, 'sklearn_mnb_bow_enron4': 0.9742173112338858, 'step_by_step_mnb_bow_enron1': 0.9385964912280702, 'step_by_step_mnb_bow_enron2': 0.9435146443514645, 'step_by_step_mnb_bow_enron4': 0.9539594843462247, 'sklearn_bnb_bern_enron1': 0.7302631578947368, 'sklearn_bnb_bern_enron2': 0.7740585774058577, 'sklearn_bnb_bern_enron4': 0.9171270718232044, 'step_by_step_bern_enron1': 0.7346491228070176, 'step_by_step_bern_enron2': 0.7782426778242678, 'step_by_step_bern_enron4': 0.9171270718232044, 'sklearn_lr_bow_enron1': 0.9649122807017544, 'sklearn_lr_bow_enron2': 0.9518828451882845, 'sklearn_lr_bow_enron4': 0.9465930018416207, 'step_by_step_lr_bow_enron1': 0.9495614035087719, 'step_by_step_lr_bow_enron2': 0.9037656903765691, 'step_by_step_lr_bow_enron4': 0.9447513812154696}\n",
            "{'sklearn_mnb_bow_enron1': 0.8859060402684564, 'sklearn_mnb_bow_enron2': 0.8615384615384616, 'sklearn_mnb_bow_enron4': 0.989769820971867, 'step_by_step_mnb_bow_enron1': 0.8590604026845637, 'step_by_step_mnb_bow_enron2': 0.8461538461538461, 'step_by_step_mnb_bow_enron4': 0.979539641943734, 'sklearn_bnb_bern_enron1': 0.19463087248322147, 'sklearn_bnb_bern_enron2': 0.19230769230769232, 'sklearn_bnb_bern_enron4': 1.0, 'step_by_step_bern_enron1': 0.2080536912751678, 'step_by_step_bern_enron2': 0.2076923076923077, 'step_by_step_bern_enron4': 1.0, 'sklearn_lr_bow_enron1': 0.9865771812080537, 'sklearn_lr_bow_enron2': 0.9153846153846154, 'sklearn_lr_bow_enron4': 0.9974424552429667, 'step_by_step_lr_bow_enron1': 0.9194630872483222, 'step_by_step_lr_bow_enron2': 0.7153846153846154, 'step_by_step_lr_bow_enron4': 1.0}\n",
            "{'sklearn_mnb_bow_enron1': 0.9072164948453608, 'sklearn_mnb_bow_enron2': 0.8924302788844622, 'sklearn_mnb_bow_enron4': 0.9822335025380711, 'step_by_step_mnb_bow_enron1': 0.9014084507042253, 'step_by_step_mnb_bow_enron2': 0.8906882591093118, 'step_by_step_mnb_bow_enron4': 0.9683944374209861, 'sklearn_bnb_bern_enron1': 0.3204419889502762, 'sklearn_bnb_bern_enron2': 0.31645569620253167, 'sklearn_bnb_bern_enron4': 0.9455864570737605, 'step_by_step_bern_enron1': 0.33879781420765026, 'step_by_step_bern_enron2': 0.3375, 'step_by_step_bern_enron4': 0.9455864570737605, 'sklearn_lr_bow_enron1': 0.9483870967741935, 'sklearn_lr_bow_enron2': 0.9118773946360154, 'sklearn_lr_bow_enron4': 0.9641532756489494, 'step_by_step_lr_bow_enron1': 0.9225589225589227, 'step_by_step_lr_bow_enron2': 0.8017241379310345, 'step_by_step_lr_bow_enron4': 0.9630541871921182}\n",
            "{'sklearn_mnb_bow_enron1': 0.9295774647887324, 'sklearn_mnb_bow_enron2': 0.9256198347107438, 'sklearn_mnb_bow_enron4': 0.9748110831234257, 'step_by_step_mnb_bow_enron1': 0.9481481481481482, 'step_by_step_mnb_bow_enron2': 0.9401709401709402, 'step_by_step_mnb_bow_enron4': 0.9575, 'sklearn_bnb_bern_enron1': 0.90625, 'sklearn_bnb_bern_enron2': 0.8928571428571429, 'sklearn_bnb_bern_enron4': 0.8967889908256881, 'step_by_step_bern_enron1': 0.9117647058823529, 'step_by_step_bern_enron2': 0.9, 'step_by_step_bern_enron4': 0.8967889908256881, 'sklearn_lr_bow_enron1': 0.9130434782608695, 'sklearn_lr_bow_enron2': 0.9083969465648855, 'sklearn_lr_bow_enron4': 0.9330143540669856, 'step_by_step_lr_bow_enron1': 0.9256756756756757, 'step_by_step_lr_bow_enron2': 0.9117647058823529, 'step_by_step_lr_bow_enron4': 0.9287410926365796}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return (np.exp(z)) / (1 + np.exp(z))\n",
        "\n",
        "# MCAP Logistic Regression training with L2 Regularization\n",
        "def train_mcap_logistic_regression(X, y, learning_rate=0.01, lambda_=0.1, iterations=2000):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)  # Initialize weights to zero\n",
        "    bias = 0  # Initialize bias\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Compute linear combination\n",
        "        z = np.dot(X, weights) + bias\n",
        "\n",
        "        # Compute predicted probabilities\n",
        "        predictions = sigmoid(z)\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = (1 / n_samples) * np.dot(X.T, (predictions - y)) + lambda_ * weights\n",
        "        db = (1 / n_samples) * np.sum(predictions - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Predict function\n",
        "def predict(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    predictions = sigmoid(z)\n",
        "    return [1 if p > 0.5 else 0 for p in predictions]\n",
        "\n",
        "def logistic_regression(train, test, name):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(train['Email_str']).toarray()\n",
        "    y = train['Type'].values\n",
        "\n",
        "    # Label encoding for the target variable\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)  # Convert 'ham' and 'spam' to 0 and 1\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
        "    best_lambda = lambdas[0]\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for lambda_ in lambdas:\n",
        "        weights, bias = train_mcap_logistic_regression(X_train, y_train, learning_rate=0.01, lambda_=lambda_, iterations=2000)\n",
        "\n",
        "        y_val_pred = predict(X_val, weights, bias)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "        print(f\"λ: {lambda_}, Validation Accuracy: {accuracy}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_lambda = lambda_\n",
        "\n",
        "    print(f\"Best λ: {best_lambda}\")\n",
        "\n",
        "    # Test data\n",
        "    X_test = vectorizer.transform(test['Email_str']).toarray()\n",
        "    y_test = test['Type'].values\n",
        "    y_test = le.transform(y_test)  # Encode test labels as 0 and 1\n",
        "\n",
        "    weights, bias = train_mcap_logistic_regression(X, y, learning_rate=0.01, lambda_=best_lambda, iterations=2000)\n",
        "\n",
        "    y_test_pred = predict(X_test, weights, bias)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    recall = recall_score(y_test, y_test_pred, pos_label=1)  # 1 represents 'spam'\n",
        "    f1 = f1_score(y_test, y_test_pred, pos_label=1)\n",
        "    precision = precision_score(y_test, y_test_pred, pos_label=1)\n",
        "\n",
        "    # Print the results\n",
        "    print(name)\n",
        "    print(f\"Accuracy = {accuracy * 100}%\")\n",
        "    print(\"Recall = \", recall)\n",
        "    print(\"F1 = \", f1)\n",
        "    print(\"Precision = \", precision)\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "# Example usage with datasets\n",
        "all_results_accuracy['step_by_step_lr_bow_enron1'], all_results_recall['step_by_step_lr_bow_enron1'], all_results_f1['step_by_step_lr_bow_enron1'], all_results_precision['step_by_step_lr_bow_enron1'] = logistic_regression(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['step_by_step_lr_bow_enron2'], all_results_recall['step_by_step_lr_bow_enron2'], all_results_f1['step_by_step_lr_bow_enron2'], all_results_precision['step_by_step_lr_bow_enron2'] = logistic_regression(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['step_by_step_lr_bow_enron4'], all_results_recall['step_by_step_lr_bow_enron4'], all_results_f1['step_by_step_lr_bow_enron4'], all_results_precision['step_by_step_lr_bow_enron4'] = logistic_regression(enron4_train_df, enron4_test_df, 'enron4')\n",
        "\n",
        "print(all_results_accuracy)\n",
        "print(all_results_recall)\n",
        "print(all_results_f1)\n",
        "print(all_results_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Gtt-FzUA_j6_"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['sklearn_lr_enron1'] = sklearn_lr_acc_e1\n",
        "# all_results_accuracy['sklearn_lr_enron2'] = sklearn_lr_acc_e2\n",
        "# all_results_accuracy['sklearn_lr_enron4'] = sklearn_lr_acc_e4\n",
        "\n",
        "# all_results_recall['sklearn_lr_enron1'] = sklearn_lr_recall_e1\n",
        "# all_results_recall['sklearn_lr_enron2'] = sklearn_lr_recall_e2\n",
        "# all_results_recall['sklearn_lr_enron4'] = sklearn_lr_recall_e4\n",
        "\n",
        "# all_results_f1['sklearn_lr_enron1'] = sklearn_lr_f1_e1\n",
        "# all_results_f1['sklearn_lr_enron2'] = sklearn_lr_f1_e2\n",
        "# all_results_f1['sklearn_lr_enron4'] = sklearn_lr_f1_e4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "eBTnh2mi5t8d"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['step_by_step_lr_enron1'] = accuracy_score_e1\n",
        "# all_results_accuracy['step_by_step_lr_enron2'] = accuracy_score_e2\n",
        "# all_results_accuracy['step_by_step_lr_enron4'] = accuracy_score_e4\n",
        "\n",
        "# all_results_f1['step_by_step_lr_enron1'] = f1_e1\n",
        "# all_results_f1['step_by_step_lr_enron2'] = f1_e2\n",
        "# all_results_f1['step_by_step_lr_enron4'] = f1_e4\n",
        "\n",
        "# all_results_recall['step_by_step_lr_enron1'] = recall_e1\n",
        "# all_results_recall['step_by_step_lr_enron2'] = recall_e2\n",
        "# all_results_recall['step_by_step_lr_enron4'] = recall_e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgTnrJhB8dM"
      },
      "source": [
        "### Scikit learn approach Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieHZROg6BsGT",
        "outputId": "a0cb775b-5d52-4c4f-977e-001ab827c260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1\n",
            "accuracy - 0.9649122807017544\n",
            "recall - 0.9664429530201343\n",
            "f1 score - 0.9473684210526316\n",
            "precision - 0.9290322580645162\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.96      0.97       307\n",
            "        spam       0.93      0.97      0.95       149\n",
            "\n",
            "    accuracy                           0.96       456\n",
            "   macro avg       0.96      0.97      0.96       456\n",
            "weighted avg       0.97      0.96      0.97       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron2\n",
            "accuracy - 0.9497907949790795\n",
            "recall - 0.9\n",
            "f1 score - 0.9069767441860466\n",
            "precision - 0.9140625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      0.97      0.97       348\n",
            "        spam       0.91      0.90      0.91       130\n",
            "\n",
            "    accuracy                           0.95       478\n",
            "   macro avg       0.94      0.93      0.94       478\n",
            "weighted avg       0.95      0.95      0.95       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "enron4\n",
            "accuracy - 0.9558011049723757\n",
            "recall - 1.0\n",
            "f1 score - 0.9702233250620347\n",
            "precision - 0.9421686746987952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.84      0.91       152\n",
            "        spam       0.94      1.00      0.97       391\n",
            "\n",
            "    accuracy                           0.96       543\n",
            "   macro avg       0.97      0.92      0.94       543\n",
            "weighted avg       0.96      0.96      0.95       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "{'sklearn_mnb_bow_enron1': 0.9407894736842105, 'sklearn_mnb_bow_enron2': 0.9435146443514645, 'sklearn_mnb_bow_enron4': 0.9742173112338858, 'step_by_step_mnb_bow_enron1': 0.9385964912280702, 'step_by_step_mnb_bow_enron2': 0.9435146443514645, 'step_by_step_mnb_bow_enron4': 0.9539594843462247, 'sklearn_bnb_bern_enron1': 0.7302631578947368, 'sklearn_bnb_bern_enron2': 0.7740585774058577, 'sklearn_bnb_bern_enron4': 0.9171270718232044, 'step_by_step_bern_enron1': 0.7346491228070176, 'step_by_step_bern_enron2': 0.7782426778242678, 'step_by_step_bern_enron4': 0.9171270718232044, 'sklearn_lr_bow_enron1': 0.9649122807017544, 'sklearn_lr_bow_enron2': 0.9518828451882845, 'sklearn_lr_bow_enron4': 0.9465930018416207, 'step_by_step_lr_bow_enron1': 0.9495614035087719, 'step_by_step_lr_bow_enron2': 0.9037656903765691, 'step_by_step_lr_bow_enron4': 0.9447513812154696, 'sklearn_lr_bern_enron1': 0.9649122807017544, 'sklearn_lr_bern_enron2': 0.9497907949790795, 'sklearn_lr_bern_enron4': 0.9558011049723757}\n",
            "{'sklearn_mnb_bow_enron1': 0.8859060402684564, 'sklearn_mnb_bow_enron2': 0.8615384615384616, 'sklearn_mnb_bow_enron4': 0.989769820971867, 'step_by_step_mnb_bow_enron1': 0.8590604026845637, 'step_by_step_mnb_bow_enron2': 0.8461538461538461, 'step_by_step_mnb_bow_enron4': 0.979539641943734, 'sklearn_bnb_bern_enron1': 0.19463087248322147, 'sklearn_bnb_bern_enron2': 0.19230769230769232, 'sklearn_bnb_bern_enron4': 1.0, 'step_by_step_bern_enron1': 0.2080536912751678, 'step_by_step_bern_enron2': 0.2076923076923077, 'step_by_step_bern_enron4': 1.0, 'sklearn_lr_bow_enron1': 0.9865771812080537, 'sklearn_lr_bow_enron2': 0.9153846153846154, 'sklearn_lr_bow_enron4': 0.9974424552429667, 'step_by_step_lr_bow_enron1': 0.9194630872483222, 'step_by_step_lr_bow_enron2': 0.7153846153846154, 'step_by_step_lr_bow_enron4': 1.0, 'sklearn_lr_bern_enron1': 0.9664429530201343, 'sklearn_lr_bern_enron2': 0.9, 'sklearn_lr_bern_enron4': 1.0}\n",
            "{'sklearn_mnb_bow_enron1': 0.9072164948453608, 'sklearn_mnb_bow_enron2': 0.8924302788844622, 'sklearn_mnb_bow_enron4': 0.9822335025380711, 'step_by_step_mnb_bow_enron1': 0.9014084507042253, 'step_by_step_mnb_bow_enron2': 0.8906882591093118, 'step_by_step_mnb_bow_enron4': 0.9683944374209861, 'sklearn_bnb_bern_enron1': 0.3204419889502762, 'sklearn_bnb_bern_enron2': 0.31645569620253167, 'sklearn_bnb_bern_enron4': 0.9455864570737605, 'step_by_step_bern_enron1': 0.33879781420765026, 'step_by_step_bern_enron2': 0.3375, 'step_by_step_bern_enron4': 0.9455864570737605, 'sklearn_lr_bow_enron1': 0.9483870967741935, 'sklearn_lr_bow_enron2': 0.9118773946360154, 'sklearn_lr_bow_enron4': 0.9641532756489494, 'step_by_step_lr_bow_enron1': 0.9225589225589227, 'step_by_step_lr_bow_enron2': 0.8017241379310345, 'step_by_step_lr_bow_enron4': 0.9630541871921182, 'sklearn_lr_bern_enron1': 0.9290322580645162, 'sklearn_lr_bern_enron2': 0.9140625, 'sklearn_lr_bern_enron4': 0.9421686746987952}\n",
            "{'sklearn_mnb_bow_enron1': 0.9295774647887324, 'sklearn_mnb_bow_enron2': 0.9256198347107438, 'sklearn_mnb_bow_enron4': 0.9748110831234257, 'step_by_step_mnb_bow_enron1': 0.9481481481481482, 'step_by_step_mnb_bow_enron2': 0.9401709401709402, 'step_by_step_mnb_bow_enron4': 0.9575, 'sklearn_bnb_bern_enron1': 0.90625, 'sklearn_bnb_bern_enron2': 0.8928571428571429, 'sklearn_bnb_bern_enron4': 0.8967889908256881, 'step_by_step_bern_enron1': 0.9117647058823529, 'step_by_step_bern_enron2': 0.9, 'step_by_step_bern_enron4': 0.8967889908256881, 'sklearn_lr_bow_enron1': 0.9130434782608695, 'sklearn_lr_bow_enron2': 0.9083969465648855, 'sklearn_lr_bow_enron4': 0.9330143540669856, 'step_by_step_lr_bow_enron1': 0.9256756756756757, 'step_by_step_lr_bow_enron2': 0.9117647058823529, 'step_by_step_lr_bow_enron4': 0.9287410926365796, 'sklearn_lr_bern_enron1': 0.9473684210526316, 'sklearn_lr_bern_enron2': 0.9069767441860466, 'sklearn_lr_bern_enron4': 0.9702233250620347}\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    y = X_train['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "\n",
        "    X = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    lambda_values = {'C': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "    grid_search = GridSearchCV(model, lambda_values, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_C = grid_search.best_params_['C']\n",
        "    new_model = LogisticRegression(C=best_C)\n",
        "    new_model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    # X_test['Email_str'] = X_test['Email'].apply(lambda x: ' '.join(x))\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    y_pred = new_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "    return accuracy, recall, f1 , precision\n",
        "\n",
        "all_results_accuracy['sklearn_lr_bern_enron1'], all_results_recall['sklearn_lr_bern_enron1'], all_results_precision['sklearn_lr_bern_enron1'], all_results_f1['sklearn_lr_bern_enron1'] = predict_type(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['sklearn_lr_bern_enron2'], all_results_recall['sklearn_lr_bern_enron2'], all_results_precision['sklearn_lr_bern_enron2'], all_results_f1['sklearn_lr_bern_enron2'] = predict_type(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['sklearn_lr_bern_enron4'], all_results_recall['sklearn_lr_bern_enron4'], all_results_precision['sklearn_lr_bern_enron4'], all_results_f1['sklearn_lr_bern_enron4'] = predict_type(enron4_train_df, enron4_test_df, 'enron4')\n",
        "\n",
        "print(all_results_accuracy)\n",
        "print(all_results_recall)\n",
        "print(all_results_f1)\n",
        "print(all_results_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "X9Xpvn0-CF7b"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['sklearn_lr_bern_enron1'] = sklearn_lr_acc_e1\n",
        "# all_results_accuracy['sklearn_lr_bern_enron2'] = sklearn_lr_acc_e2\n",
        "# all_results_accuracy['sklearn_lr_bern_enron4'] = sklearn_lr_acc_e4\n",
        "\n",
        "# all_results_recall['sklearn_lr_bern_enron1'] = sklearn_lr_recall_e1\n",
        "# all_results_recall['sklearn_lr_bern_enron2'] = sklearn_lr_recall_e2\n",
        "# all_results_recall['sklearn_lr_bern_enron4'] = sklearn_lr_recall_e4\n",
        "\n",
        "# all_results_f1['sklearn_lr_bern_enron1'] = sklearn_lr_f1_e1\n",
        "# all_results_f1['sklearn_lr_bern_enron2'] = sklearn_lr_f1_e2\n",
        "# all_results_f1['sklearn_lr_bern_enron4'] = sklearn_lr_f1_e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8m7kOdCFkD"
      },
      "source": [
        "### step by step Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "z5bM1Ef9B60L",
        "outputId": "8908f965-7581-4fe3-b6ef-30fbac09c897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "λ: 1e-05, Validation Accuracy: 0.9259259259259259\n",
            "λ: 0.0001, Validation Accuracy: 0.9259259259259259\n",
            "λ: 0.001, Validation Accuracy: 0.9259259259259259\n",
            "λ: 0.01, Validation Accuracy: 0.9259259259259259\n",
            "λ: 0.1, Validation Accuracy: 0.9037037037037037\n",
            "Best λ: 1e-05\n",
            "enron1\n",
            "Accuracy = 92.76315789473685%\n",
            "Recall =  0.8120805369127517\n",
            "F1 =  0.8800000000000001\n",
            "Precision =  0.9603174603174603\n",
            "-----------------------------------------------------------\n",
            "λ: 1e-05, Validation Accuracy: 0.8561151079136691\n",
            "λ: 0.0001, Validation Accuracy: 0.8561151079136691\n",
            "λ: 0.001, Validation Accuracy: 0.8561151079136691\n",
            "λ: 0.01, Validation Accuracy: 0.8561151079136691\n",
            "λ: 0.1, Validation Accuracy: 0.8273381294964028\n",
            "Best λ: 1e-05\n",
            "enron2\n",
            "Accuracy = 88.49372384937239%\n",
            "Recall =  0.6538461538461539\n",
            "F1 =  0.7555555555555555\n",
            "Precision =  0.8947368421052632\n",
            "-----------------------------------------------------------\n",
            "λ: 1e-05, Validation Accuracy: 0.9440993788819876\n",
            "λ: 0.0001, Validation Accuracy: 0.9440993788819876\n",
            "λ: 0.001, Validation Accuracy: 0.9440993788819876\n",
            "λ: 0.01, Validation Accuracy: 0.9440993788819876\n",
            "λ: 0.1, Validation Accuracy: 0.9254658385093167\n",
            "Best λ: 1e-05\n",
            "enron4\n",
            "Accuracy = 94.10681399631676%\n",
            "Recall =  1.0\n",
            "F1 =  0.9606879606879607\n",
            "Precision =  0.9243498817966903\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return (np.exp(z)) / (1 + np.exp(z))\n",
        "\n",
        "# MCAP Logistic Regression training with L2 Regularization\n",
        "def train_mcap_logistic_regression(X, y, learning_rate=0.01, lambda_=0.1, iterations=2000):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)  # Initialize weights to zero\n",
        "    bias = 0  # Initialize bias\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Compute linear combination\n",
        "        z = np.dot(X, weights) + bias\n",
        "\n",
        "        # Compute predicted probabilities\n",
        "        predictions = sigmoid(z)\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = (1 / n_samples) * np.dot(X.T, (predictions - y)) + lambda_ * weights\n",
        "        db = (1 / n_samples) * np.sum(predictions - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Predict function\n",
        "def predict(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    predictions = sigmoid(z)\n",
        "    return [1 if p > 0.5 else 0 for p in predictions]\n",
        "\n",
        "def logistic_regression(train, test, name):\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "    X = vectorizer.fit_transform(train['Email_str']).toarray()\n",
        "    y = train['Type'].values\n",
        "\n",
        "    # Label encoding for the target variable\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)  # Convert 'ham' and 'spam' to 0 and 1\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
        "    best_lambda = lambdas[0]\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for lambda_ in lambdas:\n",
        "        weights, bias = train_mcap_logistic_regression(X_train, y_train, learning_rate=0.01, lambda_=lambda_, iterations=2000)\n",
        "\n",
        "        y_val_pred = predict(X_val, weights, bias)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "        print(f\"λ: {lambda_}, Validation Accuracy: {accuracy}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_lambda = lambda_\n",
        "\n",
        "    print(f\"Best λ: {best_lambda}\")\n",
        "\n",
        "    # Test data\n",
        "    X_test = vectorizer.transform(test['Email_str']).toarray()\n",
        "    y_test = test['Type'].values\n",
        "    y_test = le.transform(y_test)  # Encode test labels as 0 and 1\n",
        "\n",
        "    weights, bias = train_mcap_logistic_regression(X, y, learning_rate=0.01, lambda_=best_lambda, iterations=2000)\n",
        "\n",
        "    y_test_pred = predict(X_test, weights, bias)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    recall = recall_score(y_test, y_test_pred, pos_label=1)  # 1 represents 'spam'\n",
        "    f1 = f1_score(y_test, y_test_pred, pos_label=1)\n",
        "    precision = precision_score(y_test, y_test_pred, pos_label=1)\n",
        "\n",
        "    # Print the results\n",
        "    print(name)\n",
        "    print(f\"Accuracy = {accuracy * 100}%\")\n",
        "    print(\"Recall = \", recall)\n",
        "    print(\"F1 = \", f1)\n",
        "    print(\"Precision = \", precision)\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "# Example usage with datasets\n",
        "all_results_accuracy['step_by_step_lr_bern_enron1'], all_results_recall['step_by_step_lr_bern_enron1'], all_results_f1['step_by_step_lr_bern_enron1'], all_results_precision['step_by_step_lr_bern_enron1'] = logistic_regression(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['step_by_step_lr_bern_enron2'], all_results_recall['step_by_step_lr_bern_enron2'], all_results_f1['step_by_step_lr_bern_enron2'], all_results_precision['step_by_step_lr_bern_enron2'] = logistic_regression(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['step_by_step_lr_bern_enron4'], all_results_recall['step_by_step_lr_bern_enron4'], all_results_f1['step_by_step_lr_bern_enron4'], all_results_precision['step_by_step_lr_bern_enron4'] = logistic_regression(enron4_train_df, enron4_test_df, 'enron4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4OBsi6WBCcVc"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['step_by_step_lr_bern_enron1'] = accuracy_score_e1\n",
        "# all_results_accuracy['step_by_step_lr_bern_enron2'] = accuracy_score_e2\n",
        "# all_results_accuracy['step_by_step_lr_bern_enron4'] = accuracy_score_e4\n",
        "\n",
        "# all_results_recall['step_by_step_lr_bern_enron1'] = recall_e1\n",
        "# all_results_recall['step_by_step_lr_bern_enron2'] = recall_e2\n",
        "# all_results_recall['step_by_step_lr_bern_enron4'] = recall_e4\n",
        "\n",
        "# all_results_f1['step_by_step_lr_bern_enron1'] = f1_e1\n",
        "# all_results_f1['step_by_step_lr_bern_enron2'] = f1_e2\n",
        "# all_results_f1['step_by_step_lr_bern_enron4'] = f1_e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px2m0gMhHa0o"
      },
      "source": [
        "## SGDClassifier BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "f6O8xs0JGuMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c1ecb5-897d-48cd-936e-09de2d256786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "42240 fits failed out of a total of 57600.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30720 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 905, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 153, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "11520 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'log_loss', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log-losss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'alpha': 0.1, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 800, 'penalty': 'l2', 'validation_fraction': 0.3, 'warm_start': True}\n",
            "Validation Accuracy: 0.9629629629629629\n",
            "enron1\n",
            "accuracy - 0.9583333333333334\n",
            "recall - 0.9798657718120806\n",
            "f1 score - 0.9389067524115756\n",
            "precision - 0.9012345679012346\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.95      0.97       307\n",
            "        spam       0.90      0.98      0.94       149\n",
            "\n",
            "    accuracy                           0.96       456\n",
            "   macro avg       0.95      0.96      0.95       456\n",
            "weighted avg       0.96      0.96      0.96       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "42240 fits failed out of a total of 57600.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30720 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 905, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 153, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "11520 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'log_loss', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log-losss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'alpha': 0.01, 'early_stopping': True, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'validation_fraction': 0.1, 'warm_start': True}\n",
            "Validation Accuracy: 0.920863309352518\n",
            "enron2\n",
            "accuracy - 0.9497907949790795\n",
            "recall - 0.9230769230769231\n",
            "f1 score - 0.9090909090909091\n",
            "precision - 0.8955223880597015\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.96      0.97       348\n",
            "        spam       0.90      0.92      0.91       130\n",
            "\n",
            "    accuracy                           0.95       478\n",
            "   macro avg       0.93      0.94      0.94       478\n",
            "weighted avg       0.95      0.95      0.95       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "42240 fits failed out of a total of 57600.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30720 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 905, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 153, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "11520 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'log_loss', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log-losss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'alpha': 0.001, 'early_stopping': True, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 800, 'penalty': 'elasticnet', 'validation_fraction': 0.3, 'warm_start': False}\n",
            "Validation Accuracy: 0.9316770186335404\n",
            "enron4\n",
            "accuracy - 0.9429097605893186\n",
            "recall - 0.9718670076726342\n",
            "f1 score - 0.9608091024020227\n",
            "precision - 0.95\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.92      0.87      0.89       152\n",
            "        spam       0.95      0.97      0.96       391\n",
            "\n",
            "    accuracy                           0.94       543\n",
            "   macro avg       0.94      0.92      0.93       543\n",
            "weighted avg       0.94      0.94      0.94       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    y = X_train['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "    X = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = SGDClassifier()\n",
        "    param_values = {'loss': ['hinge', 'log-losss','modified_huber', 'squared_hinge', 'perceptron'],\n",
        "                    'penalty': ['l2', 'l1', 'elasticnet', None],\n",
        "                    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "                    'max_iter':[200, 400, 800, 1000],\n",
        "                    'learning_rate': ['constant', 'optimal', 'adaptive'],\n",
        "                    'early_stopping': [True, False],\n",
        "                    'validation_fraction': [0.1, 0.2, 0.3],\n",
        "                    'warm_start': [True, False]}\n",
        "\n",
        "    grid_search = GridSearchCV(model, param_values, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "    y_val_pred = best_model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "    new_model = SGDClassifier(**best_params)\n",
        "    new_model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    # X_test['Email_str'] = X_test['Email'].apply(lambda x: ' '.join(x))\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    y_pred = new_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "\n",
        "all_results_accuracy['sgd_bow_enron1'], all_results_recall['sgd_bow_enron1'], all_results_f1['sgd_bow_enron1'], all_results_precision['sgd_bow_enron1'] = predict_type(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['sgd_bow_enron2'], all_results_recall['sgd_bow_enron2'], all_results_f1['sgd_bow_enron2'], all_results_precision['sgd_bow_enron2'] = predict_type(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['sgd_bow_enron4'], all_results_recall['sgd_bow_enron4'], all_results_f1['sgd_bow_enron4'], all_results_precision['sgd_bow_enron4'] = predict_type(enron4_train_df, enron4_test_df, 'enron4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0o1XdcnIY_i"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['sgd_bow_enron1'] = accuracy_score_e1\n",
        "# all_results_accuracy['sgd_bow_enron2'] = accuracy_score_e2\n",
        "# all_results_accuracy['sgd_bow_enron4'] = accuracy_score_e4\n",
        "\n",
        "# all_results_recall['sgd_bow_enron1'] = recall_e1\n",
        "# all_results_recall['sgd_bow_enron2'] = recall_e2\n",
        "# all_results_recall['sgd_bow_enron4'] = recall_e4\n",
        "\n",
        "# all_results_f1['sgd_bow_enron1'] = f1_e1\n",
        "# all_results_f1['sgd_bow_enron2'] = f1_e2\n",
        "# all_results_f1['sgd_bow_enron4'] = f1_e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w16Lr7wk66OL"
      },
      "source": [
        "## SGDClassifier Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ilNGFFH6I77p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e779779-2255-4a61-bad3-a5fcaa891123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "42240 fits failed out of a total of 57600.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30720 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 905, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 153, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "11520 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'log_loss', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log-losss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'alpha': 0.01, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 800, 'penalty': 'l2', 'validation_fraction': 0.2, 'warm_start': False}\n",
            "Validation Accuracy: 0.9629629629629629\n",
            "enron1\n",
            "accuracy - 0.9627192982456141\n",
            "recall - 0.9865771812080537\n",
            "f1 score - 0.9453376205787781\n",
            "precision - 0.9074074074074074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.95      0.97       307\n",
            "        spam       0.91      0.99      0.95       149\n",
            "\n",
            "    accuracy                           0.96       456\n",
            "   macro avg       0.95      0.97      0.96       456\n",
            "weighted avg       0.97      0.96      0.96       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "42240 fits failed out of a total of 57600.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30720 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 905, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 153, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "11520 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'log_loss', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log-losss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'alpha': 0.01, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 400, 'penalty': 'elasticnet', 'validation_fraction': 0.3, 'warm_start': True}\n",
            "Validation Accuracy: 0.935251798561151\n",
            "enron2\n",
            "accuracy - 0.9560669456066946\n",
            "recall - 0.9692307692307692\n",
            "f1 score - 0.9230769230769231\n",
            "precision - 0.8811188811188811\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.95      0.97       348\n",
            "        spam       0.88      0.97      0.92       130\n",
            "\n",
            "    accuracy                           0.96       478\n",
            "   macro avg       0.93      0.96      0.95       478\n",
            "weighted avg       0.96      0.96      0.96       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "42240 fits failed out of a total of 57600.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30720 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 905, in fit\n",
            "    self._more_validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\", line 153, in _more_validate_params\n",
            "    raise ValueError(\"eta0 must be > 0\")\n",
            "ValueError: eta0 must be > 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "11520 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'log_loss', 'hinge', 'perceptron', 'huber', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log-losss' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'alpha': 0.001, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'squared_hinge', 'max_iter': 800, 'penalty': 'l2', 'validation_fraction': 0.3, 'warm_start': True}\n",
            "Validation Accuracy: 0.9751552795031055\n",
            "enron4\n",
            "accuracy - 0.9429097605893186\n",
            "recall - 1.0\n",
            "f1 score - 0.9618696186961869\n",
            "precision - 0.9265402843601895\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.80      0.89       152\n",
            "        spam       0.93      1.00      0.96       391\n",
            "\n",
            "    accuracy                           0.94       543\n",
            "   macro avg       0.96      0.90      0.92       543\n",
            "weighted avg       0.95      0.94      0.94       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    y = X_train['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
        "\n",
        "    X = vectorizer.fit_transform(X_train['Email_str'])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = SGDClassifier()\n",
        "    param_values = {'loss': ['hinge', 'log-losss','modified_huber', 'squared_hinge', 'perceptron'],\n",
        "                    'penalty': ['l2', 'l1', 'elasticnet', None],\n",
        "                    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "                    'max_iter':[200, 400, 800, 1000],\n",
        "                    'learning_rate': ['constant', 'optimal', 'adaptive'],\n",
        "                    'early_stopping': [True, False],\n",
        "                    'validation_fraction': [0.1, 0.2, 0.3],\n",
        "                    'warm_start': [True, False]}\n",
        "\n",
        "    grid_search = GridSearchCV(model, param_values, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "    y_val_pred = best_model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "    new_model = SGDClassifier(**best_params)\n",
        "    new_model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_test = X_test['Type']\n",
        "    # X_test['Email_str'] = X_test['Email'].apply(lambda x: ' '.join(x))\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    y_pred = new_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "    return accuracy, recall, f1, precision\n",
        "\n",
        "\n",
        "all_results_accuracy['sgd_bern_enron1'], all_results_recall['sgd_bern_enron1'], all_results_f1['sgd_bern_enron1'], all_results_precision['sgd_bern_enron1'] = predict_type(enron1_train_df, enron1_test_df, 'enron1')\n",
        "all_results_accuracy['sgd_bern_enron2'], all_results_recall['sgd_bern_enron2'], all_results_f1['sgd_bern_enron2'], all_results_precision['sgd_bern_enron2'] = predict_type(enron2_train_df, enron2_test_df, 'enron2')\n",
        "all_results_accuracy['sgd_bern_enron4'], all_results_recall['sgd_bern_enron4'], all_results_f1['sgd_bern_enron4'], all_results_precision['sgd_bern_enron4'] = predict_type(enron4_train_df, enron4_test_df, 'enron4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2gD-TreDNV0y"
      },
      "outputs": [],
      "source": [
        "# all_results_accuracy['sgd_bern_enron1'] = accuracy_score_e1\n",
        "# all_results_accuracy['sgd_bern_enron2'] = accuracy_score_e2\n",
        "# all_results_accuracy['sgd_bern_enron4'] = accuracy_score_e4\n",
        "\n",
        "# all_results_recall['sgd_bern_enron1'] = recall_e1\n",
        "# all_results_recall['sgd_bern_enron2'] = recall_e2\n",
        "# all_results_recall['sgd_bern_enron4'] = recall_e4\n",
        "\n",
        "# all_results_f1['sgd_bern_enron1'] = f1_e1\n",
        "# all_results_f1['sgd_bern_enron2'] = f1_e2\n",
        "# all_results_f1['sgd_bern_enron4'] = f1_e4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "QzpGVohQbYzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33edcaf-5bcd-4423-e40d-34195f797865"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sklearn_mnb_bow_enron1': 0.9407894736842105,\n",
              " 'sklearn_mnb_bow_enron2': 0.9435146443514645,\n",
              " 'sklearn_mnb_bow_enron4': 0.9742173112338858,\n",
              " 'step_by_step_mnb_bow_enron1': 0.9385964912280702,\n",
              " 'step_by_step_mnb_bow_enron2': 0.9435146443514645,\n",
              " 'step_by_step_mnb_bow_enron4': 0.9539594843462247,\n",
              " 'sklearn_bnb_bern_enron1': 0.7302631578947368,\n",
              " 'sklearn_bnb_bern_enron2': 0.7740585774058577,\n",
              " 'sklearn_bnb_bern_enron4': 0.9171270718232044,\n",
              " 'step_by_step_bern_enron1': 0.7346491228070176,\n",
              " 'step_by_step_bern_enron2': 0.7782426778242678,\n",
              " 'step_by_step_bern_enron4': 0.9171270718232044,\n",
              " 'sklearn_lr_bow_enron1': 0.9649122807017544,\n",
              " 'sklearn_lr_bow_enron2': 0.9518828451882845,\n",
              " 'sklearn_lr_bow_enron4': 0.9465930018416207,\n",
              " 'step_by_step_lr_bow_enron1': 0.9495614035087719,\n",
              " 'step_by_step_lr_bow_enron2': 0.9037656903765691,\n",
              " 'step_by_step_lr_bow_enron4': 0.9447513812154696,\n",
              " 'sklearn_lr_bern_enron1': 0.9649122807017544,\n",
              " 'sklearn_lr_bern_enron2': 0.9497907949790795,\n",
              " 'sklearn_lr_bern_enron4': 0.9558011049723757,\n",
              " 'step_by_step_lr_bern_enron1': 0.9276315789473685,\n",
              " 'step_by_step_lr_bern_enron2': 0.8849372384937239,\n",
              " 'step_by_step_lr_bern_enron4': 0.9410681399631676,\n",
              " 'sgd_bow_enron1': 0.9583333333333334,\n",
              " 'sgd_bow_enron2': 0.9497907949790795,\n",
              " 'sgd_bow_enron4': 0.9429097605893186,\n",
              " 'sgd_bern_enron1': 0.9627192982456141,\n",
              " 'sgd_bern_enron2': 0.9560669456066946,\n",
              " 'sgd_bern_enron4': 0.9429097605893186}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "acc_df = pd.DataFrameall_results_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "zo-SyYOt4uV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb99be0c-7056-463a-ca84-e88b297f9896"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sklearn_mnb_bow_enron1': 0.8859060402684564,\n",
              " 'sklearn_mnb_bow_enron2': 0.8615384615384616,\n",
              " 'sklearn_mnb_bow_enron4': 0.989769820971867,\n",
              " 'step_by_step_mnb_bow_enron1': 0.8590604026845637,\n",
              " 'step_by_step_mnb_bow_enron2': 0.8461538461538461,\n",
              " 'step_by_step_mnb_bow_enron4': 0.979539641943734,\n",
              " 'sklearn_bnb_bern_enron1': 0.19463087248322147,\n",
              " 'sklearn_bnb_bern_enron2': 0.19230769230769232,\n",
              " 'sklearn_bnb_bern_enron4': 1.0,\n",
              " 'step_by_step_bern_enron1': 0.2080536912751678,\n",
              " 'step_by_step_bern_enron2': 0.2076923076923077,\n",
              " 'step_by_step_bern_enron4': 1.0,\n",
              " 'sklearn_lr_bow_enron1': 0.9865771812080537,\n",
              " 'sklearn_lr_bow_enron2': 0.9153846153846154,\n",
              " 'sklearn_lr_bow_enron4': 0.9974424552429667,\n",
              " 'step_by_step_lr_bow_enron1': 0.9194630872483222,\n",
              " 'step_by_step_lr_bow_enron2': 0.7153846153846154,\n",
              " 'step_by_step_lr_bow_enron4': 1.0,\n",
              " 'sklearn_lr_bern_enron1': 0.9664429530201343,\n",
              " 'sklearn_lr_bern_enron2': 0.9,\n",
              " 'sklearn_lr_bern_enron4': 1.0,\n",
              " 'step_by_step_lr_bern_enron1': 0.8120805369127517,\n",
              " 'step_by_step_lr_bern_enron2': 0.6538461538461539,\n",
              " 'step_by_step_lr_bern_enron4': 1.0,\n",
              " 'sgd_bow_enron1': 0.9798657718120806,\n",
              " 'sgd_bow_enron2': 0.9230769230769231,\n",
              " 'sgd_bow_enron4': 0.9718670076726342,\n",
              " 'sgd_bern_enron1': 0.9865771812080537,\n",
              " 'sgd_bern_enron2': 0.9692307692307692,\n",
              " 'sgd_bern_enron4': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "all_results_recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "tirIX7Oe4w67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6077a01-5531-4b68-ba25-1e5d47163ca0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sklearn_mnb_bow_enron1': 0.9072164948453608,\n",
              " 'sklearn_mnb_bow_enron2': 0.8924302788844622,\n",
              " 'sklearn_mnb_bow_enron4': 0.9822335025380711,\n",
              " 'step_by_step_mnb_bow_enron1': 0.9014084507042253,\n",
              " 'step_by_step_mnb_bow_enron2': 0.8906882591093118,\n",
              " 'step_by_step_mnb_bow_enron4': 0.9683944374209861,\n",
              " 'sklearn_bnb_bern_enron1': 0.3204419889502762,\n",
              " 'sklearn_bnb_bern_enron2': 0.31645569620253167,\n",
              " 'sklearn_bnb_bern_enron4': 0.9455864570737605,\n",
              " 'step_by_step_bern_enron1': 0.33879781420765026,\n",
              " 'step_by_step_bern_enron2': 0.3375,\n",
              " 'step_by_step_bern_enron4': 0.9455864570737605,\n",
              " 'sklearn_lr_bow_enron1': 0.9483870967741935,\n",
              " 'sklearn_lr_bow_enron2': 0.9118773946360154,\n",
              " 'sklearn_lr_bow_enron4': 0.9641532756489494,\n",
              " 'step_by_step_lr_bow_enron1': 0.9225589225589227,\n",
              " 'step_by_step_lr_bow_enron2': 0.8017241379310345,\n",
              " 'step_by_step_lr_bow_enron4': 0.9630541871921182,\n",
              " 'sklearn_lr_bern_enron1': 0.9290322580645162,\n",
              " 'sklearn_lr_bern_enron2': 0.9140625,\n",
              " 'sklearn_lr_bern_enron4': 0.9421686746987952,\n",
              " 'step_by_step_lr_bern_enron1': 0.8800000000000001,\n",
              " 'step_by_step_lr_bern_enron2': 0.7555555555555555,\n",
              " 'step_by_step_lr_bern_enron4': 0.9606879606879607,\n",
              " 'sgd_bow_enron1': 0.9389067524115756,\n",
              " 'sgd_bow_enron2': 0.9090909090909091,\n",
              " 'sgd_bow_enron4': 0.9608091024020227,\n",
              " 'sgd_bern_enron1': 0.9453376205787781,\n",
              " 'sgd_bern_enron2': 0.9230769230769231,\n",
              " 'sgd_bern_enron4': 0.9618696186961869}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "all_results_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "rkugjtbn4yRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9e0949-b5a2-4374-a818-f3b35a1da5f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sklearn_mnb_bow_enron1': 0.9295774647887324,\n",
              " 'sklearn_mnb_bow_enron2': 0.9256198347107438,\n",
              " 'sklearn_mnb_bow_enron4': 0.9748110831234257,\n",
              " 'step_by_step_mnb_bow_enron1': 0.9481481481481482,\n",
              " 'step_by_step_mnb_bow_enron2': 0.9401709401709402,\n",
              " 'step_by_step_mnb_bow_enron4': 0.9575,\n",
              " 'sklearn_bnb_bern_enron1': 0.90625,\n",
              " 'sklearn_bnb_bern_enron2': 0.8928571428571429,\n",
              " 'sklearn_bnb_bern_enron4': 0.8967889908256881,\n",
              " 'step_by_step_bern_enron1': 0.9117647058823529,\n",
              " 'step_by_step_bern_enron2': 0.9,\n",
              " 'step_by_step_bern_enron4': 0.8967889908256881,\n",
              " 'sklearn_lr_bow_enron1': 0.9130434782608695,\n",
              " 'sklearn_lr_bow_enron2': 0.9083969465648855,\n",
              " 'sklearn_lr_bow_enron4': 0.9330143540669856,\n",
              " 'step_by_step_lr_bow_enron1': 0.9256756756756757,\n",
              " 'step_by_step_lr_bow_enron2': 0.9117647058823529,\n",
              " 'step_by_step_lr_bow_enron4': 0.9287410926365796,\n",
              " 'sklearn_lr_bern_enron1': 0.9473684210526316,\n",
              " 'sklearn_lr_bern_enron2': 0.9069767441860466,\n",
              " 'sklearn_lr_bern_enron4': 0.9702233250620347,\n",
              " 'step_by_step_lr_bern_enron1': 0.9603174603174603,\n",
              " 'step_by_step_lr_bern_enron2': 0.8947368421052632,\n",
              " 'step_by_step_lr_bern_enron4': 0.9243498817966903,\n",
              " 'sgd_bow_enron1': 0.9012345679012346,\n",
              " 'sgd_bow_enron2': 0.8955223880597015,\n",
              " 'sgd_bow_enron4': 0.95,\n",
              " 'sgd_bern_enron1': 0.9074074074074074,\n",
              " 'sgd_bern_enron2': 0.8811188811188811,\n",
              " 'sgd_bern_enron4': 0.9265402843601895}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "all_results_precision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the combined data\n",
        "data = []\n",
        "\n",
        "# Iterate through the keys in the accuracy dictionary\n",
        "for key in all_results_accuracy.keys():\n",
        "    # Split the key into parts\n",
        "    parts = key.split('_')\n",
        "\n",
        "    # Determine the data representation ('bow' or 'bern')\n",
        "    if 'bow' in parts:\n",
        "        rep_idx = parts.index('bow')\n",
        "        data_representation = 'bow'\n",
        "    elif 'bern' in parts:\n",
        "        rep_idx = parts.index('bern')\n",
        "        data_representation = 'bern'\n",
        "    else:\n",
        "        data_representation = 'unknown'\n",
        "\n",
        "    # Extract model name (all parts before data representation)\n",
        "    model_name = '_'.join(parts[:rep_idx])\n",
        "\n",
        "    # Extract dataset (all parts after data representation)\n",
        "    dataset = '_'.join(parts[rep_idx+1:])\n",
        "\n",
        "    # Retrieve the corresponding metrics\n",
        "    acc = all_results_accuracy.get(key, None)\n",
        "    rec = all_results_recall.get(key, None)\n",
        "    f1_score = all_results_f1.get(key, None)\n",
        "    prec = all_results_precision.get(key, None)\n",
        "\n",
        "    # Append the extracted data as a dictionary\n",
        "    data.append({\n",
        "        'model name': model_name,\n",
        "        'data representation': data_representation,\n",
        "        'dataset': dataset,\n",
        "        'acc': acc,\n",
        "        'recall': rec,\n",
        "        'f1': f1_score,\n",
        "        'precision': prec\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Optional: Reorder the columns if desired\n",
        "df = df[['model name', 'data representation', 'dataset', 'acc', 'recall', 'f1', 'precision']]\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('model_metrics.csv', index=False)\n",
        "\n",
        "print(\"Data has been successfully saved to 'model_metrics.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWcbFavxeckd",
        "outputId": "a8a157a9-8963-47c6-ac04-9e3414766177"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully saved to 'model_metrics.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l1CPMKTfyO9"
      },
      "source": [
        "## Result Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "tRHCp0pMbZ-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "19a62cbc-0db2-4802-b0a6-53e4cb2f8de4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Creating a bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(all_results_accuracy.keys(), all_results_accuracy.values(), color='skyblue')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model Accuracies')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "6qth30YOcYwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "3ec7e50c-09a8-41cf-84e8-a1cafa7c582d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6oElEQVR4nOzdd3gU5f6/8feG9EDohN6RIi2AIr0KIoKgR5QiRUBRmoIIiBBBvqAoCEpVOoqgiO2IjSZgPXTpTXo/SOgteX5/8GMP6wYEJTMPu/frunIps5vsnTKzk09mZj3GGCMAAAAAAADAQSFuBwAAAAAAACD4MJQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAtwWPx6OXX37Z7Yx/bMaMGSpWrJjCwsKUIUMGt3P87Ny5Ux6PR1OnTr3p9128eLE8Ho8WL158y7sAAEDgYSgFAMBtYvv27XrqqadUsGBBRUZGKjY2VlWqVNGoUaN09uxZt/NwAzZt2qS2bduqUKFCevfdd/XOO+9c874vv/yyPB6PQkJCtGfPHr/bT5w4oaioKHk8HnXp0iU1s2+5qVOnyuPxpPjWp08f7/2+/fZbtW/fXiVLllSaNGmUP3/+m3qcU6dOKSEhQSVLllRMTIwyZ86ssmXLqnv37tq/f/8t/qwAAMDNCnU7AAAA/LUvv/xSjzzyiCIiItS6dWuVLFlSFy5c0LJly9SrVy+tX7/+ugOOQHD27FmFht7euy6LFy9WcnKyRo0apcKFC9/Q+0REROiDDz7QCy+84LN87ty5qZHoqEGDBqlAgQI+y0qWLOn9/5kzZ2r27NkqV66ccubMeVMf++LFi6pevbo2bdqkNm3aqGvXrjp16pTWr1+vmTNnqmnTpjf9MQEAwK11e+/ZAQAQBH7//Xc99thjypcvnxYuXKgcOXJ4b+vcubO2bdumL7/80sXC1JOcnKwLFy4oMjJSkZGRbuf8Y4cPH5akmzpt7/77709xKDVz5kw1bNhQH3/88a1MdFSDBg1UoUKFa94+ZMgQvfvuuwoLC9MDDzygdevW3fDH/vTTT7Vq1Sq9//77atGihc9t586d04ULF/529806ffq0YmJiHHs8AABuF5y+BwCA5YYNG6ZTp05p0qRJPgOpKwoXLqzu3bt7/33p0iW98sorKlSokCIiIpQ/f369+OKLOn/+vM/75c+fXw888IAWL16sChUqKCoqSqVKlfJeD2ju3LkqVaqUIiMjVb58ea1atcrn/du2bau0adNqx44dql+/vmJiYpQzZ04NGjRIxhif+77xxhuqXLmyMmfOrKioKJUvX15z5szx+1yunIr2/vvv684771RERIS+/vpr721XX1Pq5MmTevbZZ5U/f35FREQoW7Zsuvfee7Vy5Uqfj/nRRx+pfPnyioqKUpYsWdSqVSvt27cvxc9l3759atKkidKmTausWbPq+eefV1JS0jW+M77Gjh3rbc6ZM6c6d+6s48eP+3y9ExISJElZs2a94WtktWjRQqtXr9amTZu8yw4ePKiFCxf6DVuuOHz4sNq3b6+4uDhFRkaqTJkymjZtmt/9jh8/rrZt2yp9+vTKkCGD2rRp49N8tU2bNulf//qXMmXKpMjISFWoUEGff/75X/b/Ezlz5lRYWNjfet/t27dLkqpUqeJ325XTX6+2adMmNWvWTFmzZlVUVJSKFi2qfv36+dxn1apVatCggWJjY5U2bVrVqVNHP//8s899rpya+P333+uZZ55RtmzZlDt3bu/tX331lapVq6aYmBilS5dODRs21Pr1630+xsGDB9WuXTvlzp1bERERypEjhx588EHt3Lnzb30tAACwFUMpAAAs98UXX6hgwYKqXLnyDd2/Q4cOGjBggMqVK6c333xTNWrU0NChQ/XYY4/53Xfbtm1q0aKFGjVqpKFDh+qPP/5Qo0aN9P777+u5555Tq1atNHDgQG3fvl3NmjVTcnKyz/snJSXpvvvuU1xcnIYNG6by5csrISHBO3y5YtSoUYqPj9egQYM0ZMgQhYaG6pFHHknxCK+FCxfqueee06OPPqpRo0Zd8zpCnTp10rhx4/Twww9r7Nixev755xUVFaWNGzd67zN16lQ1a9ZMadKk0dChQ9WxY0fNnTtXVatW9Ru+JCUlqX79+sqcObPeeOMN1ahRQ8OHD7+h0yJffvllde7cWTlz5tTw4cP18MMPa8KECapXr54uXrwoSRo5cqSaNm0qSRo3bpxmzJihhx566C8/dvXq1ZU7d27NnDnTu2z27NlKmzatGjZs6Hf/s2fPqmbNmpoxY4Zatmyp119/XenTp1fbtm01atQo7/2MMXrwwQc1Y8YMtWrVSoMHD9bevXvVpk0bv4+5fv163XPPPdq4caP69Omj4cOHKyYmRk2aNNEnn3zyl5/DtSQmJuro0aM+b7dKvnz5JEnTp0/3G5L+2dq1a1WxYkUtXLhQHTt21KhRo9SkSRN98cUX3vusX79e1apV05o1a/TCCy+of//++v3331WzZk398ssvfh/zmWee0YYNGzRgwADvdbJmzJihhg0bKm3atHrttdfUv39/bdiwQVWrVvUZOD388MP65JNP1K5dO40dO1bdunXTyZMntXv37lvwlQEAwCIGAABYKzEx0UgyDz744A3df/Xq1UaS6dChg8/y559/3kgyCxcu9C7Lly+fkWR+/PFH77JvvvnGSDJRUVFm165d3uUTJkwwksyiRYu8y9q0aWMkma5du3qXJScnm4YNG5rw8HBz5MgR7/IzZ8749Fy4cMGULFnS1K5d22e5JBMSEmLWr1/v97lJMgkJCd5/p0+f3nTu3PmaX4sLFy6YbNmymZIlS5qzZ896l//73/82ksyAAQP8PpdBgwb5fIz4+HhTvnz5az6GMcYcPnzYhIeHm3r16pmkpCTv8tGjRxtJZvLkyd5lCQkJRpLP1+Zarr7v888/bwoXLuy97a677jLt2rUzxlz+ulz9dRg5cqSRZN577z2fr0WlSpVM2rRpzYkTJ4wxxnz66adGkhk2bJj3fpcuXTLVqlUzksyUKVO8y+vUqWNKlSplzp07512WnJxsKleubIoUKeJdtmjRIr+fk5RMmTLFSErx7VoaNmxo8uXLd92Pe7UzZ86YokWLGkkmX758pm3btmbSpEnm0KFDfvetXr26SZcunc/P/JXP8YomTZqY8PBws337du+y/fv3m3Tp0pnq1av7fW5Vq1Y1ly5d8i4/efKkyZAhg+nYsaPPYxw8eNCkT5/eu/yPP/4wkszrr79+w58rAAC3K46UAgDAYidOnJAkpUuX7obuP2/ePElSjx49fJb37NlTkvyOTCpRooQqVark/XfFihUlSbVr11bevHn9lu/YscPvMa9+5bcrp99duHBB8+fP9y6Piory/v8ff/yhxMREVatWze9UO0mqUaOGSpQo8Ref6eXrMv3yyy/XfBW15cuX6/Dhw3rmmWd8rkfVsGFDFStWLMWjtDp16uTz72rVqqX4OV9t/vz5unDhgp599lmFhPxv16pjx46KjY29Jdf7atGihbZt26b//Oc/3v9e69S9efPmKXv27GrevLl3WVhYmLp166ZTp07p+++/994vNDRUTz/9tPd+adKkUdeuXX0+3rFjx7Rw4UI1a9ZMJ0+e9B7R9N///lf169fX1q1b/U6HvFFjxozRd9995/N2q0RFRemXX35Rr169JF0+aq59+/bKkSOHunbt6j2d9ciRI1qyZImeeOIJn5956fLPs3T5KLpvv/1WTZo0UcGCBb2358iRQy1atNCyZcu86+oVHTt2VJo0abz//u6773T8+HE1b97c58iwNGnSqGLFilq0aJG3Ozw8XIsXL9Yff/xxy74eAADYiAudAwBgsSvXvTl58uQN3X/Xrl0KCQnxe2W37NmzK0OGDNq1a5fP8j//Ep4+fXpJUp48eVJc/udfkkNCQnx+SZekO+64Q5J8Tkf697//rcGDB2v16tU+17a68kv/1f78amzXMmzYMLVp00Z58uRR+fLldf/996t169beniufa9GiRf3et1ixYlq2bJnPssjISGXNmtVnWcaMGf9yMHCtxwkPD1fBggX9vuZ/R3x8vIoVK6aZM2cqQ4YMyp49u2rXrn3NniJFivgMyCSpePHiPr27du1Sjhw5lDZtWp/7/fnz2LZtm4wx6t+/v/r375/iYx4+fFi5cuW66c/r7rvvvu6Fzv+p9OnTa9iwYRo2bJh27dqlBQsW6I033tDo0aOVPn16DR482Dt0vPpV//7syJEjOnPmTIo/S8WLF1dycrL27NmjO++807v8zz/HW7dulaRrft+urOsRERF67bXX1LNnT8XFxemee+7RAw88oNatWyt79uw39wUAAMByDKUAALBYbGyscubMeVOvOialPOxJydVHctzIcvMX1+ZJydKlS9W4cWNVr15dY8eOVY4cORQWFqYpU6b4XCfpiquPqrqeZs2aqVq1avrkk0/07bff6vXXX9drr72muXPnqkGDBjfdea3P2RYtWrTQuHHjlC5dOj366KN+Q6fUcuU6Ys8//7zq16+f4n3+PAS1Ub58+fTEE0+oadOmKliwoN5//30NHjw41R7vzz/HV76OM2bMSHG4FBr6v93yZ599Vo0aNdKnn36qb775Rv3799fQoUO1cOFCxcfHp1ozAABOYygFAIDlHnjgAb3zzjv66aeffE61S0m+fPmUnJysrVu3eo+MkaRDhw7p+PHj3os/3yrJycnasWOH9+goSdqyZYskeS9Q/vHHHysyMlLffPONIiIivPebMmXKP378HDly6JlnntEzzzyjw4cPq1y5cvq///s/NWjQwPu5bt682e/olM2bN9+yr8XVj3P1UWMXLlzQ77//rrp1696Sx2nRooUGDBigAwcOaMaMGdftWbt2rZKTk30GV1deve9Kb758+bRgwQKdOnXK52ipzZs3+3y8K59TWFjYLftc3JQxY0YVKlTIO+i98vldb/CbNWtWRUdH+31tpMtf15CQEL+jC/+sUKFCkqRs2bLd0NexUKFC6tmzp3r27KmtW7eqbNmyGj58uN57772/fF8AAG4XXFMKAADLvfDCC4qJiVGHDh106NAhv9u3b9/ufVW1+++/X9LlV3q72ogRIyQpxVdr+6dGjx7t/X9jjEaPHq2wsDDVqVNH0uUjkDwej5KSkrz327lzpz799NO//ZhJSUlKTEz0WZYtWzblzJnTe3pghQoVlC1bNo0fP97nlMGvvvpKGzduvGVfi7p16yo8PFxvvfWWz5FkkyZNUmJi4i17nEKFCmnkyJEaOnSo7r777mve7/7779fBgwc1e/Zs77JLly7p7bffVtq0aVWjRg3v/S5duqRx48Z575eUlKS3337b5+Nly5ZNNWvW1IQJE3TgwAG/xzty5Mg//dRSxZo1a1J8Nb9du3Zpw4YN3lPxsmbNqurVq2vy5Ml+r2535fuZJk0a1atXT5999pnPaamHDh3SzJkzVbVqVe/pd9dSv359xcbGasiQId5XZLzala/jmTNndO7cOZ/bChUqpHTp0vn8HAMAEAg4UgoAAMsVKlRIM2fO1KOPPqrixYurdevWKlmypC5cuKAff/xRH330kdq2bStJKlOmjNq0aaN33nlHx48fV40aNfTrr79q2rRpatKkiWrVqnVL2yIjI/X111+rTZs2qlixor766it9+eWXevHFF73XZ2rYsKFGjBih++67Ty1atNDhw4c1ZswYFS5cWGvXrv1bj3vy5Enlzp1b//rXv1SmTBmlTZtW8+fP13/+8x8NHz5c0uUje1577TW1a9dONWrUUPPmzXXo0CGNGjVK+fPn13PPPXdLvgZZs2ZV3759NXDgQN13331q3LixNm/erLFjx+quu+5Sq1atbsnjSFL37t3/8j5PPvmkJkyYoLZt22rFihXKnz+/5syZox9++EEjR470XjS/UaNGqlKlivr06aOdO3eqRIkSmjt3rt+wT7p8QfKqVauqVKlS6tixowoWLKhDhw7pp59+0t69e7VmzZpb9jlebe3atfr8888lXb62VWJioveUuzJlyqhRo0bXfN/vvvtOCQkJaty4se655x6lTZtWO3bs0OTJk3X+/Hm9/PLL3vu+9dZbqlq1qsqVK6cnn3xSBQoU0M6dO/Xll19q9erVkqTBgwfru+++U9WqVfXMM88oNDRUEyZM0Pnz5zVs2LC//FxiY2M1btw4Pf744ypXrpwee+wxZc2aVbt379aXX36pKlWqaPTo0dqyZYvq1KmjZs2aqUSJEgoNDdUnn3yiQ4cO6bHHHvv7X0wAAGzk6mv/AQCAG7ZlyxbTsWNHkz9/fhMeHm7SpUtnqlSpYt5++21z7tw57/0uXrxoBg4caAoUKGDCwsJMnjx5TN++fX3uY4wx+fLlMw0bNvR7HEmmc+fOPst+//13v5epb9OmjYmJiTHbt2839erVM9HR0SYuLs4kJCSYpKQkn/efNGmSKVKkiImIiDDFihUzU6ZMMQkJCebPuyIpPfbVtyUkJBhjjDl//rzp1auXKVOmjEmXLp2JiYkxZcqUMWPHjvV7v9mzZ5v4+HgTERFhMmXKZFq2bGn27t3rc58rn8ufpdR4LaNHjzbFihUzYWFhJi4uzjz99NPmjz/+SPHjHTly5C8/3o3eN6Wv2aFDh0y7du1MlixZTHh4uClVqpSZMmWK3/v+97//NY8//riJjY016dOnN48//rhZtWqVkeR3/+3bt5vWrVub7Nmzm7CwMJMrVy7zwAMPmDlz5njvs2jRIiPJLFq06LrNU6ZMMZLMf/7znxu6X0pvbdq0ue777tixwwwYMMDcc889Jlu2bCY0NNRkzZrVNGzY0CxcuNDv/uvWrTNNmzY1GTJkMJGRkaZo0aKmf//+PvdZuXKlqV+/vkmbNq2Jjo42tWrVMj/++ONNfW6LFi0y9evXN+nTpzeRkZGmUKFCpm3btmb58uXGGGOOHj1qOnfubIoVK2ZiYmJM+vTpTcWKFc2HH3543c8XAIDbkceYv3HFUgAAEPTatm2rOXPm6NSpU26nAAAA4DbENaUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOa0oBAAAAAADAcRwpBQAAAAAAAMcxlAIAAAAAAIDjQt0OcFpycrL279+vdOnSyePxuJ0DAAAAAAAQUIwxOnnypHLmzKmQkGsfDxV0Q6n9+/crT548bmcAAAAAAAAEtD179ih37tzXvD3ohlLp0qWTdPkLExsb63INAAAAAABAYDlx4oTy5MnjncFcS9ANpa6cshcbG8tQCgAAAAAAIJX81WWTuNA5AAAAAAAAHMdQCgAAAAAAAI5zdSi1ZMkSNWrUSDlz5pTH49Gnn376l++zePFilStXThERESpcuLCmTp2a6p0AAAAAAAC4tVwdSp0+fVplypTRmDFjbuj+v//+uxo2bKhatWpp9erVevbZZ9WhQwd98803qVwKAAAAAACAW8nVC503aNBADRo0uOH7jx8/XgUKFNDw4cMlScWLF9eyZcv05ptvqn79+qmVCQAAAAAAgFvstrqm1E8//aS6dev6LKtfv75++uknl4oAAAAAAADwd7h6pNTNOnjwoOLi4nyWxcXF6cSJEzp79qyioqL83uf8+fM6f/68998nTpxI9U4AAAAAAABc3211pNTfMXToUKVPn977lidPHreTAAAAAAAAgt5tNZTKnj27Dh065LPs0KFDio2NTfEoKUnq27evEhMTvW979uxxIhUAAAAAAADXcVudvlepUiXNmzfPZ9l3332nSpUqXfN9IiIiFBERkdppAAAAAAAAuAmuHil16tQprV69WqtXr5Yk/f7771q9erV2794t6fJRTq1bt/bev1OnTtqxY4deeOEFbdq0SWPHjtWHH36o5557zo18AAAAAAAA/E2uDqWWL1+u+Ph4xcfHS5J69Oih+Ph4DRgwQJJ04MAB74BKkgoUKKAvv/xS3333ncqUKaPhw4dr4sSJql+/viv9AAAAAAAA+Hs8xhjjdoSTTpw4ofTp0ysxMVGxsbFu5+AWe3XVUUcfr098FkcfDwAAAAAA293o7OW2utA5AAAAAAAAAgNDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjgt1OwBAcHDyIvRcgB4AAAAA7MdQCgAA4AYxYLcTr74LAMDtiaEUAAAAAAQRBux2YsBuL9aZ1MM1pQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI5rSgEAgGviGgoAAABILQylACCIMXCwF98bAADgFvZD4BSGUgAAAABSBb/YAgCuh6EUAAAAAMBxTg4tJQaXgI240DkAAAAAAAAcx1AKAAAAAAAAjuP0PQAAAOAW4FQkAABuDkdKAQAAAAAAwHEcKQUAAHCb4RXNAADBgOe7wMeRUgAAAAAAAHAcR0oBAAAAAYajCwAAtwOGUgAAwHr8gg0AABB4GErd5thJBwAAAIC/j1fOBNzDNaUAAAAAAADgOI6UAlIJR7EBAAAAAHBtHCkFAAAAAAAAxzGUAgAAAAAAgOM4fQ+3BBcHBAAAAAAAN4OhFBDguLYVAAAAAMBGnL4HAAAAAAAAxzGUAgAAAAAAgOM4fQ8AAABAQONyBgBgJ46UAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA414dSY8aMUf78+RUZGamKFSvq119/ve79R44cqaJFiyoqKkp58uTRc889p3PnzjlUCwAAAAAAgFvB1aHU7Nmz1aNHDyUkJGjlypUqU6aM6tevr8OHD6d4/5kzZ6pPnz5KSEjQxo0bNWnSJM2ePVsvvviiw+UAAAAAAAD4J1wdSo0YMUIdO3ZUu3btVKJECY0fP17R0dGaPHlyivf/8ccfVaVKFbVo0UL58+dXvXr11Lx58788ugoAAAAAAAB2cW0odeHCBa1YsUJ169b9X0xIiOrWrauffvopxfepXLmyVqxY4R1C7dixQ/PmzdP9999/zcc5f/68Tpw44fMGAAAAAAAAd4W69cBHjx5VUlKS4uLifJbHxcVp06ZNKb5PixYtdPToUVWtWlXGGF26dEmdOnW67ul7Q4cO1cCBA29pOwAAAKRXVx117LH6xGdx7LEAAIAzXL/Q+c1YvHixhgwZorFjx2rlypWaO3euvvzyS73yyivXfJ++ffsqMTHR+7Znzx4HiwEAAAAAAJAS146UypIli9KkSaNDhw75LD906JCyZ8+e4vv0799fjz/+uDp06CBJKlWqlE6fPq0nn3xS/fr1U0iI/4wtIiJCERERt/4TAAAAAAAAwN/m2pFS4eHhKl++vBYsWOBdlpycrAULFqhSpUopvs+ZM2f8Bk9p0qSRJBljUi8WAAAAAAAAt5RrR0pJUo8ePdSmTRtVqFBBd999t0aOHKnTp0+rXbt2kqTWrVsrV65cGjp0qCSpUaNGGjFihOLj41WxYkVt27ZN/fv3V6NGjbzDKQAAAAAAANjP1aHUo48+qiNHjmjAgAE6ePCgypYtq6+//tp78fPdu3f7HBn10ksvyePx6KWXXtK+ffuUNWtWNWrUSP/3f//n1qcAAAAAAACAv8HVoZQkdenSRV26dEnxtsWLF/v8OzQ0VAkJCUpISHCgDAAAAAAAAKnltnr1PQAAAAAAAAQGhlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgONC3Q4AAAAAgGDw6qqjjj1Wn/gsjj0WAPxdHCkFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA41wfSo0ZM0b58+dXZGSkKlasqF9//fW69z9+/Lg6d+6sHDlyKCIiQnfccYfmzZvnUC0AAAAAAABuhVA3H3z27Nnq0aOHxo8fr4oVK2rkyJGqX7++Nm/erGzZsvnd/8KFC7r33nuVLVs2zZkzR7ly5dKuXbuUIUMG5+MBAAAAAADwt930kVL58+fXoEGDtHv37n/84CNGjFDHjh3Vrl07lShRQuPHj1d0dLQmT56c4v0nT56sY8eO6dNPP1WVKlWUP39+1ahRQ2XKlPnHLQAAAAAAAHDOTQ+lnn32Wc2dO1cFCxbUvffeq1mzZun8+fM3/cAXLlzQihUrVLdu3f/FhISobt26+umnn1J8n88//1yVKlVS586dFRcXp5IlS2rIkCFKSkq66ccHAAAAAACAe/7WUGr16tX69ddfVbx4cXXt2lU5cuRQly5dtHLlyhv+OEePHlVSUpLi4uJ8lsfFxengwYMpvs+OHTs0Z84cJSUlad68eerfv7+GDx+uwYMHX/Nxzp8/rxMnTvi8AQAAAAAAwF1/+0Ln5cqV01tvvaX9+/crISFBEydO1F133aWyZctq8uTJMsbcyk5JUnJysrJly6Z33nlH5cuX16OPPqp+/fpp/Pjx13yfoUOHKn369N63PHny3PIuAAAAAAAA3Jy/PZS6ePGiPvzwQzVu3Fg9e/ZUhQoVNHHiRD388MN68cUX1bJly+u+f5YsWZQmTRodOnTIZ/mhQ4eUPXv2FN8nR44cuuOOO5QmTRrvsuLFi+vgwYO6cOFCiu/Tt29fJSYmet/27Nlzk58pAAAAAAAAbrWbfvW9lStXasqUKfrggw8UEhKi1q1b680331SxYsW892natKnuuuuu636c8PBwlS9fXgsWLFCTJk0kXT4SasGCBerSpUuK71OlShXNnDlTycnJCgm5PE/bsmWLcuTIofDw8BTfJyIiQhERETf7aQIAAAAAACAV3fSRUnfddZe2bt2qcePGad++fXrjjTd8BlKSVKBAAT322GN/+bF69Oihd999V9OmTdPGjRv19NNP6/Tp02rXrp0kqXXr1urbt6/3/k8//bSOHTum7t27a8uWLfryyy81ZMgQde7c+WY/DQAAAAAAALjopo+U2rFjh/Lly3fd+8TExGjKlCl/+bEeffRRHTlyRAMGDNDBgwdVtmxZff31196Ln+/evdt7RJQk5cmTR998842ee+45lS5dWrly5VL37t3Vu3fvm/00AAAAAAAA4KKbHkodPnxYBw8eVMWKFX2W//LLL0qTJo0qVKhwUx+vS5cu1zxdb/HixX7LKlWqpJ9//vmmHgMAAAAAAAB2uenT9zp37pzixcL37dvHaXQAAAAAAAC4ITc9lNqwYYPKlSvntzw+Pl4bNmy4JVEAAAAAAAAIbDc9lIqIiNChQ4f8lh84cEChoTd9NiAAAAAAAACC0E0PperVq6e+ffsqMTHRu+z48eN68cUXde+9997SOAAAAAAAAASmmz606Y033lD16tWVL18+xcfHS5JWr16tuLg4zZgx45YHAgAAAAAAIPDc9FAqV65cWrt2rd5//32tWbNGUVFRateunZo3b66wsLDUaAQAAAAAAECA+VsXgYqJidGTTz55q1sAAAAAAAAQJP72lck3bNig3bt368KFCz7LGzdu/I+jAAAAAAAAENhueii1Y8cONW3aVL/99ps8Ho+MMZIkj8cjSUpKSrq1hQAAAAAAAAg4N/3qe927d1eBAgV0+PBhRUdHa/369VqyZIkqVKigxYsXp0IiAAAAAAAAAs1NHyn1008/aeHChcqSJYtCQkIUEhKiqlWraujQoerWrZtWrVqVGp0AAAAAAAAIIDd9pFRSUpLSpUsnScqSJYv2798vScqXL582b958a+sAAAAAAAAQkG76SKmSJUtqzZo1KlCggCpWrKhhw4YpPDxc77zzjgoWLJgajQAAAAAAAAgwNz2Ueumll3T69GlJ0qBBg/TAAw+oWrVqypw5s2bPnn3LAwEAAAAAABB4bnooVb9+fe//Fy5cWJs2bdKxY8eUMWNG7yvwAQAAAAAAANdzU9eUunjxokJDQ7Vu3Tqf5ZkyZWIgBQAAAAAAgBt2U0OpsLAw5c2bV0lJSanVAwAAAAAAgCBw06++169fP7344os6duxYavQAAAAAAAAgCNz0NaVGjx6tbdu2KWfOnMqXL59iYmJ8bl+5cuUtiwMAAAAAAEBguumhVJMmTVIhAwAAAAAAAMHkpodSCQkJqdEBAAAAAACAIHLT15QCAAAAAAAA/qmbPlIqJCREHo/nmrfzynwAAAAAAAD4Kzc9lPrkk098/n3x4kWtWrVK06ZN08CBA29ZGAAAAAAAAALXTQ+lHnzwQb9l//rXv3TnnXdq9uzZat++/S0JAwAAAAAAQOC6ZdeUuueee7RgwYJb9eEAAAAAAAAQwG7JUOrs2bN66623lCtXrlvx4QAAAAAAABDgbvr0vYwZM/pc6NwYo5MnTyo6OlrvvffeLY0DAAAAAABAYLrpodSbb77pM5QKCQlR1qxZVbFiRWXMmPGWxgEAAAAAACAw3fRQqm3btqmQAQAAAAAAgGBy09eUmjJlij766CO/5R999JGmTZt2S6IAAAAAAAAQ2G56KDV06FBlyZLFb3m2bNk0ZMiQWxIFAAAAAACAwHbTQ6ndu3erQIECfsvz5cun3bt335IoAAAAAAAABLabHkply5ZNa9eu9Vu+Zs0aZc6c+ZZEAQAAAAAAILDd9FCqefPm6tatmxYtWqSkpCQlJSVp4cKF6t69ux577LHUaAQAAAAAAECAuelX33vllVe0c+dO1alTR6Ghl989OTlZrVu35ppSAAAAAAAAuCE3PZQKDw/X7NmzNXjwYK1evVpRUVEqVaqU8uXLlxp9AAAAAAAACEA3PZS6okiRIipSpMitbAEAAAAAAECQuOlrSj388MN67bXX/JYPGzZMjzzyyC2JAgAAAAAAQGC76aHUkiVLdP/99/stb9CggZYsWXJLogAAAAAAABDYbnooderUKYWHh/stDwsL04kTJ25JFAAAAAAAAALbTQ+lSpUqpdmzZ/stnzVrlkqUKHFLogAAAAAAABDYbvpC5/3799dDDz2k7du3q3bt2pKkBQsWaObMmZozZ84tDwQAAAAAAEDguemhVKNGjfTpp59qyJAhmjNnjqKiolSmTBktXLhQmTJlSo1GAAAAAAAABJibHkpJUsOGDdWwYUNJ0okTJ/TBBx/o+eef14oVK5SUlHRLAwEAAAAAABB4bvqaUlcsWbJEbdq0Uc6cOTV8+HDVrl1bP//8861sAwAAAAAAQIC6qSOlDh48qKlTp2rSpEk6ceKEmjVrpvPnz+vTTz/lIucAAAAAAAC4YTd8pFSjRo1UtGhRrV27ViNHjtT+/fv19ttvp2YbAAAAAAAAAtQNHyn11VdfqVu3bnr66adVpEiR1GwCAAAAAABAgLvhI6WWLVumkydPqnz58qpYsaJGjx6to0ePpmYbAAAAAAAAAtQND6Xuuecevfvuuzpw4ICeeuopzZo1Szlz5lRycrK+++47nTx5MjU7AQAAAAAAEEBu+tX3YmJi9MQTT2jZsmX67bff1LNnT7366qvKli2bGjdu/LcixowZo/z58ysyMlIVK1bUr7/+ekPvN2vWLHk8HjVp0uRvPS4AAAAAAADccdNDqasVLVpUw4YN0969e/XBBx/8rY8xe/Zs9ejRQwkJCVq5cqXKlCmj+vXr6/Dhw9d9v507d+r5559XtWrV/tbjAgAAAAAAwD3/aCh1RZo0adSkSRN9/vnnN/2+I0aMUMeOHdWuXTuVKFFC48ePV3R0tCZPnnzN90lKSlLLli01cOBAFSxY8J+kAwAAAAAAwAW3ZCj1d124cEErVqxQ3bp1vctCQkJUt25d/fTTT9d8v0GDBilbtmxq3779Xz7G+fPndeLECZ83AAAAAAAAuMvVodTRo0eVlJSkuLg4n+VxcXE6ePBgiu+zbNkyTZo0Se++++4NPcbQoUOVPn1671uePHn+cTcAAAAAAAD+GVeHUjfr5MmTevzxx/Xuu+8qS5YsN/Q+ffv2VWJiovdtz549qVwJAAAAAACAvxLq5oNnyZJFadKk0aFDh3yWHzp0SNmzZ/e7//bt27Vz5041atTIuyw5OVmSFBoaqs2bN6tQoUI+7xMREaGIiIhUqAcAAAAAAMDf5eqRUuHh4SpfvrwWLFjgXZacnKwFCxaoUqVKfvcvVqyYfvvtN61evdr71rhxY9WqVUurV6/m1DwAAAAAAIDbhKtHSklSjx491KZNG1WoUEF33323Ro4cqdOnT6tdu3aSpNatWytXrlwaOnSoIiMjVbJkSZ/3z5AhgyT5LQcAAAAAAIC9XB9KPfroozpy5IgGDBiggwcPqmzZsvr666+9Fz/fvXu3QkJuq0tfAQAAAAAA4C+4PpSSpC5duqhLly4p3rZ48eLrvu/UqVNvfRAAAAAAAABSFYcgAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADgu1O0AAABeXXXU0cfrE5/F0ccDAAAA4I+hFAAA/x/DMQAAAMA5nL4HAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOC50DiCoOHkhay5iDQAAAADXxlAKAFzAcAwAAABAsOP0PQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcVYMpcaMGaP8+fMrMjJSFStW1K+//nrN+7777ruqVq2aMmbMqIwZM6pu3brXvT8AAAAAAADs4/pQavbs2erRo4cSEhK0cuVKlSlTRvXr19fhw4dTvP/ixYvVvHlzLVq0SD/99JPy5MmjevXqad++fQ6XAwAAAAAA4O9yfSg1YsQIdezYUe3atVOJEiU0fvx4RUdHa/LkySne//3339czzzyjsmXLqlixYpo4caKSk5O1YMECh8sBAAAAAADwd7k6lLpw4YJWrFihunXrepeFhISobt26+umnn27oY5w5c0YXL15UpkyZUisTAAAAAAAAt1iomw9+9OhRJSUlKS4uzmd5XFycNm3adEMfo3fv3sqZM6fPYOtq58+f1/nz573/PnHixN8PBgAAAAAAwC3h+ul7/8Srr76qWbNm6ZNPPlFkZGSK9xk6dKjSp0/vfcuTJ4/DlQAAAAAAAPgzV4dSWbJkUZo0aXTo0CGf5YcOHVL27Nmv+75vvPGGXn31VX377bcqXbr0Ne/Xt29fJSYmet/27NlzS9oBAAAAAADw97k6lAoPD1f58uV9LlJ+5aLllSpVuub7DRs2TK+88oq+/vprVahQ4bqPERERodjYWJ83AAAAAAAAuMvVa0pJUo8ePdSmTRtVqFBBd999t0aOHKnTp0+rXbt2kqTWrVsrV65cGjp0qCTptdde04ABAzRz5kzlz59fBw8elCSlTZtWadOmde3zAAAAAAAAwI1zfSj16KOP6siRIxowYIAOHjyosmXL6uuvv/Ze/Hz37t0KCfnfAV3jxo3ThQsX9K9//cvn4yQkJOjll192Mh0AAAAAAAB/k+tDKUnq0qWLunTpkuJtixcv9vn3zp07Uz8IAAAAAAAAqeq2fvU9AAAAAAAA3J4YSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOCuGUmPGjFH+/PkVGRmpihUr6tdff73u/T/66CMVK1ZMkZGRKlWqlObNm+dQKQAAAAAAAG4F14dSs2fPVo8ePZSQkKCVK1eqTJkyql+/vg4fPpzi/X/88Uc1b95c7du316pVq9SkSRM1adJE69atc7gcAAAAAAAAf5frQ6kRI0aoY8eOateunUqUKKHx48crOjpakydPTvH+o0aN0n333adevXqpePHieuWVV1SuXDmNHj3a4XIAAAAAAAD8XaFuPviFCxe0YsUK9e3b17ssJCREdevW1U8//ZTi+/z000/q0aOHz7L69evr008/TfH+58+f1/nz573/TkxMlCSdOHHiH9bb4dypk4491okT4VZ0SNdusaVDCs7vDR103A4dEtsQOuig459hG0IHHbdfh2T/umtLhxScPyO3Q8ft5MrMxRhz/TsaF+3bt89IMj/++KPP8l69epm77747xfcJCwszM2fO9Fk2ZswYky1bthTvn5CQYCTxxhtvvPHGG2+88cYbb7zxxhtvvPHm4NuePXuuOxdy9UgpJ/Tt29fnyKrk5GQdO3ZMmTNnlsfjcbHMPSdOnFCePHm0Z88excbG0kEHHbdJCx100HH7ddjUQgcddNx+HTa10EEHHbdfh5uMMTp58qRy5sx53fu5OpTKkiWL0qRJo0OHDvksP3TokLJnz57i+2TPnv2m7h8REaGIiAifZRkyZPj70QEkNjbWihWEDjpuhw7JnhY66KDj9uuQ7Gmhgw46br8OyZ4WOuig4/brcEv69On/8j6uXug8PDxc5cuX14IFC7zLkpOTtWDBAlWqVCnF96lUqZLP/SXpu+++u+b9AQAAAAAAYB/XT9/r0aOH2rRpowoVKujuu+/WyJEjdfr0abVr106S1Lp1a+XKlUtDhw6VJHXv3l01atTQ8OHD1bBhQ82aNUvLly/XO++84+anAQAAAAAAgJvg+lDq0Ucf1ZEjRzRgwAAdPHhQZcuW1ddff624uDhJ0u7duxUS8r8DuipXrqyZM2fqpZde0osvvqgiRYro008/VcmSJd36FG47ERERSkhI8DutkQ466LC7hQ466Lj9OmxqoYMOOm6/Dpta6KCDjtuv43bgMeavXp8PAAAAAAAAuLVcvaYUAAAAAAAAghNDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlELQmDp1qhITE93OwJ/wAqAAgNQ0cOBAHT161O0M4LbA/jJwc3iO+ecYSgW4NWvWKE2aNI493rx589ShQwe98MIL2rRpk89tf/zxh2rXru1Yy589+eST2r9/v2uP/2d79uzRE0884chjnT17VsuWLdOGDRv8bjt37pymT5/uSEdKIiIitHHjRtce/89Onz6tJUuWuJ1hhXbt2lm1zvzxxx+O/qwmJydfc/nu3bsd6/iz2rVra9euXa49/p8dOnRIgwYNcuzx9u7dq1OnTvktv3jxoqvrbsGCBbV161bXHv/PLl265OjP6dixY1W3bl01a9ZMCxYs8Lnt6NGjKliwYKo3nDhxwu8tMTFR//d//6cdO3Z4l7mNbdlltm3LnNxnZn/55rBvdplt+2VOPs/wHBPYPIbDFALamjVrFB8ff80dkltp5syZat26te677z4lJiZq+fLlmjhxolq2bCnp8i8uOXPmVFJSUqp2ZMqUKcXlx48fV2xsrEJCLs9ijx07lqodf2XNmjUqV65cqn89tmzZonr16mn37t3yeDyqWrWqZs2apRw5ckhy7vvSo0ePFJePGjVKrVq1UubMmSVJI0aMSNWOv+LU9+XixYvq16+f5s6dq0yZMqlTp04+Q0qnvi+StHbt2hSXV6hQQR9++KH3ib506dKp3nI9Tn1vTpw4oQ4dOuiLL75QbGysnnrqKSUkJHh/WXHqe/P555+nuPyhhx7SqFGjlCdPHklS48aNU7Xjrzj1fTlw4IAefPBBrVixQh6PRy1atNDYsWOVNm1aSc59X956660Ul/fo0UMvvPCCsmfPLknq1q1bqnb8Fae+L9Llr0nfvn3Vrl07JSYm6sMPP9TLL7+svn37SnLue3OtgYIxRh6Px/tfJ74m18O27DIbt2VO7DOzv3zzgm3fjP0yXzzHBL5QtwPwzzz00EPXvT0xMVEej8eRltdff10jRozw7oh/+OGHeuKJJ3Tu3Dm1b9/ekQbp8hNKjRo19Mgjj3iXGWO8f5HKlSuXIx3X2gm7YseOHY509O7dWyVLltTy5ct1/PhxPfvss6pSpYoWL16svHnzOtIgSSNHjlSZMmWUIUMGn+XGGG3cuFExMTGO/aza4P/+7/80ffp0Pf/88zp+/Lh69OihX375RRMmTPDex6m/GZQtW9b7ZPpnDz/8sGNPsn/116WTJ0+m6uNf0b9/f61Zs0YzZszQ8ePHNXjwYK1cuVJz585VeHi4JGe+N02aNLnm96Vr166S5Mj35Vo7x1ds3rw5VR//ij59+igkJES//PKLjh8/rj59+qhWrVr69ttvlTFjRknOfF+effZZ5cqVS6GhvrtQycnJmj59usLCwuTxeFwfSjlpwoQJevfdd9WiRQtJ0tNPP60mTZro7Nmzjh5FlyNHDpUtW1Y9e/b0/kJtjFHdunU1ceJEFShQwJEOtmW+bNmW2bLPzP6yvWzZN7Nlv8wWPMcEPo6Uus2FhYXp3nvvVVxcXIq3Hzt2TP/+978d2WilTZtWv/32m88KuWjRIjVu3Fivv/66mjZt6sgUe9u2bWrRooWKFy+uMWPGeP+KHhYWpjVr1qhEiRKp+vhXhISEXPMJ5QonnlDi4uI0f/58lSpVStLljeczzzyjefPmadGiRYqJiXHk+/Lqq6/qnXfe0cSJE30OS3f6+3KtvwxekZSUpFOnTqX616NIkSJ688039cADD0i6/HPboEEDVa1aVZMnT9bhw4cdO1KqbNmyyp07t9544w1FRUVJuvxzUqRIEX311VcqUqSIJClfvnyp2nFlnbkWp3bC8uXLp2nTpqlmzZqSLh8W3rBhQ2XIkEGff/65jh8/7sj3pkGDBkqTJo0mT56sbNmyeZfbtC1z8i+DuXLl0ieffKK7775bknT+/Hk98sgj2rNnjxYsWKCLFy868n3p1KmTfvnlF82cOVPFixf3Lnf6+1KuXLnr3n727Flt2bLFkW1IdHS0NmzYoPz583uXrVu3TnXr1lW7du307LPPOvK9OXbsmNq3b6/ExETNmDHD+0u1W+vMtbAtu8zp74st+8zsL/tj38yXLftltjzP8BwT+DhS6jZXvHhxPfzww9f8y8rq1av173//25GW2NhYHTp0yOdJtlatWvr3v/+tBx54QHv37nWko3Dhwvrxxx/Vr18/lS1bVtOmTVOVKlUceeyr5ciRQ2PHjtWDDz6Y4u2rV69W+fLlU73j7NmzPn/R93g8GjdunLp06aIaNWpo5syZqd4gXT7KoU6dOmrVqpUaNWqkoUOHKiwszJHHvtr58+f19NNPe4d0f7Zr1y4NHDgw1Tv27dunkiVLev9duHBhLV68WLVr19bjjz+uYcOGpXrDFb/++qteeOEFPfzww3rvvfcUHx/vvS1nzpypvtNzRbp06dSvXz9VrFgxxdu3bt2qp556KtU7jhw54vM5Z8mSRfPnz1f9+vV1//33a+LEianeIElfffWV3nzzTVWoUEFjx4717iQ7LVOmTBo2bJjq1KmT4u3r169Xo0aNUr0jMTHRe0SUdPl6dHPnztUjjzyiWrVq6b333kv1BkkaP368PvnkE9WvX18vvPCCunTp4sjj/tmGDRv02GOPXfMvswcOHNCWLVscacmSJYv27Nnj8wtDyZIltXDhQtWuXduxa6BkypRJn3zyicaNG6e7775bb7zxhpo3b+7IY1+NbZkvW7Zltuwzs7/sj30zX7bsl9nyPMNzTBAwuK21bdvWPPPMM9e8fcOGDSZ//vyOtDz44INmwIABKd62aNEiExMTY0JCQhxpuWLBggUmb968pm/fviYsLMysX7/escdu1KiR6d+//zVvX716tfF4PKnecdddd5np06eneFvnzp1NhgwZHP2+nDx50rRu3dqULl3a/Pbbb45/XypXrmxGjhx5zdtXr17tyNejQIECZv78+X7L9+3bZ+644w5z7733Or6+zJs3z+TOndsMGTLEJCUlmdDQUEe/NzVr1jSvvfbaNW93ap0pWrSo+fLLL/2Wnzx50lSqVMmUKVPG0e/NqlWrTIkSJcyTTz5pTp8+7fj3pV69euaVV1655u1OfV9KlSpl5syZ47f84sWLpkmTJiZv3ryOfl/27t1rateube677z5z4MABx78v5cuXN2PHjr3m7atWrXLs69G8eXPz7LPPpnjbunXrTNasWR3fnq1fv96UKVPGNG/enG3ZnwTrtsyWfWb2l/2xb5Yyt/fLbHme4Tkm8PHqe7e58ePH6/XXX7/m7cWLF9fvv//uSMtzzz2nyMjIFG+rWbOmvvjiC7Vu3dqRlitq166tlStXatOmTYqJiXH0lQh79eqlypUrX/P2woULa9GiRane0bRpU33wwQcp3jZ69Gg1b97csWsXSZcPW582bZr69u2runXrOn4+fMOGDXX8+PFr3p4pUyZHfk5r166d4lFqOXPm1MKFCx1bb6/WoEEDLV++XEuXLvWe7uGkFi1aXHMbIknZs2dXQkJCqnfUq1dPU6ZM8VueNm1affPNN9dtTA1ly5bV8uXL5fF4VLZsWUfXV+ny6WpX/3Xyz/LmzZvi1+tWa9Cggd555x2/5aGhofroo49UtmzZVG+4Wq5cuTR//nxVr15d8fHxjn9fqlSpct3reaVLl07Vq1d3pKVPnz7XvODunXfeqYULF2rAgAGOtFxRokQJ/frrr8qePbtKlizpPQXGCWzLUub2tsyWfWb2l/2xb5Yyt/fLbHme4Tkm8HFNKQCu2bNnj1auXKk6dep4r2UQLHbt2qVNmzapfv36Kd6+f/9+fffdd2rTpo3DZZe99dZbWrRokd5++23lzp3blQa3/PHHH9q/f7/uvPPOFG8/efKkVq5cqRo1ajhcdvkFFBYtWqS+ffv6XJslGFy6dElnzpxRbGzsNW/ft2+fY6c1XG3FihVatmyZWrdu7XOKIeAmtmXAzbF53yyY98sQ+BhKBZDk5GRt27ZNhw8f9ns5W6f+Wmpbiy0dAAAEg+PHj+vXX3/1e971eDx6/PHHXe+Q5PhRKLCPLfuHdAA3h+eYwMRQKkD8/PPPatGihXbt2uV3OLTTLxlqS4stHadPn9arr76qBQsWpLjh2rFjBx0udEiXLza7aNGiFDucPAzYlg6bWhYsWHDNn5HJkyfT4UJHUlKSpk6des2OhQsX0uFCh2TPzvEXX3yhli1b6tSpU4qNjfV5BTqPx6Njx44FVYdkx7pLhy9b9g/pSJkt+yF0+LLhecaWbbstHYGEV98LEJ06dVKFChX05ZdfKkeOHNd9KeJgabGlo0OHDvr+++/1+OOP02FRx7vvvqunn35aWbJkUfbs2f2eUJx6orelw6aWgQMHatCgQapQoYKrPyN0+OrevbumTp2qhg0bqmTJknRY0vFXO8dODqV69uypJ554QkOGDFF0dLRjj2trhy3rLh2+bNk/pMOfLfshdPiy5XnGlm27LR0BxflrqyM1REdHm61bt7qdYYyxp8WWjvTp05tly5a5nUHHn+TNm9e8+uqrbmdY02GMPS3Zs2e/5itG0uGezJkzp/hKXnS4q0iRIqZ79+7m9OnTbqeY6Ohos337drczrOmwZd2lw5ct+4d0+LNlP4QOX7Y8z9iybbelI5Dw6nsBomLFitq2bZvbGZLsabGlI2PGjMqUKZPbGXT8yR9//KFHHnnE7QxrOiR7Wi5cuHDdV66kwx3h4eEqXLiw2xl0/Mm+ffvUrVs3K/5aW79+fS1fvtztDGs6bFl36fBly/4hHf5s2Q+hw5ctzzO2bNtt6QgknL4XILp27aqePXvq4MGDKlWqlMLCwnxuv9bLaAZyiy0dr7zyigYMGKBp06a5ujGnw9cjjzyib7/9Vp06dXKtwaYOm1o6dOigmTNnqn///nRY1NGzZ0+NGjVKo0ePdvX0Djp8Xdk5LliwoGsNVzRs2FC9evXShg0bUnzebdy4cVB12LLu0uHLlv1DOvzZsh9Chy9bnmds2bbb0hFIuNB5gAgJ8T/ozePxyBjj+EUKbWmxpSM+Pl7bt2+XMUb58+f323CtXLmSDhc6hg4dqhEjRqhhw4YpPqF069YtqDpsaunevbumT5+u0qVLq3Tp0n4dI0aMoMOFjqZNm2rRokXKlCmT7rzzTr+OuXPn0uFCx6RJkzRo0CC1a9fO9Z3jlJ53r3D7+d+NDlvWXTp82bJ/SIc/W/ZD6PBly/OMLdt2WzoCCUOpALFr167r3p4vXz6HSuxpsaVj4MCB1709ISGBDhc6ChQocM3bPB6PY68CaEuHTS21atW6bodTr2pGh6927dpd9/YpU6bQ4UIHO8f2smXdpcOXLfuHdPizZT+EDl88zyC1MZQCAAAAAACA47imVADZvn27Ro4cqY0bN0qSSpQooe7du6tQoUJB22JLhyStWLHC23HnnXcqPj7e8QY6UnZlNu/mdWFs6pDsadm7d68kKXfu3HRY0nHkyBFt3rxZklS0aFFlzZqVDgs6bPH999/rjTfe8Hne7dWrl6pVqxaUHVfYsO7S8T+27B/ScW227IfQYRdbtu22dAQKXn0vQHzzzTcqUaKEfv31V++5+r/88ovuvPNOfffdd0HZYkvH4cOHVbt2bd11113q1q2bunXrpvLly6tOnTo6cuQIHS51SNL06dNVqlQpRUVFKSoqSqVLl9aMGTMcbbCpw5aW5ORkDRo0SOnTp1e+fPmUL18+ZciQQa+88oqSk5PpcKnj9OnTeuKJJ5QjRw5Vr15d1atXV86cOdW+fXudOXOGDpc6pMs7x40aNVLhwoVVuHBhNW7cWEuXLnW0QZLee+891a1bV9HR0d7te1RUlOrUqaOZM2cGXYct6y4dvmzZP6QjZTbsh9Dhz4bnGVu27bZ0BBSDgFC2bFnTu3dvv+W9e/c28fHxQdliS0ezZs1MhQoVzIYNG7zL1q9fbypUqGAee+wxOlzqGD58uImOjjYvvPCC+eyzz8xnn31mevXqZaKjo82IESOCrsOmlj59+pisWbOasWPHmjVr1pg1a9aYMWPGmKxZs5oXX3yRDpc6nnzySVOwYEEzb948k5iYaBITE82XX35pChUqZDp16kSHSx0zZswwoaGhplmzZmbUqFFm1KhRplmzZiYsLMy8//77jnUYY0yxYsVS3FYMHz7cFCtWLOg6bFl36fBly/4hHf5s2Q+hw5ctzzO2bNtt6QgkDKUCREREhNmyZYvf8s2bN5uIiIigbLGlIzY21vz6669+y3/55ReTPn16OlzqyJ8/v5k2bZrf8qlTp5r8+fMHXYdNLTly5DCfffaZ3/JPP/3U5MyZkw6XOjJnzmwWLVrkt3zhwoUmS5YsdLjUYdPOcXh4uNm6davf8q1btzr6vGtLhy3rLh2+bNk/pMOfLfshdPiy5XnGlm27LR2BhNP3AkTWrFm1evVqv+WrV69WtmzZgrLFlo7k5GS/l06VpLCwMMcPm6fjfw4cOKDKlSv7La9cubIOHDgQdB02tRw7dkzFihXzW16sWDEdO3aMDpc6zpw5o7i4OL/l2bJlc/R0NTp87dixQ40aNfJb3rhxY/3++++OdUhSnjx5tGDBAr/l8+fPV548eYKuw5Z1lw5ftuwf0uHPlv0QOnzZ8jxjy7bdlo5AwoXOA0THjh315JNPaseOHd6N1w8//KDXXntNPXr0CMoWWzpq166t7t2764MPPlDOnDklSfv27dNzzz2nOnXq0OFSR+HChfXhhx/qxRdf9Fk+e/ZsFSlSJOg6bGopU6aMRo8erbfeestn+ejRo1WmTBk6XOqoVKmSEhISNH36dEVGRkqSzp49q4EDB6pSpUp0uNRxZee4cOHCPsvd2Dnu2bOnunXrptWrV/s8706dOlWjRo0Kug5b1l06fNmyf0iHP1v2Q+jwZcvzjC3bdls6Aorbh2rh1khOTjYjRowwuXLlMh6Px3g8HpMrVy4zcuRIk5ycHJQttnTs3r3blC1b1oSFhZmCBQuaggULmrCwMBMfH2/27NlDh0sdc+bMMWnSpDH169c3gwYNMoMGDTL169c3oaGhZu7cuUHXYVPL4sWLTUxMjClevLh54oknzBNPPGGKFy9u0qZNa5YsWUKHSx1r1641OXPmNJkzZza1a9c2tWvXNpkzZza5cuUy69ato8OljrFjx5rw8HDTqVMnM336dDN9+nTz1FNPmYiICDN+/HjHOq6YO3euqVKlismUKZPJlCmTqVKlivn000+DssOWdZcOX7bsH9Lhz5b9EDp82fQ8Y8O23aaOQOEx5v+/viRuW5cuXdLMmTNVv359xcXF6eTJk5KkdOnSBW2LLR1XGGM0f/58bdq0SZJUvHhx1a1blw6XO1auXKkRI0Z4X861ePHi6tmzp+Lj44Oyw6aW/fv3a8yYMT4/I88884z36Do63Ok4c+aM3n//fZ+Oli1bKioqig4XOz755BMNHz7cZ73t1auXHnzwQccaLl26pCFDhuiJJ55Q7ty5HXtcWzuusGXdpeMyW/YP6bg2W/ZD6PDl9vOMLdt2WzoCjqsjMdwyUVFRZufOnW5nGGPsabGh48KFCyZNmjTmt99+o8Oyjnbt2pkdO3bQYVnLhQsXTO3atVO86Cod7nYULFjQ51Uz6XC/4+LFi2bgwIGOHmV6PTExMeb33393O8OKDpvWXTp82bB/SIc/m/ZD6Pgfm55nbNi229QRSLjQeYC4++67tWrVKrczJNnTYkNHWFiY8ubNq6SkJDos6/j4449dbbCpQ7KnJSwsTGvXrnU7g44UOs6dO+d2Bh1/EhoaqmHDhunSpUtup0iS6tSpo++//97tDCs6bFp36fBlw/4hHf5s2g+h439sep6xYdtuU0cg4ULnAeKZZ55Rz549tXfvXpUvX14xMTE+t5cuXTroWmzp6Nevn1588UXNmDFDmTJlcuQx6fhrTZo00aeffqrnnnvOtQabOmxqadWqlSZNmqRXX32VDos6OnfurNdee00TJ05UaKh7uw90+Lqyc5w/f37XGq5o0KCB+vTpo99++y3F593GjRsHVYct6y4dvmzZP6TDny37IXT4suV5xpZtuy0dgYRrSgWIkBD/g948Ho+MMfJ4PI4emWJLiy0d8fHx2rZtmy5evKh8+fL5bbhWrlxJhwsdgwcP1vDhw1WnTp0Un1C6desWVB02tXTt2lXTp09XkSJFUuwYMWIEHS50NG3aVAsWLFDatGlVqlQpv465c+fS4ULH+PHjNXDgQLVs2dL1neOUnnevcPv5340OW9ZdOnzZsn9Ihz9b9kPo8GXL84wt23ZbOgIJQ6kAsWvXruveni9fPodK7GmxpWPgwIHXvT0hIYEOFzoKFChwzds8Ho927NgRVB02tdSqVeu6HQsXLqTDhY527dpd9/YpU6bQ4UIHO8f2smXdpcOXLfuHdPizZT+EDl88zyC1MZQCAAAAAACA47imVADZunWrFi1apMOHDys5OdnntgEDBgRliy0dknThwoUUO/LmzUuHix0AgFtnwYIFWrBgQYrb98mTJwddB+xky/4hHcDNsWXbbktHoOBIqQDx7rvv6umnn1aWLFmUPXt2eTwe720ej8ex6/TY1GJLx5YtW9S+fXv9+OOPPsudPlefDl9JSUmaOnXqNZ9QnDqNwJYOm1pOnz6tV1999ZodTh2uToevQ4cO6fnnn/d2/Hn3wal1lw5/tuwcDxw4UIMGDVKFChWUI0cOn+ddSfrkk0+CqsOWdZcOX7bsH9Lhz5b9EDr82fA8Y8u23ZaOQMKRUgFi8ODB+r//+z/17t3b7RRrWmzpaNeunUJDQ/Xvf/87xQ0XHe50dO/eXVOnTlXDhg1VsmTJoO+wqaVDhw76/vvv9fjjj7v6M0KHr7Zt22r37t3q378/HRZ1/NXOsZPGjx+vqVOn6vHHH3etwaYOW9ZdOnzZsn9Ihz9b9kPo8GXL84wt23ZbOgKKQUBIly6d2b59u9sZxhh7WmzpiI6ONhs3bnQ7g44/yZw5s/nyyy/dzrCmwxh7WtKnT2+WLVvmdgYdf5I2bVqzatUqtzPo+JPs2bOb6dOnu51hjDEmU6ZMZtu2bW5nWNNhy7pLhy9b9g/p8GfLfggdvmx5nrFl225LRyC59qX0cVt55JFH9O2337qdIcmeFls6SpQooaNHj7qdQcefhIeHq3Dhwm5nWNMh2dOSMWNGZcqUye0MOv4kT548fqeo0eF+x4ULF1S5cmW3MyRdPhJm5syZbmdY02HLukuHL1v2D+nwZ8t+CB2+bHmesWXbbktHIOGaUgFi6NChGjFihBo2bKhSpUopLCzM5/Zu3boFXYstHQsXLtRLL72kIUOGpNgRGxtLhwsdw4cP144dOzR69GhXT3expcOmlvfee0+fffaZpk2bpujoaDos6fj22281fPhwTZgwQfnz56fDko7evXsrbdq06t+/v2sNV3Tv3l3Tp09X6dKlVbp0ab/t+4gRI4Kqw5Z1lw5ftuwf0uHPlv0QOnzZ8jxjy7bdlo5AwlAqQBQoUOCat3k8HscuHmlTiy0dISEh3se8mnH4wt50+GratKkWLVqkTJky6c477/R7Qpk7d25QddjUEh8fr+3bt8sYo/z58/t1OHXRVTp8ZcyYUWfOnNGlS5cUHR3t13Hs2DE6XOiwaee4Vq1a17zN4/E4dlFeWzpsWXfp8GXL/iEd/mzZD6HDly3PM7Zs223pCCRc6DxA/P77724neNnSYkvHokWL3E6QRMefZciQQU2bNnU7w5oOyZ6WJk2auJ0giY4/GzlypNsJkuj4s7Vr16ps2bKSpHXr1vnc5vRf1m3ZvtvSYcu6S4cvW/YP6fBny34IHb5seZ6xZdtuS0cg4UipAHPhwgX9/vvvKlSokEJD3Z052tJiSwcAAMFg27Zt2r59u6pXr66oqCjvkbDB2gE72bJ/SAdwc2zZttvSEQi40HmAOHPmjNq3b6/o6Gjdeeed2r17tySpa9euevXVV4OyxZYOSVq6dKlatWqlypUra9++fZKkGTNmaNmyZXS42HHp0iXNnz9fEyZM0MmTJyVJ+/fv16lTp4Kyw6aW48ePa+LEierbt6/3NKiVK1d6f17ocKdj+/bteumll9S8eXMdPnxYkvTVV19p/fr1dLjYIV3eOf7mm2909uxZSXLlIuz//e9/VadOHd1xxx26//77deDAAUlS+/bt1bNnz6DrkOxZd+n4H1v2D+lImS37IXT4c/t5xpZtuy0dAcW5F/pDaurWrZspX768Wbp0qYmJifG+tOunn35qypYtG5QttnTMmTPHREVFmQ4dOpiIiAhvx9tvv20aNGhAh0sdO3fuNMWKFTPR0dEmTZo03o5u3bqZp556Kug6bGpZs2aNyZo1qylcuLAJDQ31dvTr1888/vjjdLjUsXjxYhMVFWXq1q1rwsPDvR1Dhw41Dz/8MB0udRw9etTUrl3beDweExIS4u1o166d6dGjh2Mdxhjz+OOPm/r165s9e/aYtGnTelu+/vprU6JEiaDrsGXdpcOXLfuHdPizZT+EDl+2PM/Ysm23pSOQMJQKEHnz5jU//fSTMcb4rBxbt2416dKlC8oWWzrKli1rpk2b5texcuVKExcXR4dLHQ8++KBp1aqVOX/+vE/HokWLTOHChYOuw6aWOnXqmF69ehljfH9GfvjhB5MvXz46XOq45557zPDhw/06fvnlF5MrVy46XOqwaec4Li7OrF692hjj+zXZvn27iYmJCboOW9ZdOnzZsn9Ihz9b9kPo8GXL84wt23ZbOgIJJwwHiCNHjihbtmx+y0+fPu34ua22tNjSsXnzZlWvXt1vefr06XX8+HE6XOpYunSpfvzxR4WHh/ssz58/v6OnEdjSYVPLf/7zH02YMMFvea5cuXTw4EE6XOr47bffNHPmTL/l2bJl09GjR+lwqePbb7/VN998o9y5c/ssL1KkiHbt2uVYh3T5+TU6Otpv+bFjxxQRERF0Hbasu3T4smX/kA5/tuyH0OHLlucZW7bttnQEEq4pFSAqVKigL7/80vvvK08iEydOVKVKlYKyxZaO7Nmza9u2bX7Lly1bpoIFC9LhUkdycrKSkpL8lu/du1fp0qULug6bWiIiInTixAm/5Vu2bFHWrFnpcKkjQ4YM3usmXG3VqlXKlSsXHS512LRzXK1aNU2fPt37b4/Ho+TkZA0bNuy6L6EdqB22rLt0+LJl/5AOf7bsh9Dhy5bnGVu27bZ0BBS3D9XCrbF06VKTNm1a06lTJxMZGWm6d+9u7r33XhMTE2OWL18elC22dAwZMsSUKFHC/PzzzyZdunRm6dKl5r333jNZs2Y1b731Fh0udTRr1sx07NjRGHP50NsdO3aYkydPmtq1a5u2bdsGXYdNLe3btzdNmjQxFy5c8Hbs2rXLxMfHm+7du9PhUkfPnj1N1apVzYEDB0y6dOnM1q1bzbJly0zBggXNyy+/TIdLHQ0aNDAvvfSSMeZ/621SUpJ55JFHHL22lTHG/PbbbyZbtmzmvvvuM+Hh4eZf//qXKV68uImLizPbtm0Lug5b1l06fNmyf0iHP1v2Q+jwZcvzjC3bdls6AglDqQCybds206FDB3PXXXeZ4sWLm5YtW5q1a9cGdYsNHcnJyWbw4MEmJibGeDwe4/F4TGRkpHfjToc7HXv27DElSpQwxYsXN6Ghoeaee+4xmTNnNkWLFjWHDh0Kug6bWo4fP27q1q1rMmTIYNKkSWPy5MljwsLCTPXq1c2pU6focKnj/PnzpkOHDiY0NNR4PB4TFhZmQkJCTKtWrcylS5focKnDtp3j48ePm8GDB5tHHnnENGjQwPTr18/s378/KDtsWXfp8GfD/iEd/mzZD6HDl03PMzZs223qCBQeY1x4zWC45tVXX1WnTp2UIUMGt1OsaXGq48KFC9q2bZtOnTqlEiVKKG3atD637927Vzlz5lRISOqeVUvH/1y6dEmzZ8/WmjVrdOrUKZUrV04tW7ZUVFRUqj2mzR22tfzwww8+HXXr1nW8gQ5/e/bs0W+//aZTp04pPj5eRYoUocPljsTERI0ePdrn56Nz587KkSOH4y034plnntGgQYOUJUuWoOiwZd2l4+YE236qLR227IfQ4et2ep4JtueYgOD2VAzOSpcunfcVAtxmSwsddFzP/fffb8VfPmzpMMaelpIlS5rdu3e7nUHHn9iy7tLh6+mnnzZHjhxxO8MYY8/XxJYOW9ZdOnzZ8vNBhz9b9kPo8GXL84wtP6u2dNwOuNB5kDEWHRhnSwsdvujwtWTJEp09e9btDGs6JHtadu7cqYsXL7qdQcef2LLu0uHrvffeS/Ei026w5WtiS4ct6y4dvmz5+aDDny37IXT4suV5xpafVVs6bgcMpQAAAAIcO8cAgNTE8wz+LoZSAAAAAAAAcBxDKQAAAAAAADiOoRRgCY/H43aCJDoA/DO2rLt0AAAA2I+hVJCpVq2aKy/tnhJbWmzpsOU8bDoA/BO2rLt02KtVq1aKjY11O8OaDtjJlv1DOoCbY8u23ZaO20Go2wG4NVq3bq1atWqpevXqKlSo0DXvN2/evKBpsaVj4cKFqly5siIjI697vw0bNihnzpx0ONRxo1588UVlypTJ7QxrOiR7WiZMmKC4uDi3M4KmY8eOHSpYsOBf3u+rr75Srly56HCo40Y5sXNcvXp11axZUzVq1FCVKlWuuZ0fN25cUHTcqGDZhtjUkZycrG3btunw4cNKTk72ua169eqSnNlnpuPvsWU/hA5fTjzPHD9+XL/++muKP6utW7eW5My23ZaOQOEx/AkvIHTo0EFLlizRtm3blCtXLtWoUcO7Q1akSJGgbLGlI23atLp06ZLuuusun51kp//aRIe/rVu3atGiRSk+oQwYMCDoOmxqWbBggRYsWJBix+TJk+lwoSMkJES5c+f22ZYWLlzYkcem49pudADjhMGDB2vJkiX68ccfdenSJVWoUMGnLTo6Oqg6JDvWXTp8/fzzz2rRooV27drldySjx+NRUlISHS50SFLevHm962rNmjWv+0dlOpx1I0OY1PbFF1+oZcuWOnXqlGJjY31Oj/d4PDp27FhQdQQShlIBZt++fVqyZIm+//57ff/999qyZYty5MihvXv3Bm2L2x0XL17Ur7/+6n38H3/8URcuXFCFChVUq1YtDR48mA4XOt599109/fTTypIli7Jnz+73hLJy5cqg6rCpZeDAgRo0aJAqVKigHDly+F2T55NPPqHDhY59+/Zp8eLF3nV369atypkzp2rUqKFatWqpQ4cOdLjQYdMA5opLly7pP//5j77//nstXrxYCxcuVEhIiM6dOxdUHbasu3T4Klu2rO644w4NHDgwxY706dPT4UKHJL333ntasmSJFi9e7PNH5StDGaf+qEyHL1uGMHfccYfuv/9+DRkyxJXnNts6AopBQDl9+rT55ptvTJ8+fcw999xjwsPDTdmyZYO6xZaOK9atW2fatGljQkNDTUhICB0udeTNm9e8+uqrjj2e7R3G2NOSPXt2M336dLcz6PgLW7ZssWIbQsdlFy9eND/++KMZOnSoqV+/vgkLCzMRERGOdxhjzObNm82ECRPMY489ZnLkyGEyZcpkmjRpEnQdtqy7dPiKjo42W7dudTuDjr+wf/9+88EHH5iWLVu6un2nw5giRYqY7t27m9OnTzv2mCmJjo4227dvd7XBpo5AwjWlAsSLL76oxYsXa9WqVSpevLhq1KihPn36qHr16sqYMWNQttjSsWXLFi1evNj7F/Xz58+rWrVqeuONN1SzZk06XOr4448/9Mgjjzj2eLZ3SPa0XLhwQZUrV3Y7g44/OXPmjJYtW+Zdf1etWqVixYqpS5cujq67dKRsx44d+u2337RmzRqtXbtW6dKl814LxiktWrTwbterV6/ufd4tXbq0o69CaEuHLesuHb4qVqyobdu2uXK6LR1/7ept66JFi7Rq1SqVLFnS8e0qHf+zb98+devWzfWjgurXr6/ly5ff0PUcg6EjkHD6XoAICQlR1qxZ9dxzz+mhhx7SHXfcEfQttnV0795dDzzwgEqVKuXKS4TT4at9+/a666671KlTJ8cf28YOm1p69+6ttGnTqn///nRY1BEeHq6MGTOqZcuWqlmzpqpVq+b4Hz3o8JfSAKZmzZqOD2Cky9v3LFmy6IknnlDt2rVVtWpVV36JsaXDlnWXDl+ffPKJXnrpJfXq1UulSpVSWFiYz+2lS5emw4UOSapcubL3j8lXTkN24w/sdPh66KGH9Nhjj6lZs2aOPu6fTZo0SYMGDVK7du1S/Flt3LhxUHUEEoZSAWLNmjXeayYsXbpU4eHh3h3TmjVrOjqQsaXFlo5nn31WS5Ys0YYNG1SuXDnv4zu9k0yHr6FDh2rEiBFq2LBhik8o3bp1C6oOm1q6d++u6dOnq3Tp0ipdurRfx4gRI+hwoaNJkyZatmyZwsPDveut088vdPizZQAjXT7acunSpd4jYTdu3KiyZct6vzb16tULqg5b1l06fIWEhPgt83g8MsY4emFvOvxlypRJISEhqlevnmvbVDr82TKESeln9Qq31xk3OgIJQ6kAtWbNGr355pt6//33lZyc7OrKYUuL2x3Hjx/X0qVLvRfFXb9+veLj4/XDDz/Q4UJHgQIFrnmbx+PRjh07gqrDppZatWpdt2PhwoV0uNBxxdq1a73r7dKlSxUaGqqaNWvq/fffp8OFDlsGMCnZtm2bBg8e7Przv1sdtqy7dPjatWvXdW/Ply8fHS50SJIxRr/99pt3e7ZkyRLvH5Vr1aqljh070uFCB0MYpDaGUgHCGKNVq1Z5r22xbNkynThxQqVLl1aNGjX05ptvBl2LLR1X/Pe//9X333+vRYsWafHixdqwYYMyZsyoo0eP0uFwhzFGu3fvVrZs2RQVFZXqj2d7h00tSUlJ+uGHH1SqVClXToWi469d2bYuWrRIixYt0jfffCNjjC5dukSHix1XuDkIurJdv/K8u2HDBmXIkMF7WmH37t2DpsOWdZcOXxcvXlSxYsX073//W8WLF6fDko6UGGO0YsUKjR492tXBNh12uHjxoqKiorR69WqVLFky6DsCDRc6DxCZMmXSqVOnVKZMGdWoUUMdO3ZUtWrVlCFDhqBtsaWjW7duPkOX6tWrq2PHjqpZs6ZKlSpFhwsdxhgVKVJE69evd+zldG3usKklTZo0qlevnjZu3OjqLy50+BsxYoR3wH/y5EmVKVNG1atX15NPPqlq1arR4VLHtQYwjRo1Uo0aNRzrkKRs2bIpS5YsqlatmivPLzZ12LLu0uErLCxM586dc+3x6bi+lStX+vwx+eTJkypVqpS6du3q6PaMjv+xZQgTFhamvHnzuj6Is6Uj0DCUChDvvfeeqlWrptjYWLdTrGmxpePAgQN68sknVbNmTVc35nT8T0hIiIoUKaL//ve/rg5gbOmwraVkyZLasWPHdU8npMN5H3zwgWrUqOEduqRPn54OCzpsGMBcsXbtWt15552uPLaNHbasu3T46ty5s1577TVNnDhRoaHu/SpEh7+7775b8fHx3j8mV69e3ZVtKx3/Y9MQpl+/fnrxxRc1Y8YMZcqUKeg7Agmn7wWgvXv3SpJy587tcok9LbZ0wB5ffPGFhg0bpnHjxrk6pLOlw6aWr7/+Wn379tUrr7yi8uXLKyYmxud2pwbNdOB2sH79eisGMFc7cuSINm/eLEkqWrSosmbNGpQdtqy7dPhq2rSpFixYoLRp06pUqVJ+HXPnzqXDhQ5JOnHihBXPaXT4mjRpkubOnev6ECY+Pl7btm3TxYsXlS9fPr+f1ZUrVwZVRyBhKBUgkpOTNXjwYA0fPlynTp2SJKVLl049e/ZUv379rnuBukBtsaVDkrZv366RI0dq48aNkqQSJUqoe/fuKlSokGMNdPjKmDGjzpw5o0uXLik8PNzvOkrHjh0Lqg6bWq5eN69+SXs3X5GIjsuOHz+uSZMm+ay77du3d/wvt3T4c3sAI0mnT59W165dNX36dCUnJ0u6fNpW69at9fbbbzv2qoC2dNiy7tLhq127dte9fcqUKXS40HG1FStW+GxXy5Ur53gDHf9jyxBm4MCB1709ISEhqDoCCUOpANG3b19NmjRJAwcOVJUqVSRJy5Yt08svv6yOHTvq//7v/4KuxZaOb775Ro0bN1bZsmW9HT/88IPWrFmjL774Qvfeey8dLnRMmzbture3adMmqDoke1q+//77697u1HUU6PC1fPly1a9fX1FRUbr77rslSf/5z3909uxZffvtt47tJNPhy5YBjCQ99dRTmj9/vkaPHu3zvNutWzfde++9GjduXFB12LLu0oHbxeHDh/Xoo4/q+++/914D9vjx46pVq5ZmzZrl2LCdDl8MYZDqDAJCjhw5zGeffea3/NNPPzU5c+YMyhZbOsqWLWt69+7tt7x3794mPj6eDpc6ANycqlWrmrZt25qLFy96l128eNG0adPGVKtWjQ6XOp588klTsGBBM2/ePJOYmGgSExPNl19+aQoVKmQ6derkWIcxxmTOnNksWrTIb/nChQtNlixZgq4D9rp48aL57rvvzPjx482JEyeMMcbs27fPnDx5kg4XO5o1a2YqVKhgNmzY4F22fv16U6FCBfPYY4/R4VKHTf744w/z7rvvmj59+pj//ve/xhhjVqxYYfbu3RuUHYGCoVSAiIiIMJs3b/ZbvmnTJhMZGRmULTZ1bNmyxW/55s2bTUREBB0udRhjzLZt20y/fv3MY489Zg4dOmSMMWbevHlm3bp1QdlhU8uSJUtMy5YtTaVKlbxP8NOnTzdLly6lw6WOyMhIs3HjRr/l69evN1FRUXS41GHTACYqKsrnl6cr1q1bZ6Kjo4Ouwxg71l06fO3cudMUK1bMREdHmzRp0pjt27cbY4zp1q2beeqpp+hwqcMYY2JjY82vv/7qt/yXX34x6dOnp8OlDmPsGMKsWbPGZM2a1RQuXNiEhoZ6f1b79etnHn/88aDrCCTOXVQHqapMmTIaPXq03/LRo0erTJkyQdliS0fWrFm1evVqv+WrV69WtmzZ6HCp4/vvv1epUqX0yy+/aO7cud7rjq1Zs8bRw5Bt6bCp5eOPP/aeFrVy5UqdP39ekpSYmKghQ4bQ4VJHbGysdu/e7bd8z549SpcuHR0udZw5c0ZxcXF+y7Nly6YzZ8441iFJlSpVUkJCgs9LzJ89e1YDBw5UpUqVgq7DlnWXDl/du3dXhQoV9Mcff/hcO/HKBb/pcKdDunw92LCwML/lYWFh3tOT6XC+Y+3atbrjjjv02muv6Y033tDx48clXb4Ift++fR3r6NGjh9q2bautW7cqMjLSu/z+++/XkiVLgq4joLg9FcOtsXjxYhMTE2OKFy9unnjiCfPEE0+Y4sWLm7Rp05olS5YEZYstHQMHDjQZMmQwr776qlmyZIlZsmSJGTp0qMmQIYMZNGgQHS513HPPPWb48OHGGGPSpk3r/SvHL7/8YnLlyhV0HTa1lC1b1kybNs2vY+XKlSYuLo4Olzq6du1qcufObWbNmmV2795tdu/ebT744AOTO3du0717dzpc6qhdu7Z55JFHzNmzZ73Lzpw5Yx555BFTp04dxzqMMea3334zOXPmNJkzZza1a9c2tWvXNpkzZza5cuVy9GhLWzpsWXfp8JUpUyazadMmv47ff//d0aMc6fDXuHFjU716dbNv3z7vsr1795oaNWqYJk2a0OFSR506dUyvXr2MMb4/Iz/88IPJly+fYx2xsbFm27Ztfh07d+509GwLWzoCCUOpALJv3z7z4osvmoceesg89NBDpl+/fj4bsWBssaEjOTnZjBgxwuTKlct4PB7j8XhMrly5zMiRI01ycjIdLnXExMSYHTt2GGP8d8KcfEKxpcOmlqioKPP777/7dWzfvp0OFzvOnz9vunXrZsLDw01ISIgJCQkxERER5tlnnzXnzp2jw6UOWwYwV5w+fdq88847pkePHqZHjx7m3XffNWfOnAnKDlvWXTp8ZciQwaxfv96vY+nSpSZbtmx0uNRhjDG7d+82ZcuWNWFhYaZgwYKmYMGCJiwszMTHx5s9e/bQ4VKHLUOYrFmzmpUrV/p1fPvttyZ37txB1xFIQt0+Ugu3Ts6cOR19lb3rsaXFhg6Px6PnnntOzz33nE6ePClJjp7aQUfKMmTIoAMHDqhAgQI+y1etWqVcuXIFXYdNLdmzZ9e2bduUP39+n+XLli1TwYIF6XCpIzw8XKNGjdLQoUO1fft2SVKhQoUcfXU3OvyVLFlSW7du1fvvv69NmzZJkpo3b66WLVv6nIrjlOjoaHXs2NHxx7Wxw5Z1lw5f9erV08iRI/XOO+9IurxfcurUKSUkJOj++++nw6UOScqTJ49Wrlyp+fPne7dnxYsXV926delwsSMiIkInTpzwW75lyxbHXgFQkho3bqxBgwbpww8/lHT5Z3X37t3q3bu3Hn744aDrCChuT8Vw6xw7dsy8/vrr3lPV3njjDe+F6IK1xZYOY4w5dOiQ93S1w4cPu9JAx//07NnTVK1a1Rw4cMCkS5fObN261SxbtswULFjQvPzyy0HXYVPLkCFDTIkSJczPP/9s0qVLZ5YuXWree+89kzVrVvPWW2/R4VLH1a6cruY2OuyzadMm07lzZ+9RW507d07xgvDB0GHLukuHrz179pgSJUqY4sWLm9DQUHPPPfeYzJkzm6JFi3pf4IMO5ztgr/bt25smTZqYCxcumLRp05odO3aYXbt2mfj4eEdPVz9+/LipW7euyZAhg0mTJo3JkyePCQsLM9WrVzenTp0Kuo5AwlAqQHz//fcmNjbW5MmTxzRt2tQ0bdrU5M2b18TGxprvv/8+KFts6Thx4oRp1aqVSZMmjfd0tdDQUNOyZUtz/PhxOlzqOH/+vOnQoYMJDQ01Ho/HhIWFmZCQENOqVStz6dKloOuwqSU5OdkMHjzYxMTEeH9GIiMjzUsvveRYAx3+Ll68aF566SUTGxvrPV0tNjbW9OvXz1y4cIEOlzqMsWMAY4wxc+bM8f5S+9xzz5nnnnvOVKpUyYSGhpo5c+YEXYct6y4d/i5evGhmzJhhevXqZZ5++mnXTjOlw9/8+fNNw4YNvaerNWzY0Hz33Xd0uNhh2xBm6dKlZsyYMea1115z5XtiW0cgYCgVIEqWLGk6duzo84vjpUuXzJNPPmlKliwZlC22dDRr1swUKVLEfP311yYxMdEkJiaar7/+2hQtWtQ8+uijdLjUccWuXbvMl19+aWbPnm22bNni+OPb1mFTy/nz58369evNL7/8Yk6ePEmHyx2dOnUy2bJlM+PHjzdr1qwxa9asMePHjzfZs2c3nTp1osOlDlsGMMYYU7BgQdO/f3+/5QMGDDAFCxYMuo4r3F536cDtYsyYMSY0NNQ89thjZtSoUWbUqFGmefPmJiwszIwePZoOlzquYAiD1MJQKkBERkZ6Xznjaps2bTKRkZFB2WJLR3R0tFm6dKnf8iVLlpjo6Gg6XOq4WnJysqMXWbe9wxh7Wmw5LYqOyxc6nTdvnt/yL7/80sTGxtLhUodNA5ioqCizdetWv+Vbtmxx9FW8bOm4GtsQuzpsObqQDl+5cuUyb7/9tt/y0aNHm5w5c9LhUodNbDhyzKaOQBHi9jWtcGuUK1dOGzdu9Fu+ceNGlSlTJihbbOnInDmz0qdP77c8ffr0ypgxIx0udUjSpEmTVLJkSUVGRioyMlIlS5bUxIkTHW2wqcOWlkuXLql///5Knz698ufPr/z58yt9+vR66aWXdPHiRTpc6oiIiPC7QLEkFShQQOHh4XS41HHgwAG1bt3ab3mrVq104MABxzokqWbNmlq6dKnf8mXLlqlatWpB12HLukuHr48//lglS5bUihUrVKZMGZUpU0YrV65UqVKl9PHHH9PhUockHT9+XPfdd5/f8nr16ikxMZEOlzokacGCBXrggQdUqFAhFSpUSA888IDmz5/vaMPYsWN13333KV26dOrevbu6d++u2NhY3X///RozZkzQdQQUt6di+PuunC6wZs0aM2vWLJM3b17z+uuvm6VLl5qlS5ea119/3eTPn9/MmjUraFps6bjahAkTTN26dc2BAwe8yw4cOGDq1atnxo8fT4dLHf379zcxMTGmT58+5rPPPjOfffaZ6dOnj0mbNm2KRx0EeodNLbacFkWHr4EDB5rmzZubc+fOeZedO3fOtGzZ0tEL4dPhq0GDBmby5Ml+yydPnmzq1auX6o9/ZVvx2WefmXHjxpmsWbOazp07mxkzZpgZM2aYzp07m2zZsplx48YFRcfVbFl36fBly9GFdPhr3ry5GTZsmN/y119/3dFLPNDhy5bTCG05csyWjkDiMcYYtwdj+HtCQkLk8Xj0V99Cj8ejpKSkoGixpSM+Pl4ej8f7761bt+r8+fPKmzevJGn37t2KiIhQkSJFtHLlSjoc6rha1qxZ9dZbb6l58+Y+yz/44AN17dpVR48eDaoOm1rSp0+vWbNmqUGDBj7L582bp+bNmzv210E6pIceesjn3/Pnz1dERIT3aNM1a9bowoULqlOnjubOnUuHQx2ff/659//379+vAQMGqFmzZrrnnnskST///LM++ugjDRw4UJ06dUq1Duny8+6NcOL534aOq7ENsbMjOjpaa9euVeHChX2Wb926VWXKlNGZM2focLDjrbfe8v7/iRMn9MYbb6hKlSqqVKmSpMvbsx9++EE9e/bUSy+9RIdDHVfLnTu3+vTpoy5duvgsHzNmjIYMGaJ9+/Y50pE2bVqtXr06xZ/V+Ph4nTp1Kqg6Akmo2wH4+37//Xe3E7xsabGlo0mTJm4nSKLjei5evKgKFSr4LS9fvrwuXboUdB02tdhyWhQd8jvV9uGHH/b5d548eVL18elIWUrb1LFjx2rs2LE+yzp37pzqQ6nk5ORU/fg3ypaOq7ENsbPjyumdf/6F0q3TTIO948033/T5d8aMGbVhwwZt2LDBuyxDhgyaPHlyqg5h6Li2651G2Lt3b0caJKlx48b65JNP1KtXL5/ln332mR544IGg6wgkHCkVZBo2bKiJEycqR44cbqdY02JLxwcffKDGjRsrJiaGDgc6unbtqrCwMI0YMcJn+fPPP6+zZ886dk64LR02tQwaNEibNm3SlClTFBERIUk6f/682rdvryJFiighIYEOFzpu1A8//KAKFSp4W+mwo8MmpUqV0rx58xwb4rnVYcu6S4c9RxfSgdtRixYtFB8f7zeEeeONN7R8+XLNmjUr1R7bliPHbOkIVAylgky6dOm0Zs0aFSxY0O0Ua1ps6YiNjdXq1avpSMWOHj16eP//0qVLmjp1qvLmzevdCfvll1+0e/dutW7dWm+//fYte1xbO2xqseW0KDr+uUDehtzOHbYMgiR7nndTo8OWdZcOX7ac3knHrWHLdjWQO2wZwhQoUOCG7ufxeLRjx46A7whUnL4HWMKW+XAgd6xatcrn3+XLl5ckbd++XZKUJUsWZcmSRevXr7/lj21jh00ttpwWRcc/F8jbkL/Dlo6dO3c6+gpnwcqWdZcOX7ac3knHrWHLdjWQO2w5jdCWS7PY0hGoGEoBCBqLFi266ffZu3evcubMecN/VbydOmxqmTJlyk2/T2qcFkUHgH/ClnWXjn/OlqML6YAbbuchTCAfwRaobu1vNwAQYEqUKKGdO3e6nWFNh2RPS4MGDRx7xRc6AKQWW9ZdOnzZcnQhHbhdxMbGWnHqWiAfwRaoGEoBwHXY8oRiS4dkTwsdvmzpAHBzbFl36QDwT7Du4u9iKAUAAG4Zj8fjdoIkOgAgUNmyXaUDuDUYSgWZF198UZkyZXI7Q5I9LbZ05MuXT2FhYW5n0AHAjzFGu3fv1rlz527ovnQ402GTixcvqk6dOtq6detf3nfChAmKi4sL6A4AqceW7SodwK3Bhc4DyObNm/X2229r48aNkqTixYura9euKlq0qPc+ffv2DaoWWzokafny5T4dFSpU8Ll93bp1dLjQAeCvGWNUuHBhrV+/XkWKFLnufU+ePEmHQx0XL17Ufffdp/Hjx/9lR2oPYMLCwrR27dobum+LFi0CvgNA6vnqq6+UK1cutzPosJQtR47Z0nE7YCgVID7++GM99thjqlChgipVqiRJ+vnnn1WyZEnNmjXL7yV4g6HFlo69e/eqefPm+uGHH5QhQwZJ0vHjx1W5cmXNmjVLuXPnpsOFjhtlyxOKLR2SPS10+ErNjpCQEBUpUkT//e9//3L4kZro8GXbAKZVq1aaNGmSXn311VR/rNuh40YFwzbkZtjSAWf06NHjhu87YsQISVLVqlXpSOWOv8OWddeWI8ds6bgdMJQKEC+88IL69u2rQYMG+SxPSEjQCy+84OhQypYWWzo6dOigixcvauPGjd4jtDZv3qx27dqpQ4cO+vrrr+lwoeNG2fKEYkuHZE8LHb5Su+PVV19Vr169NG7cOJUsWTJVH4uOG2fTAObSpUuaPHmy5s+fr/LlyysmJsbn9iu/QAVDhzFGe/bsUbZs2RQZGfmX96XDmQ5bji6k439WrVrl8++VK1fq0qVL3n3ELVu2KE2aNCpfvvwtf2w6bi1b9odsOXLMlo7bgcfY8tODfyQ6Olpr165V4cKFfZZv3bpVZcqU0ZkzZ4KuxZaOqKgo/fjjj4qPj/dZvmLFClWrVo0OlzqutmfPHklSnjx5UrwtZ86cSpMmTdB02NJy+PBhbd68WZJUtGhRZcuWLVUfj47ry5gxo86cOaNLly4pPDxcUVFRPrcfO3aMDhc6unbtqunTp6tIkSKuDoIkqVatWte8zePxaOHChUHTkZycrMjIyBs6xZMOZ2XNmlU//vgjHZZ1SJe3V4sXL9a0adOUMWNGSdIff/yhdu3aqVq1aurZsycdLnTcqGXLlumuu+5SRETELfuYf+fIsdRgS0eg4kipAFGzZk0tXbrUbwCzbNkyVatWLShbbOnIkyePLl686Lc8KSlJOXPmpMOljkuXLmngwIF66623dOrUKUlS2rRp1bVrVyUkJHgvsp7SUCYQO2xqOXnypJ555hnNmjVLSUlJkqQ0adLo0Ucf1ZgxY5Q+ffpUfXw6UjZy5EhHHuev0OFr3bp1KleunKTLf0G/mtOnUixatMjRx7sWGzpsOcWTDn+2HF1Ih7/hw4fr22+/9Q5gpMt/ABg8eLDq1avn2BCGDntOI7TlyDFbOgIVQ6nb2Oeff+79/8aNG6t3795asWKF7rnnHkmXr5/00UcfaeDAgUHTYkvH1V5//XV17dpVY8aM8V7Me/ny5erevbveeOMNOlzq6Nq1q+bOnathw4Z5rzn2008/6eWXX9Z///tfjRs3Lqg6bGrp0KGDVq1apX//+98+Hd27d9dTTz2lWbNm0eFCR5s2bRx5nL9Chy8bBjBImS2neNLhy4bTO+lI2YkTJ3TkyBG/5UeOHEnVF42gw58tQ5irn+NGjBihdOnSXfPIsWDoCFScvncbCwkJuaH7eTwe71/XA73Flo6MGTP6/IX69OnTunTpkkJDL8+Br/x/TExMqp7iQce1pU+fXrNmzVKDBg18ls+bN0/NmzdXYmJiUHXY1BITE6NvvvnG7y9uS5cu1X333afTp0/T4VDHiRMnbvi+sbGxdDjUYZOHHnrohu87d+7cgO+4mi2neNLhy4bTO+lIWevWrbV06VINHz5cd999tyTpl19+Ua9evVStWjVNmzaNDhc6bDmNMFeuXPr222915513+ixft26d6tWrp/379wdVRyDhSKnbWHJystsJXra02NJhy2kddFxbRESE8ufP77e8QIECCg8PD7oOm1oyZ86c4ilp6dOn9zmEnY7U78iQIcNfngJmjEn1QT8dvmwawDh1+uhfsaXjarY899Hhy5ajC+nwN378eD3//PNq0aKF91IPoaGhat++vV5//XU6XOqw5XRGjmALXBwpBSAoDRo0SJs2bdKUKVO8F2Q8f/682rdvryJFiighISGoOmxqeeedd/TRRx9pxowZyp49uyTp4MGDatOmjR566CE99dRTdDjU8f3339/wfWvUqEGHQx3t2rW74ftOmTIl1Tr+rh9++EEVKlS4pRfDvZ07APg6ffq0tm/fLkkqVKiQ3ymFdDjbkS5dOn3xxReqWbOmz/JFixapcePGjg1ibDlyzJaOQMJQKoAsWLBACxYs0OHDh/2OGJo8eXJQttjSkZycrG3btqXYUb16dToc6vjz0QXz589XRESEypQpI0las2aNLly4oDp16jh6molbHba1XBEfH69t27bp/Pnzyps3ryRp9+7dioiI8LtA7sqVK+lwqONGPfPMMxo0aJCyZMlCh0UdNg1gYmNjtXr1ahUsWDDgOmw5xZMOX7YcXUgHbke2DGHOnDmj559/XpMnT07xyDGnhnW2dAQSTt8LEAMHDtSgQYNUoUIF5ciRw/FX3LGxxZaOn3/+WS1atNCuXbv05xmwE9f7ouN//nx6x8MPP+zzbyde4c6mDttarmjSpInjj5kSOv6e9957T88//7zrQxg6fDVo0MCKQZAkv22/W1Kjw5ZTPOnwZcvpnXRcX9OmTVP8efF4PIqMjFThwoXVokUL78W26XCmw5bTCKOjozV27Fi9/vrrrh45ZktHIOFIqQCRI0cODRs2TI8//rjbKda02NJRtmxZ3XHHHRo4cGCKwzGndgzoAOCEdOnSac2aNa4PP+iws8OmltTosOUUTzr+OVuOLgymjrZt2+rTTz9VhgwZvK/qtnLlSh0/flz16tXTmjVrtHPnTi1YsEBVqlShw6GOK9w+jRCBiyOlAsSFCxdUuXJltzMk2dNiS8fWrVs1Z84cFS5cmA6LOmC348ePa86cOdq+fbt69eqlTJkyaeXKlYqLi1OuXLnocKkDwPX9ncFKapziScc/Z8vRhcHUkT17drVo0UKjR4/2vqJ2cnKyunfvrnTp0mnWrFnq1KmTevfurWXLltHhUMcVMTExKl26dKo/zrXYcuSYLR2BJMTtANwaHTp00MyZM93OkGRPiy0dFStW1LZt29zOoONPDh06pMcff1w5c+ZUaGio0qRJ4/MWbB02taxdu1Z33HGHXnvtNb3xxhs6fvy4pMvXtejbty8dLnUASB3vvffeTV17iQ5n2HIySTB1TJo0Sc8++6x3ACNJISEh6tq1q9555x15PB516dJF69ato8PBjqZNm+qhhx7ye3v44YfVsmVLJSQkaPPmzanaIF0+m2LhwoVauXKlPB6PPB6PVq1apYULF+rSpUuaPXu2ypQpox9++CEoOgIJR0oFiHPnzumdd97R/PnzVbp0aYWFhfncPmLEiKBrsaWja9eu6tmzpw4ePKhSpUr5dTj1Fwc6fLVt21a7d+9W//79Xb3mmC0dNrX06NFDbdu21bBhw5QuXTrv8vvvv18tWrSgw6UO4FZxczt3NVs6gmnocCNs6YDzLl26pE2bNumOO+7wWb5p0ybvNcciIyNTfd2lw1f69Omvexrh7Nmz9dprr6X6aYS2HDlmS0cgYSgVINauXauyZctKkt+03OmdLltabOm4cuHoJ554wufxnbiwJx3XtmzZMi1dutT7M+IWWzpsavnPf/6jCRMm+C3PlSuXDh48SIdLHbi92TKAkewZOtjSAeCyxx9/XO3bt9eLL76ou+66S9Ll58AhQ4aodevWki5fp+zOO++kw8EOW4YwkyZN0g8//JDikWOVK1fWkCFD1KVLF1WrVi3VGmzqCCQMpQLEokWL3E7wsqXFlo7ff//d7QRJdPxZnjx5rPiFxJYOyZ6WiIiIFE/d2LJli7JmzUqHSx03qlWrVqn60u50/D1OrtuHDx/2nspRtGhRZcuWzef2kydPBlUHgBvz5ptvKi4uTsOGDdOhQ4ckXR6IPPfcc+rdu7ckqV69errvvvvocLDDliGMLUeO2dIRSBhKAaksX758bidIouPPRo4cqT59+mjChAnKnz9/0HfY1NK4cWMNGjRIH374oaTLR3js3r1bvXv39h5pR4fzHdLlC67/+uuvOnz4sJKTk31uu/JX23HjxtHhcIdkxwDm5MmTeuaZZzRr1izvjnmaNGn06KOPasyYMY69uqotHbi92fILZTB1XLhwQc8995z69eunEydOeF9ZrkSJEt5rW+bNm5cOhztsGcLYcuSYLR2BxGNs+LM4/rFz587p7bff1qJFi1LcOV65cmXQtdjSIUn79+/XsmXLUuzo1q0bHS50ZMyYUWfOnNGlS5cUHR3td22rY8eOBVWHTS2JiYn617/+peXLl+vkyZPKmTOnDh48qEqVKmnevHmOvQQxHb6++OILtWzZUqdOnVJsbKzPzqfH43Hs54MOXzYNYB599FGtWrVKb7/9tipVqiRJ+umnn9S9e3eVLVtWs2bNCqqOG5UuXTqtWbPG9VdXo4MOtzvq1aunhx56SJ06ddLx48dVrFgxhYWF6ejRoxoxYoSefvrpVHtsOq6tW7du+uCDD1IcwrRo0UKjRo3SxIkTNXXq1FQ9fS8pKUmvvvqqRo8e7XPkWJcuXdS7d2+lSZNGu3fvVkhIiHLnzh3wHYGEoVSAaNmypb799lv961//UlxcnN+kOiEhIehabOmYOnWqnnrqKYWHhytz5sx+v7js2LGDDhc6pk2bdt3b27RpE1Qdkl0tkvTDDz9ozZo1OnXqlMqVK6e6des6+vh0+Lrjjjt0//33a8iQIYqOjnb0sem4NpsGMDExMfrmm29UtWpVn+VLly7Vfffdp9OnTwdVx416+umn9corryhLlix0ONjxV0cXOoWO/8mSJYv3CJOJEyfq7bff1qpVq/Txxx9rwIAB2rhxIx0udNgyhDl79qyMMYqOjvY7cqx+/fqp9ri2dgQUg4AQGxtrli1b5naGMcaeFls6cufObQYPHmySkpLosKgD9po2bZo5d+6c3/Lz58+badOm0eFSR3R0tNm+fbtjj0fHjXcsXbrUb/mSJUtMdHS0oy158uQxa9eu9Vu+Zs0akytXrqDrMMaYP/74w3zzzTdmxowZZtq0aT5vdLjTceLECdOqVSsTGhpqPB6P8Xg8JjQ01LRs2dIcP36cDpc6jDEmKirK7Nq1yxhjzCOPPGJefvllY4wxu3fvNlFRUXS41HHmzBlz+vRpY4wxiYmJZs2aNWbEiBHm66+/dqzBGGPuvfdeM27cOGPM5W1JXFycyZ07t4mMjDRjx44Nuo5AwlAqQBQvXtysWbPG7QxjjD0ttnRkypTJbNu2ze0MOlJw6dIl89FHH5lBgwaZQYMGmTlz5piLFy8GbYctLSEhIebQoUN+y48ePWpCQkLocKmjadOmZvbs2Y49Hh03xqYBzIQJE0zdunXNgQMHvMsOHDhg6tWrZ8aPHx90HZ9//rlJly6d8Xg8Jn369CZDhgzet4wZM9LhUkezZs1MkSJFzNdff20SExNNYmKi+frrr03RokXNo48+SodLHcYYU6pUKTNq1Cize/duExsba3788UdjjDHLly83cXFxdLjUYcsQJnPmzGbdunXGGGPeffddU7p0aZOUlGQ+/PBDU6xYsaDrCCQMpQLEvHnzzH333Wd27tzpdoo1LbZ09OrVywwdOtTVBjr8rVu3zhQsWNBER0eb+Ph4Ex8fb2JiYkz+/PnNb7/9FnQdNrV4PB5z+PBhv+WrV6929BcXOnxNnDjR5M2b1yQkJJg5c+aYzz77zOeNDnc6bBnAGGNM2bJlTdq0aU1YWJgpVKiQKVSokAkLCzNp06b1blOuvAVDR5EiRUz37t29Rxi4hQ5fthxdSIe/jz76yISFhZmQkBBz7733epcPGTLE3HfffXS41GHLEMaWI8ds6QgkXFMqQBw5cuT/tXfncVXWef/H3+ewyXIOqAdcIxdMcGMwM1ArExOzMlBzwfJWUW8r0XCZpp8palmZ41rmhjcjLZrjgpVNUmZSipEJ2KJpJiAFpiYlN8py+Pz+YDjjCTVmbs91fT3n/Xw8eIx8YR7X6w+H8frw/V4Xhg8fjszMTN0flqxKiyodVqsVDz74IC5duoSuXbvW61i6dCk7dOiIiopCYGAgNm7ciMaNGwMALly4gLFjx+Ls2bM4cOCAS3Wo0BIREQGDwYC8vDx07twZ7u7/ekGs1WrFqVOnMHDgQNtb6NihTUedK18F/XsGg8H2kG12aNsRERGB77//HhUVFba3MBUWFsLLywsdOnSw+15Hv+Bj/vz5Df5eRz7XUZUOX19ffPXVV7o/oJod9oKDg7Fr1y507drVbv3IkSMYNGgQioqK2KFDR52SkhIUFxcjPDzc9nM2OzsbZrMZoaGh7NChw8fHB8eOHUNwcDCGDx+Ozp07Izk5GadPn0bHjh1RXl6uSUe3bt0wYcIExMXFoUuXLvjggw8QFRWFL7/8Eg888ABKSkpcqsOZuP/xt9DNYNSoUfjxxx/xwgsvXPWh3q7YokrHiy++iN27d6Njx44AUO/B3uzQpyM3NxeHDh2yDV+A2rfPLVy40PZmEVfqUKElNjbW1hETEwM/Pz/b1zw9PdGmTRsMHTqUHRp31Pn9mzL1wg57dX9PVKDlS1WuR5WOmJgYHDp0SPchDDvsPfvss5g+fTpef/11NG/eHEDtjf+sWbMwZ84cdujUUad58+a2jjo9e/Zkh44dISEhSE9PR1xcHHbv3o2kpCQAtQ/HN5vNmnXMnTsX8fHxSEpKQnR0tO3lHhkZGYiIiHC5DmfCnVJOwsfHB1lZWQgPD9c7RZkWVToaN26MZcuWYezYsexQqCM8PBzLli1Dv3797NY//vhjTJs2DV999ZVLdajUsnHjRowcORJeXl6aXI8df6yqqgre3t7Izc1Fly5d2KFIh4pKS0uxdetWnDx5ErNmzUKTJk1w+PBhNGvWDK1atXKpjg0bNmDBggUYN27cVXcGDx48mB06dKiyu5AddLPYunUr4uPjYbVaER0djYyMDAC1v2jOzMzEP/7xD81aVNg5plKHs+BOKScRGhqKS5cu6Z0BQJ0WVTq8vLzQu3dvvTPYAeC3336z/fnFF1/E1KlTMW/ePERGRgIADh48iAULFmDRokUu0aFaS51+/frh7NmzttcKZ2dn46233kKnTp0wadIkdujQ4eHhgeDgYM2OpLHj36PCAAaoPe7Tv39/+Pv7Iz8/HxMnTkSTJk2wfft2FBYWIi0tzaU6Jk6cCABYsGBBva9pecSTHfZU2V3IDrpZDBs2DH369LENYepER0cjLi5O0xYVdo6p1OE09H2kFd0ou3fvll69esnevXvl3Llztrdn1H24YosqHS+88IIkJiZqdj12XJvBYBCj0Wj7qHv18dU+d4UO1Vrq9OnTR9LS0kSk9oHNJpNJoqKixGKxyPz589mhU0dKSooMGjRIzp8/r9k12fHH8vLyJDAwUEJCQsTd3V1OnjwpIiKzZ8+Wxx57TNOW6OhomTVrloiI+Pn52Vr2798vt956q8t1EBER0R/j8T0nUbdt8PfP5BERTX/7pFKLKh1xcXH4+OOP0bRpU3Tu3LnedvXt27ezQ6OOffv2Nfh777nnHqfvANRqqdO4cWMcPHgQHTt2xMqVK/H2229j//79yMjIwOTJk/HDDz+wQ4eOuiMeVVVVuPXWW+Hr62v3da2OdbDDXv/+/dG9e3e8/PLLMJlMyMvLQ7t27XDgwAHEx8cjPz9fkw4A8Pf3x+HDh9G+fXu7loKCAnTs2BGXL192mQ5Vjniy4+pU2V3IDiIiHt9zGnv37tU7wUaVFlU6AgICMGTIEL0z2IH/bKjyxBNPYMGCBbBYLE7XoVpLnaqqKtvzkz766CPbc0ZCQ0NRXFzskGuy44+pcsSDHfa++OILrF27tt56q1atNH8DkJeXl92R4DrHjx9HYGCgS3WocsSTHfWpcryTHURE/6TrPi3S3OOPPy5nz57VO0NE1GlRpeOzzz6Ty5cv653Bjt8xmUy2ox/sqOXolp49e8rTTz8tmZmZ0qhRI8nNzRURkaysLGnVqpXDrssOuhkFBgbK4cOHRcT+qFpGRoa0bt1a05aEhASJjY2VyspK8fPzkx9++EEKCgokIiJCpk2b5nIdqhzxZIc9VY53soOIqBaHUi7GlW5s2cGOG+HKf6Cxo5ajW/bu3SsBAQFiNBpl3LhxtvVnnnlG4uLiHHZddtDNSJUBjIhIaWmp9O/fXwICAsTNzU1uueUW8fDwkLvvvlvKyspcruNPf/qT+Pn5iZeXl9x2220SERFh98EOfTrMZrN8//33ImL//2f5+fni5eXFDp06iMh18fieixGFHiGmSgs77LGD9Na3b1+cO3cOv/32Gxo3bmxbnzRpEnx8fGyf79+/Hz169LAdbWOHYzusViuWLVuGLVu2oLCwEJWVlXZf/+WXXxxyXXZc35IlSzBs2DAEBQXh0qVLuOeee1BSUoKoqCgsXLhQk4Y6/v7++PDDD7F//37k5eWhrKwM3bt3R//+/V2yQ5Ujnuywp8LxTnYQEV1Bz4kYac+Vdluwgx3scAxVWlTZTecqHXPmzJEWLVrIX//6V2nUqJE899xzkpCQIE2bNpUVK1Y47LrsaJjPPvtMVq1aJYsWLZIPP/xQ8+uLiGzcuPGqx64rKipk48aNLtdBalJldyE7iIhqcSjlYlS5mRRRp4Ud7GDHv0eVFnZo29GuXTt57733bNeqO+6xYsUKGTVqlMOuy47rU2kAYzQa5cyZM/XWz507J0aj0eU6SE2qHO9kBxFRLR7fIyIioj9UUlKCrl27AgD8/Pzw66+/AgAefPBBzJkzhx06dYwbNw4DBw5EUFCQ3frFixcxbtw4jBkzRrMWEYHBYKi3XlRUBH9/f5frUOWIJzvsqXK8kx1ERLU4lCJSxNX+Aa0Hdth79NFHYTab9c5QpgNQq4W007p1axQXFyM4OBjt27dHRkYGunfvji+++MJhz7Fixx9TYQATEREBg8EAg8GA6OhouLv/65+XVqsVp06dwsCBA12mo878+fORkpKCGTNm4Nlnn8Xs2bORn5+P9PR0zJ07lx06daSlpWHEiBHo3bs3evfubVuvrKzE5s2bNRvksoOIqBaHUi5GpZtJVVpU6RBFHuztSh2lpaXIzs7Gzz//jJqaGruv1f0jbPXq1S7ToVoLqSUuLg579uzBnXfeicTERDz66KPYsGEDCgsLkZSUxA6NO1QawNQ9wDo3NxcxMTHw8/Ozfc3T0xNt2rTB0KFDXaajzptvvon169fjgQcewLx58zBq1Ci0b98e3bp1w8GDBzF16lR26NChyu5CdhAR1TKIKneg9H/WkJtJV2tRpQMAfv75Z3z33XcAgI4dO9b7P392aNvx7rvvYvTo0SgrK4PZbLbbaWAwGDQ7RqBKh2otDWE2m5Gbm4t27dqxQ4eOrKwsZGVloUOHDnjooYc0uSY7/mX+/Pm2/5wxY8Y1BzCenp4Ob6mzceNGjBw5UtOdYip3+Pr64ujRowgODkaLFi2wa9cudO/eHT/88AMiIiJsRz7ZoW2H0WjEmTNn6r1ZLi8vD/fee69m/1/HDiKiWtwp5ST+6GZSywGMKi2qdFy8eBFPPPEENm/eDKvVCgBwc3PDiBEjsGrVKs2OV7DD3owZMzB+/Hi88MIL8PHx0eSaKneo1tIQqvxOxVU7oqKiEBUVpek12fEvycnJAIA2bdooMYABgH79+uHs2bNo3bo1ACA7OxtvvfUWOnXqhEmTJrlchypHPNlRS5XdhewgIrJn1DuAboy6m8mysjKUlpbiwoULtg+tf8OhSosqHRMmTMDnn3+O9957D6WlpSgtLcV7772HQ4cO4b//+7/ZoVPHjz/+iKlTp+o+fFGlQ6WW5ORkFBQU/OH3Xbx40aG7gthR3+uvv47evXujZcuWtqbly5dj586dDr0uO66tbgBTJzs7G0899RTWrVunWUOd+Ph47N27F0Dtg+D79++P7OxszJ49GwsWLHC5jrojngCQmJiIOXPmoEOHDhgzZgzGjx/PDo07YmNj8fDDD0NEEBMTg4cfftj2MXLkSKxduxZvvPEGOzTuICKCHq/8oxvPx8dHiVeSi6jTolLHp59+Wm89MzNTfHx82KFTR1xcnLz99tuaXU/1DhF1WsLDw8XNzU369esnb7755lVfd88O7b322mtisVjk+eefF29vb9vP19TUVOnbty87dOro06ePpKWliYhIcXGxmEwmiYqKEovFIvPnz9esQ0QkICBAjh07JiIiK1askF69eomIyO7du6Vt27Yu1/F7Bw4ckCVLlsg777yjWwM7RP72t7/p9nOUHURE9fH4npOIiYnBoUOHdH+miUotqnQ0bdr0qkfS/P390bhxY3bo1PHAAw9g1qxZ+Pbbb9G1a1d4eHjYfX3w4MEu1aFSS25uLnJycpCamopp06bhySefxMiRIzF+/HjccccdmjSwo75XXnkF69evR2xsLF566SXbeo8ePTBz5kx26NTx9ddfo2fPngCALVu2oGvXrti/fz8yMjIwefJkTd9qVlVVZTuG9dFHH9l+ZoSGhqK4uNjlOn7PVY+aqtahyvFOdhAR/ZPeUzG6MVJSUiQ4OFiSk5Nl69atsnPnTrsPV2xRpWPt2rXSv39/KS4utq0VFxfLgAEDZM2aNezQqcNgMFzzw2g0ulyHai11KisrZdu2bfLggw+Kh4eHdO3aVZYvXy6lpaXs0LijUaNGkp+fLyIifn5+tp1Bx48fl0aNGjn8+uy4Ol9fXzl16pSIiDz00EPy0ksviYhIQUGBph0iIj179pSnn35aMjMzpVGjRpKbmysiIllZWdKqVSuX6xARSUtLk169ekmLFi1sf1+WLVsm6enp7NCpQ5XdhewgIqrFoZSTUOlmUpUWVTr+9Kc/iZ+fn3h4eEj79u2lffv24uHhIX5+fhIREWH3wQ7tOujmUFFRIZs3b5YBAwaIu7u73H333RISEiImk0k2b97MDg07wsLCbDeOVw5hVq5cqen/XtlhT6UBzN69eyUgIECMRqOMGzfOtv7MM89IXFycy3WocsSTHfZUOd7JDiKiWjy+5yRqamr0TrBRpUWVjtjYWL0TALDjSlVVVfD29kZubi66dOni8h2qtQDAl19+idTUVGzatAleXl4YM2YMVq1ahZCQEAC1R6emTp2KESNGsEOjjunTp+PJJ5/E5cuXISLIzs7Gpk2b8OKLLyIlJcVh12XH9S1atAhxcXFYvHgx/uu//gvh4eEAgHfeecd2rE8rffv2xblz5/Dbb7/ZHceeNGmS3QsU9u/fjx49ejjsjWuqdKhyxJMd9lQ53skOIqJ/0nkoRjdAZWWluLm5yVdffaV3ijItqnSQutq2bWvbUcCOWqq0dOnSRdzd3WXQoEGyY8cOqa6urvc9Z8+eFYPBwA4NO0RE3njjDQkJCbHtOm3VqpWkpKQ4/LrsuL7q6mr55Zdf7NZOnTolZ86csX3+2WefKfMwY5PJpMSLSBzdocoRT3bYU2V3ITuIiGoZ9R6K0f+dh4cHgoODYbVa9U5RpkWVjjqlpaVISUnBM888g19++QUAcPjwYfz444/s0Klj9uzZ+H//7//Zrq8XVTpUahk+fDjy8/Oxa9cuxMbGws3Nrd73WCwWh++GZEd9o0ePxokTJ1BWVoaSkhIUFRUhISHB4ddlx/W5ubnVe1FEmzZtEBQUZPv8/vvv1/xn/bWIiN4JABzf0bZtW+Tm5tZb/+CDDxAWFubQa7Pj2hYtWoS1a9eib9++GDVqlG67C9lBRPRPek/F6MZISUmRQYMGyfnz5/VOUaZFlY68vDwJDAyUkJAQcXd3t/1mcPbs2fLYY4+xQ6eOumdbeXl5yW233abb86xU6VCtpU5NTY3U1NTocm12kDO5cmeK3lRpcXTH+vXrpVWrVrJ582bx9fWVTZs2yfPPP2/7s1bYUZ8quwvZQUTEZ0o5jVdffRXff/89WrZsiVtvvRW+vr52Xz98+LDLtajSMX36dIwdOxYvv/wyTCaTbX3QoEGIj4/XpIEd9anwbCtAnQ5ArZYNGzZg2bJlOHHiBACgQ4cOeOqppzBhwgR2aNgREREBg8HQoO915M9UdtDNaMKECfD29sazzz6L8vJyxMfHo2XLllixYgVGjhzJDp06gGvvLrzS/fffj9zcXLRr144dGnUQkWviUMpJqHQzqUqLKh1ffPEF1q5dW2+9VatWKCkpYYdOHcnJyZpd63pU6QDUaZk7dy6WLl2KxMREREVFAQCysrKQlJSEwsJCLFiwgB0adajyc5QddLMaPXo0Ro8ejfLycpSVldkdqWSHfh0NIS5yzLShVOkgIufDoZSTUOVmElCnRZUOLy8v/Pbbb/XWjx8/jsDAQHbo1EHqWr16NdavX49Ro0bZ1gYPHoxu3bohMTFRs2EQOxr+c9TRNyvscC4N3W3maFp2+Pj42L35Ty/sICIi1XAoReRggwcPxoIFC7BlyxYAtf8ILiwsxNNPP42hQ4eyQ6cOq9WKZcuWYcuWLSgsLERlZaXd17V62LcqHSq1VFVVoUePHvXWb7/9dlRXV2vSwI76Fi9ejFmzZtVbt1qtePTRR7Fp0yZ26NDRUKoMggB1hnaO6FDliCc7iIjoZsGhlJNQ5WZSpRZVOpYsWYJhw4YhKCgIly5dwj333IOSkhJERUVh4cKFmjSwo7758+cjJSUFM2bMwLPPPovZs2cjPz8f6enpmDt3rst1qNTy2GOPYfXq1Vi6dKnd+rp16zB69Gh26NSxePFiNGnSxO7tclarFSNHjsTXX3/NDp06GkqLQVBycjLGjx+PW2+99brfd/HiRaftUOWIJzuIiOimodsj1umGmjNnjrRo0UL++te/SqNGjeS5556ThIQEadq0qaxYscIlW1TpqPPZZ5/JqlWrZNGiRfLhhx9qfn122GvXrp289957IlL7Bqbvv/9eRERWrFgho0aNcrkOvVuSkpJsH4mJiWIymaRz586SkJAgCQkJ0qVLFzGbzTJlyhR2aNhxpezsbAkICJC///3vIiJSVVUlcXFxEhYWJsXFxezQqWPu3LmSn5+v2fWuJzw8XNzc3KRfv37y5ptv6vamLlU6rkeVt2iy49pMJpMSb4lkBxE5Ow6lnARvbNXt2Lhx41X/QVxRUSEbN25kh04dPj4+UlBQICIizZs3ly+//FJERE6ePClms9nlOvRu6du3b4M+7r33XnZo2PF7e/bsEZPJJDt37pTBgwdLp06dpKSkRNMGdthTbQBz+PBhSUxMFIvFIgEBATJ58mTJzs52yY6XX375quvV1dUycuRIdujU0VB+fn5KDGHYQUTOjkMpJ8EbW3U7jEajnDlzpt76uXPnxGg0skOnjttuu00OHjwoIiK9e/eWF198UURENm/eLIGBgS7XoVpLQ5w+fVqsVqveGS7XsWPHDnF3d5euXbvK2bNnHX49dvwxFQYwv1dZWSnbtm2TBx98UDw8PKRr166yfPlyKS0tdZmOwMBASUlJsVurrq6WYcOGSWhoqMOvz46rU2V3ITuIiGrxmVJOonXr1iguLkZwcDDat2+PjIwMdO/eHV988QW8vLxcskWVDhG56kM+i4qK4O/vzw6dOuLi4rBnzx7ceeedSExMxKOPPooNGzagsLAQSUlJLtehWktDdOrUCbm5uWjXrh07HNQxZMiQq64HBgYiICAAkyZNsq1t3779hl2XHf+eiIgIREREYMmSJXj33XeRmpqK3r17IzQ0FAkJCRg7dqymP1+B2p/1VVVVqKyshIigcePGePXVVzFnzhysX78eI0aMcPqOXbt2YcCAAfD398ewYcNQXV2N4cOH49ixY9i7d6/DrsuO69u5cycWLlyIe+65BwkJCRg6dKjm/1ZmBxHRv3Ao5SRUuplUpUXvjro3zhgMBkRHR8Pd/V//c7NarTh16hQGDhzIDo076rz00ku2P48YMQLBwcHIyspChw4d8NBDD7lch2otDSFO/Aav/4QjOq41yIiJibnh12LH/50Kg6Avv/wSqamp2LRpE7y8vDBmzBisWrUKISEhAIBXXnkFU6dOdXiLCh133HEHtm3bhtjYWHh6emLDhg34/vvvsXfvXjRr1sxh12XH9eXm5iInJwepqamYNm0annzySYwcORLjx4/HHXfcwQ6dOojIdRlElX9N0w2VlZWlzM2kKi1ad8yfP9/2nzNmzICfn5/ta56enmjTpg2GDh0KT09PdmjYQc7DZDIhLy9P9x1KrtJx6dIl1NTUwNfXFwBsb2YMCwvTdCjDjvquNoCZMGGC3QDm+eefx5kzZxza0bVrVxw7dgwDBgzAxIkT8dBDD8HNzc3ue86dO4egoCDU1NQ4fUed9PR0PPLIIwgLC8PHH38Mi8Xi8Guyo2Gqqqpsuwt3796t2+5CdhCRK+NQisjBNm7ciJEjR+q+FZod9b3++utYs2YNTp06haysLNx6661Yvnw52rZti4cfftjlOlRr+SOuMgxSpWPAgAEYMmQIJk+ejNLSUoSGhsLDwwPnzp3D0qVL8fjjjzvkuuy4PpUGMM899xzGjx+PVq1aOfQ6Kndc64jnwYMHERISYjeA0eOoqat2XEtlZSV27NiB//mf/8HHH3+MXr164aeffsKZM2c0PWbKDiJyZUa9A+jGef3119G7d2+0bNkSBQUFAIDly5dj586dLtuiQke/fv1w9uxZ2+fZ2dl46qmnsG7dOs0a2FHf6tWrMX36dAwaNAilpaWwWq0AgICAACxfvtzlOlRrIfUcPnwYd911FwBg69ataNasGQoKCpCWloaVK1eyQ6eO4cOHIz8/H7t27UJsbGy9gRQAWCwWTXYEzZkzxzYIktqX6Tj8mqp1+Pv7X/UjJiYG7du3t1tjh3Ydv/fll19iypQpaNGiBZKSkhAREYGjR49i3759OHHiBBYuXIipU6eyQ+MOInJR2j5XnRzltddeE4vFIs8//7x4e3vbXtmampoqffv2dckWVTr69OkjaWlpIiJSXFwsJpNJoqKixGKxyPz589mhU0dYWJjs2LFDROxfc/zVV19J06ZNXa5DtZaGMJlMSrye2lU6vL29bW80feSRR2TevHkiIlJYWCje3t4Ouy47Gq6mpkZqamp0uXadlJQU6dy5s3h6eoqnp6d07txZ1q9f75Id5eXlUlZWZvv81KlTsmzZMvnggw/YoWNHly5dxN3dXQYNGiQ7duyQ6urqet9z9uxZMRgM7NCwg4hcF4dSTkKlm0lVWlTpCAgIkGPHjomIyIoVK6RXr14iIrJ7925p27YtO3TqaNSoke0VyFf+/Th+/Lg0atTI5TpUa2mIKxvZ4fiOrl27yooVK6SwsFDMZrMcOHBAREQOHTokzZo1c9h12fHHVBjAiIjMmTNHfH195S9/+Yvs3LlTdu7cKX/5y1/Ez89P5syZ43Id9913n6xevVpERC5cuCDNmjWT1q1bS6NGjeS1115jh04dCxYskKKiIs2uxw4iouvjUMpJqHQzqUqLKh2+vr5y6tQpERF56KGH5KWXXhIRkYKCAnbo2BEWFibp6ekiYv/3Y+XKlRIREeFyHaq11CksLJTCwsJrfu1qv9Flh2P8/e9/Fw8PDzEajXLffffZ1l944QUZOHCgw67LjutTZQAjImKxWOStt96qt/7WW29p+ssgVTqaNm0qX3/9tYiIrF+/Xrp16yZWq1W2bNkioaGh7NCp40oq7C5kBxG5Og6lnIRKN5OqtKjS0bNnT3n66aclMzNTGjVqJLm5uSIikpWVJa1atWKHTh3r16+XVq1ayebNm8XX11c2bdokzz//vO3PrtahUktVVZU8++yzYjabxWg0itFoFLPZLLNnz5bKykp26NQhUnvk9vDhw2K1Wm1rn3/+uRw9epQdOnWoMoAREfH395fjx4/XW//uu+/E39/f5TpUOeLJjvpU2V3IDiIiDqWchio3kyq1qNKxd+9eCQgIEKPRKOPGjbOtP/PMMxIXF8cOnTpERN544w0JCQkRg8EgBoNBWrVqJSkpKZo2qNShSsvkyZMlKChI1qxZI3l5eZKXlydr1qyR5s2by+TJk9mhUwepSZUBjIjIlClTJCkpqd76jBkz5IknnnC5DlWOeLLDniq7C9lBRFSLQyknosLNpGotqnRUV1fLL7/8Yrd26tQpOXPmjO3zzz77TC5fvswODTvq/O///q/dtfWiSoeIvi1ms1nef//9euu7du0Ss9nMDp06SE16D2CSkpJsH4mJiWIymaRz586SkJAgCQkJ0qVLFzGbzTJlyhSX6LiSKkc82WFPld2F7CAiqmUQ0el9veQw5eXlKCsrQ1BQkN4pyrSo0nE9ZrMZubm5aNeuHTsU6iDtBQUFYd++fQgLC7NbP3r0KO6++26cPXuWHTp0kDqmT59u+3N1dTX+9re/ITg4GJGRkQCAzz//HIWFhRgzZgxeeeUVh7bce++9Dfo+g8GAjz/+2Ok7fq+kpATFxcUIDw+H0WgEAGRnZ8NsNiM0NJQdOnQEBATgiy++QIcOHezWjx8/jp49e6K0tJQdOnQQkeviUIpIESaTCXl5eboPYZy5IyIiAgaDoUHfe/jw4Rt2XVU7VGups2DBAhw7dgypqanw8vICAFRUVCAhIQEdOnRAcnIyO3ToIHWoOoBpqKKiIrRs2dI2lHD1DtJWYmIiPDw8sHTpUrv1mTNn4tKlS1i1ahU7dOggItflrncA/edUuplUpUWVDlJTbGys3gkA1OkA1Gqpk5OTgz179qB169YIDw8HAOTl5aGyshLR0dEYMmSI7Xu3b9/ODo06SB179+79t/87Kg1gOnXqpMROWFU6yPGu3F1oMBiQkpKCjIyMq+4uZId2HUREAIdSNzWVbiZVaVGlg9TU0B0ljt5AqkoHoFZLnYCAAAwdOtRu7ZZbbtHs+uwgZ6TSAEaVTfqqdJDj5eTk2H1+++23AwBOnjwJALBYLLBYLPjmm2/YoWEHERHA43suQUQavHvI0VRpUaXjSs58bE7FjsWLF2PWrFn11q1WKx599FFs2rTJIddVtUO1FiK6sVT52a5SiyodpCZVdheyg4icHX+qOInFixdfdd1qtSI+Pt4lW1TpaChVhmSu0rF48WJs2LDBbs1qtWLkyJHIzc116LVV7FCtpbq6Gh999BHWrl2LixcvAgB++uknlJWVsUPHDiIiV9GpUyfk5+frncEOInJ6PL7nJBYvXowmTZogISHBtlZ3M/n111+7ZIsqHQ2lyqZFV+nYtWsXBgwYAH9/fwwbNgzV1dUYPnw4jh079h89r+Vm71CppaCgAAMHDkRhYSEqKipw3333wWQyYdGiRaioqMCaNWvYoUMHEZErcZV/DzWUKh1E5Hy4U8pJ7Nq1CzNnzsTWrVsB1P5W/ZFHHsE333yjy42tCi2qdCQnJ6OgoOAPv+/ixYsOPULADnt33HEHtm3bhvHjx+Odd97B0KFD8d1332Hv3r1o3ry5w66raodKLdOmTUOPHj1w4cIFeHt729bj4uKwZ88edujUQXSjuMqOXCIiImoAIaexZ88eMZlMsnPnThk8eLB06tRJSkpKXLpFhY7w8HBxc3OTfv36yZtvvimXL1/W9PrsuL4dO3aIu7u7dO3aVc6ePevyHSq0NGnSRI4dOyYiIn5+fnLy5EkRETl16pR4e3uzQ6cOurmZTCbb3x29Xfn3mB2kKlX+frCDiJwdj+85kX79+iEtLQ1Dhw5FWFgY9u3bB4vF4tItKnTk5uYiJycHqampmDZtGp588kmMHDkS48ePxx133MEODTuGDBly1fXAwEAEBARg0qRJtrXt27c7fYdqLXVqampgtVrrrRcVFcFkMmnSwA5yNqLx0ZvTp08DuPqbIr/99lu0bNnSpTqIiIjo6vj2vZvYtW4mDx48iJCQELvhi143tlq3qNJxLVVVVXj33XeRmpqK3bt3IzQ0FAkJCRg7diz8/f3Z4eCOcePGNfh7U1NTb/j1VesA1GqpM2LECPj7+2PdunUwmUw4cuQIAgMD8fDDDyM4OJgdOnWQ+q43gDl9+jRatmwJNzc3h12/uroa8+fPx8qVK20P4ffz80NiYiKSk5Ph4eHhsGur2EE3N7PZjNzcXN3fzsgOInJ23Cl1E7vWTXtMTIzGJeq0qNJxLSKCqqoqVFZWQkTQuHFjvPrqq5gzZw7Wr1+PESNGsMOBHVfevF+6dAk1NTXw9fUFAOTn5yM9PR1hYWEO//uiSodqLXWWLFmCmJgYdOrUCZcvX0Z8fDxOnDgBi8WCTZs2sUOnDlJTQwcwVxtU3WiJiYnYvn07Xn75ZURFRQEAsrKyMG/ePJw/fx6rV692eINKHXRzU+X39uwgIqenz6lButHKy8ulrKzM9vmpU6dk2bJl8sEHH7hsiyodIiKHDh2SJ598Upo0aSItWrSQp59+Wk6cOGH7+sqVKyUoKIgdGnbcd999snr1ahERuXDhgjRr1kxat24tjRo1ktdee82h11axQ7WWqqoqeeONN2TWrFny+OOPy/r166W8vFzTBnbQzWDy5MkSFBQka9askby8PMnLy5M1a9ZI8+bNZfLkyZq2mM1mef/99+ut79q1S8xms8t1kPoKCwulsLDwml+rrq5mhw4dRORaOJRyEirdTKrSokpHly5dxN3dXQYNGiQ7duy46v+hnz17VgwGAzs07GjatKl8/fXXIiKyfv166datm1itVtmyZYuEhoY69NoqdqjUsm/fPqmqqqq3XlVVJfv27WOHTh2kJpUGMIGBgfLtt9/WW//222/FYrG4XAepqaqqSp599lkxm81iNBrFaDSK2WyW2bNnS2VlJTt06iAi18WhlJNQ5WZSpRZVOhYsWCBFRUWaXY8dDePt7S0FBQUiIvLII4/IvHnzRKT2N4FavtFMlQ6VWoxGo5w5c6be+rlz58RoNLJDpw5Sk0oDmPnz58uoUaPs3qp6+fJlGT16tO3niSt1kJpU2V3IDiKiWnymlJMoLy+3vYUpIyMDQ4YMgdFoRGRkJAoKClyyRZWOOXPm2P4s/zyPbzAYNLs+O64uJCQE6enpiIuLw+7du5GUlAQA+Pnnn2E2m12uQ6UWEbnq34nz58/bnnfFDu07SE1TpkzBc889h9TUVHh5eQEAKioqsHDhQkyZMkXTlpycHOzZswetW7dGeHg4ACAvLw+VlZWIjo62exmJI182okoHqemtt97C5s2bcf/999vWunXrhltuuQWjRo3S7Jlj7CAiqsWhlJNQ5WZSpRZVOgBgw4YNWLZsGU6cOAEA6NChA5566ilMmDCBHTp1zJ07F/Hx8UhKSkJ0dLTtYbgZGRmIiIhwuQ4VWupuFA0GA8aOHWu7wQYAq9WKI0eOoFevXuzQuIPUptIAJiAgAEOHDrVb0+IB66p2kJq8vLzQpk2beutt27aFp6cnO3TqICLXxaGUk9D7ZlLFFpU6li5disTERLu3ACUlJaGwsBALFixghw4dw4YNQ58+fVBcXGy7kQOA6OhoxMXFadKgUocKLXVvzxQRmEwmeHt7277m6emJyMhITJw4kR0ad5DaVBrAXPk2Tz2p0kFqUmV3ITuIiGoZRPh+T2dRUlJiu5k0Go0AgOzsbJjNZoSGhrpkiwodgYGBWLlyJUaNGmW3vmnTJiQmJuLcuXPs0KGD1PXnP/8Z8+bNg4+PDwAgPz8f6enpCAsLQ0xMDDt06iBqiOrqanzyySc4efIk4uPjYTKZ8NNPP8FsNsPPz8/lOkg9cXFx2LNnD7y8vK66u/BKjtxdyA4iolrcKeVEmjdvjubNm9ut9ezZ06VbVOioqqpCjx496q3ffvvtqK6uZodOHaSunJwcpKWlYfLkySgtLUVkZCQ8PDxw7tw5LF26FI8//jg7dOggdakygCkoKMDAgQNRWFiIiooK3HfffTCZTFi0aBEqKiqwZs0al+ogNamyu5AdRES1uFOKyMESExPh4eGBpUuX2q3PnDkTly5dwqpVq9ihQwepy2KxYN++fejcuTNSUlLwyiuvICcnB9u2bcPcuXNx9OhRdujQQWr6/QDm+PHjaNeuHaZNm6b5ACY2NhYmkwkbNmxA06ZNkZeXh3bt2uGTTz7BxIkTbc8RdJUOIiIi+mPcKUXkANOnT7f92WAwICUlBRkZGYiMjAQAfP755ygsLMSYMWPYoWEH3RxUeXMmO+hmMG3aNPTo0QN5eXlo2rSpbT0uLk7zZ459+umnOHDgQL2HI7dp0wY//vijy3WQulTZXcgOIiIOpYgcIicnx+7z22+/HQBw8uRJALU7HywWC7755ht2aNhBNwdV3pzJDroZqDSAqampgdVqrbdeVFRkG6y6UgepSZXjnewgIqrFoRSRA+zdu/ff/u8UFRWhZcuWtgeys+PGd9DNQaU3Z7KDVKfSAGbAgAFYvnw51q1bB6B2Z2xZWRmSk5MxaNAgl+sgNamyu5AdRES1+EwpIkWYzWbk5uaiXbt27FCog/Shwpsz2UE3gxEjRsDf3x/r1q2DyWTCkSNHEBgYiIcffhjBwcFITU3VrKWoqAgxMTEQEZw4cQI9evTAiRMnYLFYkJmZiaCgIJfqIDU1bdoUBw4cQMeOHWEymWzPHMvPz0enTp1QXl7ODh06iMh1cacUkSJUmQ+zg1Sgwpsz2UE3gyVLliAmJgadOnXC5cuXER8fbxvAbNq0SdOW1q1bIy8vD2+//Tby8vJQVlaGhIQEjB49Gt7e3i7XQWpSZXchO4iIanGnFJEirvztFDvU6SAiUl11dbXdAKZ79+66DGAyMzPRq1cvuLvb/86zuroaBw4cwN133+1SHaQmVXYXsoOIqBaHUkSKUGUIww4iopuHSgMYNzc3FBcX1zsed/78eQQFBV11N4Yzd5CaVDneyQ4iolo8vkdERER0k7r33nuvOoD59ddfce+992o6gBERGAyGeuvnz5+Hr6+vy3WQmlQ53skOIqJaHEoRKeJq/4DWAzuIiG4eKgxghgwZAqD25/bYsWPh5eVl+5rVasWRI0fQq1cvl+kgtdXtLhw9ejRGjx5tW6+urkZmZqbmx0zZQUSujkMpIkWocpKWHURE6lNpAOPv7w+g9ue2yWSy213h6emJyMhITV4tr0oHqU2V3YXsICKqxaEUkYZOnz4NALjlllvqfe3bb79Fy5Yt2aFDBxHRzUalAUzdg5ADAwMxb948+Pj4AADy8/ORnp6OsLAwWCwWl+kgtamwu5AdRET/wqEUkYNVV1dj/vz5WLlyJcrKygAAfn5+SExMRHJyMjw8PABcfTDDDiIiuhoVBzA5OTlIS0vD5MmTUVpaisjISHh4eODcuXNYunQpHn/8cZfqILWosruQHURE9jiUInKwxMREbN++HS+//DKioqIAAFlZWZg3bx7Onz+P1atXs0OHDiIiZ6DSACYnJwfLly8HAGzduhXNmjVDTk4Otm3bhrlz52o6lFKhg9Siyu5CdhAR2TMIH9xC5FD+/v7YvHkz7r//frv1999/H6NGjcKvv/7KDh06iIicgcViwb59+9C5c2ekpKTglVdesRvAHD16VLMWHx8fHDt2DMHBwRg+fDg6d+6M5ORknD59Gh07dkR5eblLdZCa/vznP19zd2FMTAw7dOogItdl1DuAyNl5eXmhTZs29dbbtm0LT09PdujUQUTkDMrLy2EymQAAGRkZGDJkCIxGIyIjI1FQUKBpS0hICNLT03H69Gns3r0bAwYMAAD8/PPPMJvNLtdBaqrbXQjAtrtwyZIliI2N1XS3NjuIiGpxKEXkYFOmTMFzzz2HiooK21pFRQUWLlyIKVOmsEOnDiIiZ6DSAGbu3LmYOXMm2rRpgzvvvNN2RDsjIwMREREu10FqysnJwV133QXgX8c7CwoKkJaWhpUrV7JDpw4icl08vkfkYHFxcdizZw+8vLwQHh4OAMjLy0NlZSWio6Ptvnf79u3s0KiDiMgZbN26FfHx8bBarYiOjkZGRgYA4MUXX0RmZib+8Y9/aNpTUlKC4uJihIeHw2is/d1ndnY2zGYzQkNDXa6D1KPK8U52EBHV4oPOiRwsICAAQ4cOtVvT481y7CAicj7Dhg1Dnz59bAOYOtHR0YiLi9O8p3nz5mjevLndWs+ePV22g9RTt7swLi4Ou3fvRlJSEgD9jpmyg4hcHXdKERERERGRS1BldyE7iIhqcShFpIHq6mp88sknOHnyJOLj42EymfDTTz/BbDbDz8+PHTp1EBERketR5XgnO4iIOJQicriCggIMHDgQhYWFqKiowPHjx9GuXTtMmzYNFRUVWLNmDTt06CAiIiIiIiJ98e17RA42bdo09OjRAxcuXIC3t7dtve6B3+zQp4OIiIiIiIj0xQedEznYp59+igMHDsDT09NuvU2bNvjxxx/ZoVMHERERERER6Ys7pYgcrKamBlartd56UVERTCYTO3TqICIiIiIiIn1xKEXkYAMGDMDy5cttnxsMBpSVlSE5ORmDBg1ih04dREREREREpC8+6JzIwYqKihATEwMRwYkTJ9CjRw+cOHECFosFmZmZCAoKYocOHURERERERKQvDqWINFBdXY23334beXl5KCsrQ/fu3TF69Gi7B32zQ/sOIiIiIiIi0g+HUkQOlpmZiV69esHd3f69AtXV1Thw4ADuvvtudujQQURERERERPriUIrIwdzc3FBcXFzvWNr58+cRFBR01Yd+s4OIiIiIiIicHR90TuRgIgKDwVBv/fz58/D19WWHTh1ERERERESkL/c//hYi+k8MGTIEQO3b5caOHQsvLy/b16xWK44cOYJevXqxQ+MOIiIiIiIiUgOHUkQO4u/vD6B2Z5DJZLJ7iLenpyciIyMxceJEdmjcQURERERERGrgUIrIQVJTUwEAgYGBmDdvHnx8fAAA+fn5SE9PR1hYGCwWCzs07iAiIiIiIiI18JlSRA6Wk5ODtLQ0AEBpaSkiIyOxZMkSxMbGYvXq1ezQqYOIiIiIiIj0xaEUkYPl5OTgrrvuAgBs3boVzZo1Q0FBAdLS0rBy5Up26NRBRERERERE+uJQisjBysvLYTKZAAAZGRkYMmQIjEYjIiMjUVBQwA6dOoiIiIiIiEhfHEoROVhISAjS09Nx+vRp7N69GwMGDAAA/PzzzzCbzezQqYOIiIiIiIj0xaEUkYPNnTsXM2fORJs2bXDnnXciKioKQO0uoYiICHbo1EFERERERET6MoiI6B1B5OxKSkpQXFyM8PBwGI21s+Ds7GyYzWaEhoayQ6cOIiIiIiIi0g+HUkREREREREREpDke3yMiIiIiIiIiIs1xKEVERERERERERJrjUIqIiIiIiIiIiDTHoRQREREREREREWmOQykiIiIiIiIiItIch1JERERERERERKQ5DqWIiIiIiIiIiEhzHEoREREREREREZHm/j/Kf5B8G5ScjwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(all_results_f1.keys(), all_results_f1.values(), color='skyblue')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model F1 Scores')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "5sfGpr3OceCs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "c4ccb5c1-eab0-4a7b-a4e4-e40610b0d92d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC7+klEQVR4nOzdeZyN9f//8ecZsw9jX8e+FLJNlCi7SCLVh7JkCaVspYQKUV9bEWVtsZYoaaWULZTqk7Vk38maxs6YmffvDz/n43SGTM1c19s5j/vtNrea65yZ85jlus7lNdd1HY8xxggAAAAAAABwUIjbAQAAAAAAAAg+DKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAACAYzwej1588UW3M/61GTNmqHTp0goLC1O2bNnczvGza9cueTweTZ06Nc0fu3TpUnk8Hi1dujTdu5z219+3qVOnyuPxaNeuXa41AQCA/2EoBQCAg7Zv367HHntMxYsXV2RkpGJjY3X77bdrzJgxOnv2rNt5uAabNm1S+/btVaJECb311lt68803r3jfF198UR6PRyEhIdq7d6/f7SdOnFBUVJQ8Ho+6deuWkdnp7tKA59JbaGio4uLi1L59e+3fv9/tvH9txYoVatSokeLi4hQZGanChQurSZMmmjlzpttpAAAEjFC3AwAACBbz5s1T8+bNFRERobZt26pcuXJKTEzUihUr1Lt3b23YsOGqA45AcPbsWYWGXt+7H0uXLlVKSorGjBmjkiVLXtPHRERE6P3339ezzz7rs3zu3LkZkeiowYMHq1ixYjp37px++OEHTZ06VStWrNCvv/6qyMhIt/P+kQ8//FAPPvigKlWqpJ49eyp79uzauXOnli1bprfeekutWrVyOxEAgIBwfe8VAgBwndi5c6ceeughFSlSRIsXL1b+/Pm9t3Xt2lXbtm3TvHnzXCzMOCkpKUpMTFRkZOR1O6S43OHDhyUpTaft3X333akOpWbOnKnGjRvro48+Ss9ERzVq1EhVqlSRJHXq1Em5cuXS8OHD9dlnn6lFixYu1/0zL774osqWLasffvhB4eHhPrdd+vk7wRijc+fOKSoqyrHHBADASZy+BwCAA0aMGKFTp07pnXfe8RlIXVKyZEn17NnT+35SUpJeeukllShRQhERESpatKiee+45nT9/3ufjihYtqnvuuUdLly5VlSpVFBUVpfLly3uvBzR37lyVL19ekZGRqly5stasWePz8e3bt1fmzJm1Y8cONWzYUDExMSpQoIAGDx4sY4zPfV999VVVr15dOXPmVFRUlCpXrqw5c+b4fS2XTkV77733dNNNNykiIkJfffWV97bLr/Fz8uRJPfnkkypatKgiIiKUJ08e3XnnnVq9erXP5/zwww9VuXJlRUVFKVeuXGrTpo3fKWKXvpb9+/erWbNmypw5s3Lnzq1nnnlGycnJV/jJ+Bo/fry3uUCBAuratasSEhJ8vt8DBw6UJOXOnfuar5HVqlUrrV27Vps2bfIuO3jwoBYvXnzFo24OHz6sjh07Km/evIqMjFTFihU1bdo0v/slJCSoffv2ypo1q7Jly6Z27dr5NF9u06ZN+s9//qMcOXIoMjJSVapU0Wefffa3/WlRo0YNSRdPVf0nj52QkKCnnnrK+ztRsGBBtW3bVkePHpUkJSYmasCAAapcubKyZs2qmJgY1ahRQ0uWLEm3r2H79u265ZZb/AZSkpQnTx6f9y8dNXdpPcudO7fuuusu/fzzz977pHV9XrBggXd9njRpkvf78uSTT6pQoUKKiIhQyZIlNXz4cKWkpPh8jlmzZqly5crKkiWLYmNjVb58eY0ZMya9vjUAAKQrhlIAADjg888/V/HixVW9evVrun+nTp00YMAA3XzzzXrttddUq1YtDR06VA899JDffbdt26ZWrVqpSZMmGjp0qP788081adJE7733np566im1adNGgwYN0vbt29WiRQu/f8QmJyfrrrvuUt68eTVixAhVrlxZAwcO9A5fLhkzZozi4+M1ePBgDRkyRKGhoWrevHmqR3gtXrxYTz31lB588EGNGTNGRYsWTfXr7NKliyZMmKAHHnhA48eP1zPPPKOoqCht3LjRe5+pU6eqRYsWypQpk4YOHarOnTtr7ty5uuOOO/yGL8nJyWrYsKFy5sypV199VbVq1dLIkSOv6bTIF198UV27dlWBAgU0cuRIPfDAA5o0aZIaNGigCxcuSJJGjx6t++67T5I0YcIEzZgxQ/fff//ffu6aNWuqYMGCPtcjmj17tjJnzqzGjRv73f/s2bOqXbu2ZsyYodatW+uVV15R1qxZ1b59e58BgzFG9957r2bMmKE2bdro5Zdf1r59+9SuXTu/z7lhwwbddttt2rhxo/r27auRI0cqJiZGzZo108cff/y3X8O1unQR8ezZs6f5sU+dOqUaNWrojTfeUIMGDTRmzBh16dJFmzZt0r59+yRdvA7X22+/rdq1a2v48OF68cUXdeTIETVs2FBr165Nl6+hSJEiWrRokfcxr6Zjx47eYdHw4cPVt29fRUZG6ocffvDeJy3r8+bNm9WyZUvdeeedGjNmjCpVqqQzZ86oVq1aevfdd9W2bVu9/vrruv3229WvXz/16tXL+7HffPONWrZsqezZs2v48OEaNmyYateure+++y5dvi8AAKQ7AwAAMtTx48eNJHPvvfde0/3Xrl1rJJlOnTr5LH/mmWeMJLN48WLvsiJFihhJ5vvvv/cuW7BggZFkoqKizO7du73LJ02aZCSZJUuWeJe1a9fOSDLdu3f3LktJSTGNGzc24eHh5siRI97lZ86c8elJTEw05cqVM3Xr1vVZLsmEhISYDRs2+H1tkszAgQO972fNmtV07dr1it+LxMREkydPHlOuXDlz9uxZ7/IvvvjCSDIDBgzw+1oGDx7s8zni4+NN5cqVr/gYxhhz+PBhEx4ebho0aGCSk5O9y8eOHWskmcmTJ3uXDRw40Ejy+d5cyeX3feaZZ0zJkiW9t91yyy2mQ4cOxpiL35fLvw+jR482ksy7777r872oVq2ayZw5szlx4oQxxphPPvnESDIjRozw3i8pKcnUqFHDSDJTpkzxLq9Xr54pX768OXfunHdZSkqKqV69uilVqpR32ZIlS/x+T1IzZcoUI8ksXLjQHDlyxOzdu9fMmTPH5M6d20RERJi9e/em+bEHDBhgJJm5c+f6PV5KSor36zt//rzPbX/++afJmzeveeSRR3yW//X37VLzzp07r/q1vfPOO0aSCQ8PN3Xq1DH9+/c3y5cv9/ndMMaYxYsXG0mmR48eV+z9J+vzV1995XPfl156ycTExJgtW7b4LO/bt6/JlCmT2bNnjzHGmJ49e5rY2FiTlJR01a8PAABbcKQUAAAZ7MSJE5KkLFmyXNP958+fL0k+R0BI0tNPPy1JfkcmlS1bVtWqVfO+X7VqVUlS3bp1VbhwYb/lO3bs8HvMy1/57dLpd4mJiVq4cKF3+eXXtfnzzz91/Phx1ahRw+9UO0mqVauWypYt+zdf6cXrMv3444/6/fffU739559/1uHDh/XEE0/4XI+qcePGKl26dKpHaXXp0sXn/Ro1aqT6NV9u4cKFSkxM1JNPPqmQkP/tHnXu3FmxsbHpcr2vVq1aadu2bfrvf//r/e+VTt2bP3++8uXLp5YtW3qXhYWFqUePHjp16pS+/fZb7/1CQ0P1+OOPe++XKVMmde/e3efzHTt2TIsXL1aLFi108uRJHT16VEePHtUff/yhhg0bauvWrf/4FfPq16+v3Llzq1ChQvrPf/6jmJgYffbZZypYsGCaH/ujjz5SxYoVvUejXc7j8Xi/vkun1aWkpOjYsWNKSkpSlSpVUv1d/CceeeQRffXVV6pdu7ZWrFihl156STVq1FCpUqX0/fffe+/30UcfyePx+B1VeHlvWtfnYsWKqWHDhj7LPvzwQ9WoUUPZs2f3fv+OHj2q+vXrKzk5WcuWLZN0cX06ffq0vvnmm3/5HQAAwBlc6BwAgAwWGxsr6eL1k67F7t27FRIS4vfKbvny5VO2bNm0e/dun+WXD54kKWvWrJKkQoUKpbr8zz//9FkeEhKi4sWL+yy74YYbJP3vVCxJ+uKLL/Tyyy9r7dq1PtfCufSP78sVK1bsil/f5UaMGKF27dqpUKFCqly5su6++261bdvW23Ppa73xxhv9PrZ06dJasWKFz7JL1/S5XPbs2f2+5r+60uOEh4erePHift/zfyI+Pl6lS5fWzJkzlS1bNuXLl09169a9Yk+pUqV8BmSSVKZMGZ/e3bt3K3/+/MqcObPP/f76dWzbtk3GGPXv31/9+/dP9TEPHz6suLi4NH9d48aN0w033KDjx49r8uTJWrZsmSIiIv7RY2/fvl0PPPDA3z7mtGnTNHLkSG3atMl7aqV07b9316Jhw4Zq2LChzpw5o1WrVmn27NmaOHGi7rnnHm3atEl58uTR9u3bVaBAAeXIkeOKnyet63NqX8PWrVu1fv16v9/tSy5dfP2JJ57QBx98oEaNGikuLk4NGjRQixYtdNddd6X1ywcAwBEMpQAAyGCxsbEqUKCAfv311zR9XGrDntRkypQpTcvNXy5gfi2WL1+upk2bqmbNmho/frzy58+vsLAwTZkyxec6SZdc66uFtWjRQjVq1NDHH3+sr7/+Wq+88oqGDx+uuXPnqlGjRmnuvNLXbItWrVppwoQJypIlix588EG/oVNGuXQdsWeeecbvKJxL/jo0uVa33nqr99X3mjVrpjvuuEOtWrXS5s2blTlz5nR/7HfffVft27dXs2bN1Lt3b+XJk8d7vbG/Xlw9PURHR6tGjRqqUaOGcuXKpUGDBunLL79M9bpdV3Ot63Nq605KSoruvPNOv1dvvOTSEDlPnjxau3atFixYoC+//FJffvmlpkyZorZt26Z6kXwAANzGUAoAAAfcc889evPNN7Vy5UqfU+1SU6RIEaWkpGjr1q3eI2Mk6dChQ0pISFCRIkXStS0lJUU7duzw/sNWkrZs2SJJ3guUf/TRR4qMjNSCBQt8joKZMmXKv378/Pnz64knntATTzyhw4cP6+abb9b//d//qVGjRt6vdfPmzX5HFW3evDndvheXP87lR40lJiZq586dql+/fro8TqtWrTRgwAAdOHBAM2bMuGrP+vXrlZKS4jO4uvTqfZd6L12Q+9SpUz5HS23evNnn8136msLCwtLta0nNpeFQnTp1NHbsWPXt2zdNj12iRIm/Hd7OmTNHxYsX19y5c30GPamdQpfeLg3fDhw4IOli74IFC3Ts2LErHi2VHutziRIldOrUqWv62YWHh6tJkyZq0qSJUlJS9MQTT2jSpEnq37//Px48AgCQUbimFAAADnj22WcVExOjTp066dChQ363b9++3fuqanfffbeki6/0drlRo0ZJUqqv1vZvjR071vv/xhiNHTtWYWFhqlevnqSLwwaPx6Pk5GTv/Xbt2qVPPvnkHz9mcnKyjh8/7rMsT548KlCggPf0wCpVqihPnjyaOHGizymDX375pTZu3Jhu34v69esrPDxcr7/+us+RZO+8846OHz+ebo9TokQJjR49WkOHDtWtt956xfvdfffdOnjwoGbPnu1dlpSUpDfeeEOZM2dWrVq1vPdLSkrShAkTvPdLTk7WG2+84fP58uTJo9q1a2vSpEnegcrljhw58m+/NK/atWvr1ltv1ejRo3Xu3Lk0PfYDDzygdevWpfpqgJd+LpeOhrv85/Tjjz9q5cqV6fY1LFq0KNXll64Pden0yAceeEDGGA0aNOiKvemxPrdo0UIrV67UggUL/G5LSEhQUlKSJOmPP/7wuS0kJEQVKlSQJJ/1BwAAW3CkFAAADihRooRmzpypBx98UGXKlFHbtm1Vrlw5JSYm6vvvv9eHH36o9u3bS5IqVqyodu3a6c0331RCQoJq1aqln376SdOmTVOzZs1Up06ddG2LjIzUV199pXbt2qlq1ar68ssvNW/ePD333HPea9g0btxYo0aN0l133aVWrVrp8OHDGjdunEqWLKn169f/o8c9efKkChYsqP/85z+qWLGiMmfOrIULF+q///2vRo4cKeni0TXDhw9Xhw4dVKtWLbVs2VKHDh3SmDFjVLRoUT311FPp8j3InTu3+vXrp0GDBumuu+5S06ZNtXnzZo0fP1633HKL2rRpky6PI0k9e/b82/s8+uijmjRpktq3b69Vq1apaNGimjNnjr777juNHj3ae9H8Jk2a6Pbbb1ffvn21a9culS1bVnPnzvUb9kkXr/10xx13qHz58urcubOKFy+uQ4cOaeXKldq3b5/WrVuXbl9j79691bx5c02dOlVdunS55sfu3bu35syZo+bNm+uRRx5R5cqVdezYMX322WeaOHGiKlasqHvuuUdz587Vfffdp8aNG2vnzp2aOHGiypYtq1OnTqVL/7333qtixYqpSZMmKlGihE6fPq2FCxfq888/1y233KImTZpIkurUqaOHH35Yr7/+urZu3aq77rpLKSkpWr58uerUqaNu3bqly/rcu3dvffbZZ7rnnnvUvn17Va5cWadPn9Yvv/yiOXPmaNeuXcqVK5c6deqkY8eOqW7duipYsKB2796tN954Q5UqVfI5SgsAAGu49rp/AAAEoS1btpjOnTubokWLmvDwcJMlSxZz++23mzfeeMOcO3fOe78LFy6YQYMGmWLFipmwsDBTqFAh069fP5/7GHPxJeQbN27s9ziSTNeuXX2W7dy500gyr7zyindZu3btTExMjNm+fbtp0KCBiY6ONnnz5jUDBw40ycnJPh//zjvvmFKlSpmIiAhTunRpM2XKFDNw4EDz192J1B778tsGDhxojDHm/Pnzpnfv3qZixYomS5YsJiYmxlSsWNGMHz/e7+Nmz55t4uPjTUREhMmRI4dp3bq12bdvn899Ln0tf5Va45WMHTvWlC5d2oSFhZm8efOaxx9/3Pz555+pfr4jR4787ee71vum9j07dOiQ6dChg8mVK5cJDw835cuXN1OmTPH72D/++MM8/PDDJjY21mTNmtU8/PDDZs2aNUaS3/23b99u2rZta/Lly2fCwsJMXFycueeee8ycOXO891myZImRZJYsWXLV5ilTphhJ5r///a/fbcnJyaZEiRKmRIkSJikp6Zof+9LX061bNxMXF2fCw8NNwYIFTbt27czRo0eNMcakpKSYIUOGmCJFipiIiAgTHx9vvvjiC9OuXTtTpEgRv+/rpd+3y5t37tx51a/t/fffNw899JApUaKEiYqKMpGRkaZs2bLm+eefNydOnPC5b1JSknnllVdM6dKlTXh4uMmdO7dp1KiRWbVqlfc+/3Z9NsaYkydPmn79+pmSJUua8PBwkytXLlO9enXz6quvmsTERGOMMXPmzDENGjQwefLkMeHh4aZw4cLmscceMwcOHLjq1wsAgFs8xvyDq50CAICA0L59e82ZMyfdjjABAAAArhXXlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiuKQUAAAAAAADHcaQUAAAAAAAAHMdQCgAAAAAAAI4LdTvAaSkpKfr999+VJUsWeTwet3MAAAAAAAACijFGJ0+eVIECBRQScuXjoYJuKPX777+rUKFCbmcAAAAAAAAEtL1796pgwYJXvD3ohlJZsmSRdPEbExsb63INAAAAAABAYDlx4oQKFSrkncFcSdANpS6dshcbG8tQCgAAAAAAIIP83WWTuNA5AAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOM7VodSyZcvUpEkTFShQQB6PR5988snffszSpUt18803KyIiQiVLltTUqVMzvBMAAAAAAADpy9Wh1OnTp1WxYkWNGzfumu6/c+dONW7cWHXq1NHatWv15JNPqlOnTlqwYEEGlwIAAAAAACA9hbr54I0aNVKjRo2u+f4TJ05UsWLFNHLkSElSmTJltGLFCr322mtq2LBhRmUCAAAAAAAgnV1X15RauXKl6tev77OsYcOGWrly5RU/5vz58zpx4oTPGwAAAAAAANzl6pFSaXXw4EHlzZvXZ1nevHl14sQJnT17VlFRUX4fM3ToUA0aNMipRAC4rgxbc9Sxx+obn8uxxwoE/GyAa+fk+iKxzqQF2zI78XMB0oZ1JuNcV0Opf6Jfv37q1auX9/0TJ06oUKFCLhYBAIDrFTulAAAA6ee6Gkrly5dPhw4d8ll26NAhxcbGpnqUlCRFREQoIiLCiTwAAAAAAABco+vqmlLVqlXTokWLfJZ98803qlatmktFAAAAAAAA+CdcPVLq1KlT2rZtm/f9nTt3au3atcqRI4cKFy6sfv36af/+/Zo+fbokqUuXLho7dqyeffZZPfLII1q8eLE++OADzZs3z60vAZbhmhIAkL44XQ0AgODD8z+c4uqRUj///LPi4+MVHx8vSerVq5fi4+M1YMAASdKBAwe0Z88e7/2LFSumefPm6ZtvvlHFihU1cuRIvf3222rYsKEr/QAAAAAAAPhnXD1Sqnbt2jLGXPH2qVOnpvoxa9asycAqAAAAAAAAZLTr6kLnAK5fHAIMAACAy3HpDQDX1YXOAQAAAAAAEBg4UgoAAAAAAJdx5BiCEUMpAAAAAABgHS4BEvgYSgEAAADpgKMcAABIG64pBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOC4ULcDAAAAAABwy7A1Rx19vL7xuRx9PMBmDKUAAACuM07+A4p/PAEAgIzCUAoAAFiPIQwAAEDg4ZpSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjuNC5wAAAECA4cUBAADXA46UAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOK4pdZ3jegEAAAAAAOB6xJFSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjQt0OQGAYtuaoo4/XNz6Xo48HAAAAAADSF0MpIIM4OahjSAcAAAAAuN5w+h4AAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMdxTSkgwHFtKwAAAACAjThSCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzn+lBq3LhxKlq0qCIjI1W1alX99NNPV73/6NGjdeONNyoqKkqFChXSU089pXPnzjlUCwAAAAAAgPQQ6uaDz549W7169dLEiRNVtWpVjR49Wg0bNtTmzZuVJ08ev/vPnDlTffv21eTJk1W9enVt2bJF7du3l8fj0ahRo1z4CgAAAADYbtiao449Vt/4XI49FgBc71w9UmrUqFHq3LmzOnTooLJly2rixImKjo7W5MmTU73/999/r9tvv12tWrVS0aJF1aBBA7Vs2fJvj64CAAAAAACAXVwbSiUmJmrVqlWqX7/+/2JCQlS/fn2tXLky1Y+pXr26Vq1a5R1C7dixQ/Pnz9fdd9/tSDMAAAAAAADSh2un7x09elTJycnKmzevz/K8efNq06ZNqX5Mq1atdPToUd1xxx0yxigpKUldunTRc889d8XHOX/+vM6fP+99/8SJE+nzBQAAAAAAAOAfc/1C52mxdOlSDRkyROPHj9fq1as1d+5czZs3Ty+99NIVP2bo0KHKmjWr961QoUIOFgMAAAAAACA1rh0plStXLmXKlEmHDh3yWX7o0CHly5cv1Y/p37+/Hn74YXXq1EmSVL58eZ0+fVqPPvqonn/+eYWE+M/Y+vXrp169ennfP3HiBIMpAAAAAAAAl7l2pFR4eLgqV66sRYsWeZelpKRo0aJFqlatWqofc+bMGb/BU6ZMmSRJxphUPyYiIkKxsbE+bwAAAAAAAHCXa0dKSVKvXr3Url07ValSRbfeeqtGjx6t06dPq0OHDpKktm3bKi4uTkOHDpUkNWnSRKNGjVJ8fLyqVq2qbdu2qX///mrSpIl3OAUAAAAAAAD7uTqUevDBB3XkyBENGDBABw8eVKVKlfTVV195L36+Z88enyOjXnjhBXk8Hr3wwgvav3+/cufOrSZNmuj//u//3PoSAAAAAAAA8A+4OpSSpG7duqlbt26p3rZ06VKf90NDQzVw4EANHDjQgTIAAAAAAABklOvq1fcAAAAAAAAQGBhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4LhQtwMAAABwfRq25qhjj9U3PpdjjwUAAJzBkVIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4LtTtAAAAAAAIBsPWHHXssfrG53LssQDgn+JIKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4Ls1DqaJFi2rw4MHas2dPRvQAAAAAAAAgCKR5KPXkk09q7ty5Kl68uO68807NmjVL58+fz4g2AAAAAAAABKh/NJRau3atfvrpJ5UpU0bdu3dX/vz51a1bN61evTrNAePGjVPRokUVGRmpqlWr6qeffrrq/RMSEtS1a1flz59fERERuuGGGzR//vw0Py4AAAAAAADc84+vKXXzzTfr9ddf1++//66BAwfq7bff1i233KJKlSpp8uTJMsb87eeYPXu2evXqpYEDB2r16tWqWLGiGjZsqMOHD6d6/8TERN15553atWuX5syZo82bN+utt95SXFzcP/0yAAAAAAAA4ILQf/qBFy5c0Mcff6wpU6bom2++0W233aaOHTtq3759eu6557Rw4ULNnDnzqp9j1KhR6ty5szp06CBJmjhxoubNm6fJkyerb9++fvefPHmyjh07pu+//15hYWGSLl7jCgAAAAAAANeXNA+lVq9erSlTpuj9999XSEiI2rZtq9dee02lS5f23ue+++7TLbfcctXPk5iYqFWrVqlfv37eZSEhIapfv75WrlyZ6sd89tlnqlatmrp27apPP/1UuXPnVqtWrdSnTx9lypQp1Y85f/68zzWvTpw4kZYvFwAAAAAAABkgzUOpW265RXfeeacmTJigZs2aeY9YulyxYsX00EMPXfXzHD16VMnJycqbN6/P8rx582rTpk2pfsyOHTu0ePFitW7dWvPnz9e2bdv0xBNP6MKFCxo4cGCqHzN06FANGjToGr86AAAAAAAAOCHNQ6kdO3aoSJEiV71PTEyMpkyZ8o+jriQlJUV58uTRm2++qUyZMqly5crav3+/XnnllSsOpfr166devXp53z9x4oQKFSqU7m0AAAAAAAC4dmkeSh0+fFgHDx5U1apVfZb/+OOPypQpk6pUqXJNnydXrlzKlCmTDh065LP80KFDypcvX6ofkz9/foWFhfmcqlemTBkdPHhQiYmJCg8P9/uYiIgIRUREXFMTAAAAAAAAnJHmV9/r2rWr9u7d67d8//796tq16zV/nvDwcFWuXFmLFi3yLktJSdGiRYtUrVq1VD/m9ttv17Zt25SSkuJdtmXLFuXPnz/VgRQAAAAAAADslOah1G+//aabb77Zb3l8fLx+++23NH2uXr166a233tK0adO0ceNGPf744zp9+rT31fjatm3rcyH0xx9/XMeOHVPPnj21ZcsWzZs3T0OGDEnTMAwAAAAAAADuS/PpexERETp06JCKFy/us/zAgQMKDU3bp3vwwQd15MgRDRgwQAcPHlSlSpX01VdfeS9+vmfPHoWE/G9uVqhQIS1YsEBPPfWUKlSooLi4OPXs2VN9+vRJ65cBAAAAAAAAF6V5KNWgQQP169dPn376qbJmzSpJSkhI0HPPPac777wzzQHdunVTt27dUr1t6dKlfsuqVaumH374Ic2PAwAAAAAAAHukeSj16quvqmbNmipSpIji4+MlSWvXrlXevHk1Y8aMdA8EAAAAAABA4EnzUCouLk7r16/Xe++9p3Xr1ikqKkodOnRQy5YtFRYWlhGNAAAAAAAACDBpHkpJUkxMjB599NH0bgEAAAAAAECQ+EdDKeniq/Dt2bNHiYmJPsubNm36r6MAAAAAAAAQ2NI8lNqxY4fuu+8+/fLLL/J4PDLGSJI8Ho8kKTk5OX0LAQAAAAAAEHBC0voBPXv2VLFixXT48GFFR0drw4YNWrZsmapUqZLqq+UBAAAAAAAAf5XmI6VWrlypxYsXK1euXAoJCVFISIjuuOMODR06VD169NCaNWsyohMAAAAAAAABJM1HSiUnJytLliySpFy5cun333+XJBUpUkSbN29O3zoAAAAAAAAEpDQfKVWuXDmtW7dOxYoVU9WqVTVixAiFh4frzTffVPHixTOiEQAAAAAAAAEmzUOpF154QadPn5YkDR48WPfcc49q1KihnDlzavbs2ekeCAAAAAAAgMCT5qFUw4YNvf9fsmRJbdq0SceOHVP27Nm9r8AHAAAAAAAAXE2aril14cIFhYaG6tdff/VZniNHDgZSAAAAAAAAuGZpGkqFhYWpcOHCSk5OzqgeAAAAAAAABIE0v/re888/r+eee07Hjh3LiB4AAAAAAAAEgTRfU2rs2LHatm2bChQooCJFiigmJsbn9tWrV6dbHAAAAAAAAAJTmodSzZo1y4AMAAAAAAAABJM0D6UGDhyYER0AAAAAAAAIImm+phQAAAAAAADwb6X5SKmQkBB5PJ4r3s4r8wEAAAAAAODvpHko9fHHH/u8f+HCBa1Zs0bTpk3ToEGD0i0MAAAAAAAAgSvNQ6l7773Xb9l//vMf3XTTTZo9e7Y6duyYLmEAAAAAAAAIXOl2TanbbrtNixYtSq9PBwAAAAAAgACWLkOps2fP6vXXX1dcXFx6fDoAAAAAAAAEuDSfvpc9e3afC50bY3Ty5ElFR0fr3XffTdc4AAAAAAAABKY0D6Vee+01n6FUSEiIcufOrapVqyp79uzpGgcAAAAAAIDAlOahVPv27TMgAwAAAAAAAMEkzdeUmjJlij788EO/5R9++KGmTZuWLlEAAAAAAAAIbGkeSg0dOlS5cuXyW54nTx4NGTIkXaIAAAAAAAAQ2NI8lNqzZ4+KFSvmt7xIkSLas2dPukQBAAAAAAAgsKV5KJUnTx6tX7/eb/m6deuUM2fOdIkCAAAAAABAYEvzUKply5bq0aOHlixZouTkZCUnJ2vx4sXq2bOnHnrooYxoBAAAAAAAQIBJ86vvvfTSS9q1a5fq1aun0NCLH56SkqK2bdtyTSkAAAAAAABckzQPpcLDwzV79my9/PLLWrt2raKiolS+fHkVKVIkI/oAAAAAAAAQgNI8lLqkVKlSKlWqVHq2AAAAAAAAIEik+ZpSDzzwgIYPH+63fMSIEWrevHm6RAEAAAAAACCwpXkotWzZMt19991+yxs1aqRly5alSxQAAAAAAAACW5qHUqdOnVJ4eLjf8rCwMJ04cSJdogAAAAAAABDY0jyUKl++vGbPnu23fNasWSpbtmy6RAEAAAAAACCwpflC5/3799f999+v7du3q27dupKkRYsWaebMmZozZ066BwIAAAAAACDwpHko1aRJE33yyScaMmSI5syZo6ioKFWsWFGLFy9Wjhw5MqIRAAAAAAAAASbNQylJaty4sRo3bixJOnHihN5//30988wzWrVqlZKTk9M1EAAAAAAAAIEnzdeUumTZsmVq166dChQooJEjR6pu3br64Ycf0rMNAAAAAAAAASpNR0odPHhQU6dO1TvvvKMTJ06oRYsWOn/+vD755BMucg4AAAAAAIBrds1HSjVp0kQ33nij1q9fr9GjR+v333/XG2+8kZFtAAAAAAAACFDXfKTUl19+qR49eujxxx9XqVKlMrIJAAAAAAAAAe6aj5RasWKFTp48qcqVK6tq1aoaO3asjh49mpFtAAAAAAAACFDXPJS67bbb9NZbb+nAgQN67LHHNGvWLBUoUEApKSn65ptvdPLkyYzsBAAAAAAAQABJ86vvxcTE6JFHHtGKFSv0yy+/6Omnn9awYcOUJ08eNW3aNCMaAQAAAAAAEGDSPJS63I033qgRI0Zo3759ev/999OrCQAAAAAAAAHuXw2lLsmUKZOaNWumzz777B99/Lhx41S0aFFFRkaqatWq+umnn67p42bNmiWPx6NmzZr9o8cFAAAAAACAO9JlKPVvzJ49W7169dLAgQO1evVqVaxYUQ0bNtThw4ev+nG7du3SM888oxo1ajhUCgAAAAAAgPTi+lBq1KhR6ty5szp06KCyZctq4sSJio6O1uTJk6/4McnJyWrdurUGDRqk4sWLO1gLAAAAAACA9ODqUCoxMVGrVq1S/fr1vctCQkJUv359rVy58oofN3jwYOXJk0cdO3Z0IhMAAAAAAADpLNTNBz969KiSk5OVN29en+V58+bVpk2bUv2YFStW6J133tHatWuv6THOnz+v8+fPe98/ceLEP+4FAAAAAABA+nD99L20OHnypB5++GG99dZbypUr1zV9zNChQ5U1a1bvW6FChTK4EgAAAAAAAH/H1SOlcuXKpUyZMunQoUM+yw8dOqR8+fL53X/79u3atWuXmjRp4l2WkpIiSQoNDdXmzZtVokQJn4/p16+fevXq5X3/xIkTDKYAAAAAAABc5upQKjw8XJUrV9aiRYvUrFkzSReHTIsWLVK3bt387l+6dGn98ssvPsteeOEFnTx5UmPGjEl12BQREaGIiIgM6QcAAAAAAMA/4+pQSpJ69eqldu3aqUqVKrr11ls1evRonT59Wh06dJAktW3bVnFxcRo6dKgiIyNVrlw5n4/Pli2bJPktBwAAAAAAgL1cH0o9+OCDOnLkiAYMGKCDBw+qUqVK+uqrr7wXP9+zZ49CQq6rS18BAAAAAADgb7g+lJKkbt26pXq6niQtXbr0qh87derU9A8CAAAAAABAhuIQJAAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcZ8VQaty4cSpatKgiIyNVtWpV/fTTT1e871tvvaUaNWooe/bsyp49u+rXr3/V+wMAAAAAAMA+rg+lZs+erV69emngwIFavXq1KlasqIYNG+rw4cOp3n/p0qVq2bKllixZopUrV6pQoUJq0KCB9u/f73A5AAAAAAAA/qlQtwNGjRqlzp07q0OHDpKkiRMnat68eZo8ebL69u3rd//33nvP5/23335bH330kRYtWqS2bds60gwASF/D1hx19PH6xueyugMAAAAIBq4eKZWYmKhVq1apfv363mUhISGqX7++Vq5ceU2f48yZM7pw4YJy5MiR6u3nz5/XiRMnfN4AAAAAAADgLleHUkePHlVycrLy5s3rszxv3rw6ePDgNX2OPn36qECBAj6DrcsNHTpUWbNm9b4VKlToX3cDAAAAAADg33H9mlL/xrBhwzRr1ix9/PHHioyMTPU+/fr10/Hjx71ve/fudbgSAAAAAAAAf+XqNaVy5cqlTJky6dChQz7LDx06pHz58l31Y1999VUNGzZMCxcuVIUKFa54v4iICEVERKRLL4Drn5PXDLra9YJs6QAAAAAAt7h6pFR4eLgqV66sRYsWeZelpKRo0aJFqlat2hU/bsSIEXrppZf01VdfqUqVKk6kAgAAAAAAIB25/up7vXr1Urt27VSlShXdeuutGj16tE6fPu19Nb62bdsqLi5OQ4cOlSQNHz5cAwYM0MyZM1W0aFHvtacyZ86szJkzu/Z1AAAAAAAA4Nq5PpR68MEHdeTIEQ0YMEAHDx5UpUqV9NVXX3kvfr5nzx6FhPzvgK4JEyYoMTFR//nPf3w+z8CBA/Xiiy86mQ4AAAAAAIB/yPWhlCR169ZN3bp1S/W2pUuX+ry/a9eujA8CAAAAAABAhrquX30PAAAAAAAA1yeGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DiGUgAAAAAAAHAcQykAAAAAAAA4jqEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKAQAAAAAAwHEMpQAAAAAAAOA4hlIAAAAAAABwHEMpAAAAAAAAOI6hFAAAAAAAABzHUAoAAAAAAACOYygFAAAAAAAAxzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAAAAAAADgOIZSAAAAAAAAcBxDKQAAAAAAADiOoRQAAAAAAAAcx1AKAAAAAAAAjmMoBQAAAAAAAMcxlAIAAAAAAIDjGEoBAAAAAADAcQylAAAAAAAA4DgrhlLjxo1T0aJFFRkZqapVq+qnn3666v0//PBDlS5dWpGRkSpfvrzmz5/vUCkAAAAAAADSg+tDqdmzZ6tXr14aOHCgVq9erYoVK6phw4Y6fPhwqvf//vvv1bJlS3Xs2FFr1qxRs2bN1KxZM/36668OlwMAAAAAAOCfcn0oNWrUKHXu3FkdOnRQ2bJlNXHiREVHR2vy5Mmp3n/MmDG666671Lt3b5UpU0YvvfSSbr75Zo0dO9bhcgAAAAAAAPxToW4+eGJiolatWqV+/fp5l4WEhKh+/fpauXJlqh+zcuVK9erVy2dZw4YN9cknn6R6//Pnz+v8+fPe948fPy5JOnHixL+st8O5Uycde6wTJ8Kt6JCu3GJLhxScPxs66LgeOiS2IXTQQce/wzaEDjquvw7J/nXXlg4pOH9HroeO68mlmYsx5up3NC7av3+/kWS+//57n+W9e/c2t956a6ofExYWZmbOnOmzbNy4cSZPnjyp3n/gwIFGEm+88cYbb7zxxhtvvPHGG2+88cYbbw6+7d2796pzIVePlHJCv379fI6sSklJ0bFjx5QzZ055PB4Xy9xz4sQJFSpUSHv37lVsbCwddNBxnbTQQQcd11+HTS100EHH9ddhUwsddNBx/XW4yRijkydPqkCBAle9n6tDqVy5cilTpkw6dOiQz/JDhw4pX758qX5Mvnz50nT/iIgIRURE+CzLli3bP48OILGxsVasIHTQcT10SPa00EEHHddfh2RPCx100HH9dUj2tNBBBx3XX4dbsmbN+rf3cfVC5+Hh4apcubIWLVrkXZaSkqJFixapWrVqqX5MtWrVfO4vSd98880V7w8AAAAAAAD7uH76Xq9evdSuXTtVqVJFt956q0aPHq3Tp0+rQ4cOkqS2bdsqLi5OQ4cOlST17NlTtWrV0siRI9W4cWPNmjVLP//8s9588003vwwAAAAAAACkgetDqQcffFBHjhzRgAEDdPDgQVWqVElfffWV8ubNK0nas2ePQkL+d0BX9erVNXPmTL3wwgt67rnnVKpUKX3yyScqV66cW1/CdSciIkIDBw70O62RDjrosLuFDjrouP46bGqhgw46rr8Om1rooIOO66/jeuAx5u9enw8AAAAAAABIX65eUwoAAAAAAADBiaEUAAAAAAAAHMdQCgAAAAAAAI5jKAUAAAAAAADHMZQCAAAAAACA4xhKIWhMnTpVx48fdzsDf8ELgAIAMtKgQYN09OhRtzOA6wL7y0Da8Bzz7zGUCnDr1q1TpkyZHHu8+fPnq1OnTnr22We1adMmn9v+/PNP1a1b17GWv3r00Uf1+++/u/b4f7V371498sgjjjzW2bNntWLFCv32229+t507d07Tp093pCM1ERER2rhxo2uP/1enT5/WsmXL3M6wQocOHaxaZ/78809Hf1dTUlKuuHzPnj2OdfxV3bp1tXv3btce/68OHTqkwYMHO/Z4+/bt06lTp/yWX7hwwdV1t3jx4tq6datrj/9XSUlJjv6ejh8/XvXr11eLFi20aNEin9uOHj2q4sWLZ3jDiRMn/N6OHz+u//u//9OOHTu8y9zGtuwi27ZlTu4zs7+cNuybXWTbfpmTzzM8xwQ2j+EwhYC2bt06xcfHX3GHJD3NnDlTbdu21V133aXjx4/r559/1ttvv63WrVtLuvgPlwIFCig5OTlDO3LkyJHq8oSEBMXGxiok5OIs9tixYxna8XfWrVunm2++OcO/H1u2bFGDBg20Z88eeTwe3XHHHZo1a5by588vybmfS69evVJdPmbMGLVp00Y5c+aUJI0aNSpDO/6OUz+XCxcu6Pnnn9fcuXOVI0cOdenSxWdI6dTPRZLWr1+f6vIqVarogw8+8D7RV6hQIcNbrsapn82JEyfUqVMnff7554qNjdVjjz2mgQMHev+x4tTP5rPPPkt1+f33368xY8aoUKFCkqSmTZtmaMffcerncuDAAd17771atWqVPB6PWrVqpfHjxytz5sySnPu5vP7666ku79Wrl5599lnly5dPktSjR48M7fg7Tv1cpIvfk379+qlDhw46fvy4PvjgA7344ovq16+fJOd+NlcaKBhj5PF4vP914ntyNWzLLrJxW+bEPjP7y2kXbPtm7Jf54jkm8IW6HYB/5/7777/q7cePH5fH43Gk5ZVXXtGoUaO8O+IffPCBHnnkEZ07d04dO3Z0pEG6+IRSq1YtNW/e3LvMGOP9i1RcXJwjHVfaCbtkx44djnT06dNH5cqV088//6yEhAQ9+eSTuv3227V06VIVLlzYkQZJGj16tCpWrKhs2bL5LDfGaOPGjYqJiXHsd9UG//d//6fp06frmWeeUUJCgnr16qUff/xRkyZN8t7Hqb8ZVKpUyftk+lcPPPCAY0+yf/fXpZMnT2bo41/Sv39/rVu3TjNmzFBCQoJefvllrV69WnPnzlV4eLgkZ342zZo1u+LPpXv37pLkyM/lSjvHl2zevDlDH/+Svn37KiQkRD/++KMSEhLUt29f1alTR19//bWyZ88uyZmfy5NPPqm4uDiFhvruQqWkpGj69OkKCwuTx+NxfSjlpEmTJumtt95Sq1atJEmPP/64mjVrprNnzzp6FF3+/PlVqVIlPf30095/UBtjVL9+fb399tsqVqyYIx1sy3zZsi2zZZ+Z/WV72bJvZst+mS14jgl8HCl1nQsLC9Odd96pvHnzpnr7sWPH9MUXXziy0cqcObN++eUXnxVyyZIlatq0qV555RXdd999jkyxt23bplatWqlMmTIaN26c96/oYWFhWrduncqWLZuhj39JSEjIFZ9QLnHiCSVv3rxauHChypcvL+nixvOJJ57Q/PnztWTJEsXExDjycxk2bJjefPNNvf322z6HpTv9c7nSXwYvSU5O1qlTpzL8+1GqVCm99tpruueeeyRd/L1t1KiR7rjjDk2ePFmHDx927EipSpUqqWDBgnr11VcVFRUl6eLvSalSpfTll1+qVKlSkqQiRYpkaMeldeZKnNoJK1KkiKZNm6batWtLunhYeOPGjZUtWzZ99tlnSkhIcORn06hRI2XKlEmTJ09Wnjx5vMtt2pY5+ZfBuLg4ffzxx7r11lslSefPn1fz5s21d+9eLVq0SBcuXHDk59KlSxf9+OOPmjlzpsqUKeNd7vTP5eabb77q7WfPntWWLVsc2YZER0frt99+U9GiRb3Lfv31V9WvX18dOnTQk08+6cjP5tixY+rYsaOOHz+uGTNmeP9R7dY6cyVsyy5y+udiyz4z+8v+2DfzZct+mS3PMzzHBD6OlLrOlSlTRg888MAV/7Kydu1affHFF460xMbG6tChQz5PsnXq1NEXX3yhe+65R/v27XOko2TJkvr+++/1/PPPq1KlSpo2bZpuv/12Rx77cvnz59f48eN17733pnr72rVrVbly5QzvOHv2rM9f9D0ejyZMmKBu3bqpVq1amjlzZoY3SBePcqhXr57atGmjJk2aaOjQoQoLC3PksS93/vx5Pf74494h3V/t3r1bgwYNyvCO/fv3q1y5ct73S5YsqaVLl6pu3bp6+OGHNWLEiAxvuOSnn37Ss88+qwceeEDvvvuu4uPjvbcVKFAgw3d6LsmSJYuef/55Va1aNdXbt27dqsceeyzDO44cOeLzNefKlUsLFy5Uw4YNdffdd+vtt9/O8AZJ+vLLL/Xaa6+pSpUqGj9+vHcn2Wk5cuTQiBEjVK9evVRv37Bhg5o0aZLhHcePH/ceESVdvB7d3Llz1bx5c9WpU0fvvvtuhjdI0sSJE/Xxxx+rYcOGevbZZ9WtWzdHHvevfvvtNz300ENX/MvsgQMHtGXLFkdacuXKpb179/r8g6FcuXJavHix6tat69g1UHLkyKGPP/5YEyZM0K233qpXX31VLVu2dOSxL8e2zJct2zJb9pnZX/bHvpkvW/bLbHme4TkmCBhc19q3b2+eeOKJK97+22+/maJFizrScu+995oBAwaketuSJUtMTEyMCQkJcaTlkkWLFpnChQubfv36mbCwMLNhwwbHHrtJkyamf//+V7x97dq1xuPxZHjHLbfcYqZPn57qbV27djXZsmVz9Ody8uRJ07ZtW1OhQgXzyy+/OP5zqV69uhk9evQVb1+7dq0j349ixYqZhQsX+i3fv3+/ueGGG8ydd97p+Poyf/58U7BgQTNkyBCTnJxsQkNDHf3Z1K5d2wwfPvyKtzu1ztx4441m3rx5fstPnjxpqlWrZipWrOjoz2bNmjWmbNmy5tFHHzWnT592/OfSoEED89JLL13xdqd+LuXLlzdz5szxW37hwgXTrFkzU7hwYUd/Lvv27TN169Y1d911lzlw4IDjP5fKlSub8ePHX/H2NWvWOPb9aNmypXnyySdTve3XX381uXPndnx7tmHDBlOxYkXTsmVLtmV/EazbMlv2mdlf9se+Werc3i+z5XmG55jAx6vvXecmTpyoV1555Yq3lylTRjt37nSk5amnnlJkZGSqt9WuXVuff/652rZt60jLJXXr1tXq1au1adMmxcTEOPpKhL1791b16tWveHvJkiW1ZMmSDO+477779P7776d629ixY9WyZUvHrl0kXTxsfdq0aerXr5/q16/v+PnwjRs3VkJCwhVvz5EjhyO/p3Xr1k31KLUCBQpo8eLFjq23l2vUqJF+/vlnLV++3Hu6h5NatWp1xW2IJOXLl08DBw7M8I4GDRpoypQpfsszZ86sBQsWXLUxI1SqVEk///yzPB6PKlWq5Oj6Kl08Xe3yv07+VeHChVP9fqW3Ro0a6c033/RbHhoaqg8//FCVKlXK8IbLxcXFaeHChapZs6bi4+Md/7ncfvvtV72eV5YsWVSzZk1HWvr27XvFC+7edNNNWrx4sQYMGOBIyyVly5bVTz/9pHz58qlcuXLeU2CcwLYsdW5vy2zZZ2Z/2R/7Zqlze7/MlucZnmMCH9eUAuCavXv3avXq1apXr573WgbBYvfu3dq0aZMaNmyY6u2///67vvnmG7Vr187hsotef/11LVmyRG+88YYKFizoSoNb/vzzT/3++++66aabUr395MmTWr16tWrVquVw2cUXUFiyZIn69evnc22WYJCUlKQzZ84oNjb2irfv37/fsdMaLrdq1SqtWLFCbdu29TnFEHAT2zIgbWzeNwvm/TIEPoZSASQlJUXbtm3T4cOH/V7O1qm/ltrWYksHAADBICEhQT/99JPf867H49HDDz/seockx49CgX1s2T+kA0gbnmMCE0OpAPHDDz+oVatW2r17t9/h0E6/ZKgtLbZ0nD59WsOGDdOiRYtS3XDt2LGDDhc6pIsXm12yZEmqHU4eBmxLh00tixYtuuLvyOTJk+lwoSM5OVlTp069YsfixYvpcKFDsmfn+PPPP1fr1q116tQpxcbG+rwCncfj0bFjx4KqQ7Jj3aXDly37h3Skzpb9EDp82fA8Y8u23ZaOQMKr7wWILl26qEqVKpo3b57y589/1ZciDpYWWzo6deqkb7/9Vg8//DAdFnW89dZbevzxx5UrVy7ly5fP7wnFqSd6Wzpsahk0aJAGDx6sKlWquPo7Qoevnj17aurUqWrcuLHKlStHhyUdf7dz7ORQ6umnn9YjjzyiIUOGKDo62rHHtbXDlnWXDl+27B/S4c+W/RA6fNnyPGPLtt2WjoDi/LXVkRGio6PN1q1b3c4wxtjTYktH1qxZzYoVK9zOoOMvChcubIYNG+Z2hjUdxtjTki9fviu+YiQd7smZM2eqr+RFh7tKlSplevbsaU6fPu12iomOjjbbt293O8OaDlvWXTp82bJ/SIc/W/ZD6PBly/OMLdt2WzoCCa++FyCqVq2qbdu2uZ0hyZ4WWzqyZ8+uHDlyuJ1Bx1/8+eefat68udsZ1nRI9rQkJiZe9ZUr6XBHeHi4SpYs6XYGHX+xf/9+9ejRw4q/1jZs2FA///yz2xnWdNiy7tLhy5b9Qzr82bIfQocvW55nbNm229IRSDh9L0B0795dTz/9tA4ePKjy5csrLCzM5/YrvYxmILfY0vHSSy9pwIABmjZtmqsbczp8NW/eXF9//bW6dOniWoNNHTa1dOrUSTNnzlT//v3psKjj6aef1pgxYzR27FhXT++gw9elnePixYu71nBJ48aN1bt3b/3222+pPu82bdo0qDpsWXfp8GXL/iEd/mzZD6HDly3PM7Zs223pCCRc6DxAhIT4H/Tm8XhkjHH8IoW2tNjSER8fr+3bt8sYo6JFi/ptuFavXk2HCx1Dhw7VqFGj1Lhx41SfUHr06BFUHTa19OzZU9OnT1eFChVUoUIFv45Ro0bR4ULHfffdpyVLlihHjhy66aab/Drmzp1Lhwsd77zzjgYPHqwOHTq4vnOc2vPuJW4//7vRYcu6S4cvW/YP6fBny34IHb5seZ6xZdtuS0cgYSgVIHbv3n3V24sUKeJQiT0ttnQMGjToqrcPHDiQDhc6ihUrdsXbPB6PY68CaEuHTS116tS5aodTr2pGh68OHTpc9fYpU6bQ4UIHO8f2smXdpcOXLfuHdPizZT+EDl88zyCjMZQCAAAAAACA47imVADZvn27Ro8erY0bN0qSypYtq549e6pEiRJB22JLhyStWrXK23HTTTcpPj7e8QY6UndpNu/mdWFs6pDsadm3b58kqWDBgnRY0nHkyBFt3rxZknTjjTcqd+7cdFjQYYtvv/1Wr776qs/zbu/evVWjRo2g7LjEhnWXjv+xZf+QjiuzZT+EDrvYsm23pSNQ8Op7AWLBggUqW7asfvrpJ++5+j/++KNuuukmffPNN0HZYkvH4cOHVbduXd1yyy3q0aOHevToocqVK6tevXo6cuQIHS51SNL06dNVvnx5RUVFKSoqShUqVNCMGTMcbbCpw5aWlJQUDR48WFmzZlWRIkVUpEgRZcuWTS+99JJSUlLocKnj9OnTeuSRR5Q/f37VrFlTNWvWVIECBdSxY0edOXOGDpc6pIs7x02aNFHJkiVVsmRJNW3aVMuXL3e0QZLeffdd1a9fX9HR0d7te1RUlOrVq6eZM2cGXYct6y4dvmzZP6QjdTbsh9Dhz4bnGVu27bZ0BBSDgFCpUiXTp08fv+V9+vQx8fHxQdliS0eLFi1MlSpVzG+//eZdtmHDBlOlShXz0EMP0eFSx8iRI010dLR59tlnzaeffmo+/fRT07t3bxMdHW1GjRoVdB02tfTt29fkzp3bjB8/3qxbt86sW7fOjBs3zuTOnds899xzdLjU8eijj5rixYub+fPnm+PHj5vjx4+befPmmRIlSpguXbrQ4VLHjBkzTGhoqGnRooUZM2aMGTNmjGnRooUJCwsz7733nmMdxhhTunTpVLcVI0eONKVLlw66DlvWXTp82bJ/SIc/W/ZD6PBly/OMLdt2WzoCCUOpABEREWG2bNnit3zz5s0mIiIiKFts6YiNjTU//fST3/Iff/zRZM2alQ6XOooWLWqmTZvmt3zq1KmmaNGiQddhU0v+/PnNp59+6rf8k08+MQUKFKDDpY6cOXOaJUuW+C1fvHixyZUrFx0uddi0cxweHm62bt3qt3zr1q2OPu/a0mHLukuHL1v2D+nwZ8t+CB2+bHmesWXbbktHIOH0vQCRO3durV271m/52rVrlSdPnqBssaUjJSXF76VTJSksLMzxw+bp+J8DBw6oevXqfsurV6+uAwcOBF2HTS3Hjh1T6dKl/ZaXLl1ax44do8OljjNnzihv3rx+y/PkyePo6Wp0+NqxY4eaNGnit7xp06bauXOnYx2SVKhQIS1atMhv+cKFC1WoUKGg67Bl3aXDly37h3T4s2U/hA5ftjzP2LJtt6UjkHCh8wDRuXNnPfroo9qxY4d34/Xdd99p+PDh6tWrV1C22NJRt25d9ezZU++//74KFCggSdq/f7+eeuop1atXjw6XOkqWLKkPPvhAzz33nM/y2bNnq1SpUkHXYVNLxYoVNXbsWL3++us+y8eOHauKFSvS4VJHtWrVNHDgQE2fPl2RkZGSpLNnz2rQoEGqVq0aHS51XNo5LlmypM9yN3aOn376afXo0UNr1671ed6dOnWqxowZE3Qdtqy7dPiyZf+QDn+27IfQ4cuW5xlbtu22dAQUtw/VQvpISUkxo0aNMnFxccbj8RiPx2Pi4uLM6NGjTUpKSlC22NKxZ88eU6lSJRMWFmaKFy9uihcvbsLCwkx8fLzZu3cvHS51zJkzx2TKlMk0bNjQDB482AwePNg0bNjQhIaGmrlz5wZdh00tS5cuNTExMaZMmTLmkUceMY888ogpU6aMyZw5s1m2bBkdLnWsX7/eFChQwOTMmdPUrVvX1K1b1+TMmdPExcWZX3/9lQ6XOsaPH2/Cw8NNly5dzPTp08306dPNY489ZiIiIszEiRMd67hk7ty55vbbbzc5cuQwOXLkMLfffrv55JNPgrLDlnWXDl+27B/S4c+W/RA6fNn0PGPDtt2mjkDhMeb/v74krltJSUmaOXOmGjZsqLx58+rkyZOSpCxZsgRtiy0dlxhjtHDhQm3atEmSVKZMGdWvX58OlztWr16tUaNGeV/OtUyZMnr66acVHx8flB02tfz+++8aN26cz+/IE0884T26jg53Os6cOaP33nvPp6N169aKioqiw8WOjz/+WCNHjvRZb3v37q17773XsYakpCQNGTJEjzzyiAoWLOjY49racYkt6y4dF9myf0jHldmyH0KHL7efZ2zZttvSEXBcHYkh3URFRZldu3a5nWGMsafFho7ExESTKVMm88svv9BhWUeHDh3Mjh076LCsJTEx0dStWzfVi67S4W5H8eLFfV41kw73Oy5cuGAGDRrk6FGmVxMTE2N27tzpdoYVHTatu3T4smH/kA5/Nu2H0PE/Nj3P2LBtt6kjkHCh8wBx6623as2aNW5nSLKnxYaOsLAwFS5cWMnJyXRY1vHRRx+52mBTh2RPS1hYmNavX+92Bh2pdJw7d87tDDr+IjQ0VCNGjFBSUpLbKZKkevXq6dtvv3U7w4oOm9ZdOnzZsH9Ihz+b9kPo+B+bnmds2Lbb1BFIuNB5gHjiiSf09NNPa9++fapcubJiYmJ8bq9QoULQtdjS8fzzz+u5557TjBkzlCNHDkcek46/16xZM33yySd66qmnXGuwqcOmljZt2uidd97RsGHD6LCoo2vXrho+fLjefvtthYa6t/tAh69LO8dFixZ1reGSRo0aqW/fvvrll19Sfd5t2rRpUHXYsu7S4cuW/UM6/NmyH0KHL1ueZ2zZttvSEUi4plSACAnxP+jN4/HIGCOPx+PokSm2tNjSER8fr23btunChQsqUqSI34Zr9erVdLjQ8fLLL2vkyJGqV69eqk8oPXr0CKoOm1q6d++u6dOnq1SpUql2jBo1ig4XOu677z4tWrRImTNnVvny5f065s6dS4cLHRMnTtSgQYPUunVr13eOU3vevcTt5383OmxZd+nwZcv+IR3+bNkPocOXLc8ztmzbbekIJAylAsTu3buvenuRIkUcKrGnxZaOQYMGXfX2gQMH0uFCR7Fixa54m8fj0Y4dO4Kqw6aWOnXqXLVj8eLFdLjQ0aFDh6vePmXKFDpc6GDn2F62rLt0+LJl/5AOf7bsh9Dhi+cZZDSGUgAAAAAAAHAc15QKIFu3btWSJUt0+PBhpaSk+Nw2YMCAoGyxpUOSEhMTU+0oXLgwHS52AADSz6JFi7Ro0aJUt++TJ08Oug7YyZb9QzqAtLFl225LR6DgSKkA8dZbb+nxxx9Xrly5lC9fPnk8Hu9tHo/Hsev02NRiS8eWLVvUsWNHff/99z7LnT5Xnw5fycnJmjp16hWfUJw6jcCWDptaTp8+rWHDhl2xw6nD1enwdejQIT3zzDPejr/uPji17tLhz5ad40GDBmnw4MGqUqWK8ufP7/O8K0kff/xxUHXYsu7S4cuW/UM6/NmyH0KHPxueZ2zZttvSEUg4UipAvPzyy/q///s/9enTx+0Ua1ps6ejQoYNCQ0P1xRdfpLrhosOdjp49e2rq1Klq3LixypUrF/QdNrV06tRJ3377rR5++GFXf0fo8NW+fXvt2bNH/fv3p8Oijr/bOXbSxIkTNXXqVD388MOuNdjUYcu6S4cvW/YP6fBny34IHb5seZ6xZdtuS0dAMQgIWbJkMdu3b3c7wxhjT4stHdHR0Wbjxo1uZ9DxFzlz5jTz5s1zO8OaDmPsacmaNatZsWKF2xl0/EXmzJnNmjVr3M6g4y/y5ctnpk+f7naGMcaYHDlymG3btrmdYU2HLesuHb5s2T+kw58t+yF0+LLlecaWbbstHYHkypfSx3WlefPm+vrrr93OkGRPiy0dZcuW1dGjR93OoOMvwsPDVbJkSbczrOmQ7GnJnj27cuTI4XYGHX9RqFAhv1PU6HC/IzExUdWrV3c7Q9LFI2FmzpzpdoY1Hbasu3T4smX/kA5/tuyH0OHLlucZW7bttnQEEq4pFSCGDh2qUaNGqXHjxipfvrzCwsJ8bu/Ro0fQtdjSsXjxYr3wwgsaMmRIqh2xsbF0uNAxcuRI7dixQ2PHjnX1dBdbOmxqeffdd/Xpp59q2rRpio6OpsOSjq+//lojR47UpEmTVLRoUTos6ejTp48yZ86s/v37u9ZwSc+ePTV9+nRVqFBBFSpU8Nu+jxo1Kqg6bFl36fBly/4hHf5s2Q+hw5ctzzO2bNtt6QgkDKUCRLFixa54m8fjcezikTa12NIREhLifczLGYcv7E2Hr/vuu09LlixRjhw5dNNNN/k9ocydOzeoOmxqiY+P1/bt22WMUdGiRf06nLroKh2+smfPrjNnzigpKUnR0dF+HceOHaPDhQ6bdo7r1Klzxds8Ho9jF+W1pcOWdZcOX7bsH9Lhz5b9EDp82fI8Y8u23ZaOQMKFzgPEzp073U7wsqXFlo4lS5a4nSCJjr/Kli2b7rvvPrczrOmQ7Glp1qyZ2wmS6Pir0aNHu50giY6/Wr9+vSpVqiRJ+vXXX31uc/ov67Zs323psGXdpcOXLfuHdPizZT+EDl+2PM/Ysm23pSOQcKRUgElMTNTOnTtVokQJhYa6O3O0pcWWDgAAgsG2bdu0fft21axZU1FRUd4jYYO1A3ayZf+QDiBtbNm229IRCLjQeYA4c+aMOnbsqOjoaN10003as2ePJKl79+4aNmxYULbY0iFJy5cvV5s2bVS9enXt379fkjRjxgytWLGCDhc7kpKStHDhQk2aNEknT56UJP3+++86depUUHbY1JKQkKC3335b/fr1854GtXr1au/vCx3udGzfvl0vvPCCWrZsqcOHD0uSvvzyS23YsIEOFzukizvHCxYs0NmzZyXJlYuw//HHH6pXr55uuOEG3X333Tpw4IAkqWPHjnr66aeDrkOyZ92l439s2T+kI3W27IfQ4c/t5xlbtu22dAQU517oDxmpR48epnLlymb58uUmJibG+9Kun3zyialUqVJQttjSMWfOHBMVFWU6depkIiIivB1vvPGGadSoER0udezatcuULl3aREdHm0yZMnk7evToYR577LGg67CpZd26dSZ37tymZMmSJjQ01Nvx/PPPm4cffpgOlzqWLl1qoqKiTP369U14eLi3Y+jQoeaBBx6gw6WOo0ePmrp16xqPx2NCQkK8HR06dDC9evVyrMMYYx5++GHTsGFDs3fvXpM5c2Zvy1dffWXKli0bdB22rLt0+LJl/5AOf7bsh9Dhy5bnGVu27bZ0BBKGUgGicOHCZuXKlcYY47NybN261WTJkiUoW2zpqFSpkpk2bZpfx+rVq03evHnpcKnj3nvvNW3atDHnz5/36ViyZIkpWbJk0HXY1FKvXj3Tu3dvY4zv78h3331nihQpQodLHbfddpsZOXKkX8ePP/5o4uLi6HCpw6ad47x585q1a9caY3y/J9u3bzcxMTFB12HLukuHL1v2D+nwZ8t+CB2+bHmesWXbbktHIOGE4QBx5MgR5cmTx2/56dOnHT+31ZYWWzo2b96smjVr+i3PmjWrEhIS6HCpY/ny5fr+++8VHh7us7xo0aKOnkZgS4dNLf/97381adIkv+VxcXE6ePAgHS51/PLLL5o5c6bf8jx58ujo0aN0uNTx9ddfa8GCBSpYsKDP8lKlSmn37t2OdUgXn1+jo6P9lh87dkwRERFB12HLukuHL1v2D+nwZ8t+CB2+bHmesWXbbktHIOGaUgGiSpUqmjdvnvf9S08ib7/9tqpVqxaULbZ05MuXT9u2bfNbvmLFChUvXpwOlzpSUlKUnJzst3zfvn3KkiVL0HXY1BIREaETJ074Ld+yZYty585Nh0sd2bJl81434XJr1qxRXFwcHS512LRzXKNGDU2fPt37vsfjUUpKikaMGHHVl9AO1A5b1l06fNmyf0iHP1v2Q+jwZcvzjC3bdls6Aorbh2ohfSxfvtxkzpzZdOnSxURGRpqePXuaO++808TExJiff/45KFts6RgyZIgpW7as+eGHH0yWLFnM8uXLzbvvvmty585tXn/9dTpc6mjRooXp3LmzMebiobc7duwwJ0+eNHXr1jXt27cPug6bWjp27GiaNWtmEhMTvR27d+828fHxpmfPnnS41PH000+bO+64wxw4cMBkyZLFbN261axYscIUL17cvPjii3S41NGoUSPzwgsvGGP+t94mJyeb5s2bO3ptK2OM+eWXX0yePHnMXXfdZcLDw81//vMfU6ZMGZM3b16zbdu2oOuwZd2lw5ct+4d0+LNlP4QOX7Y8z9iybbelI5AwlAog27ZtM506dTK33HKLKVOmjGndurVZv359ULfY0JGSkmJefvllExMTYzwej/F4PCYyMtK7cafDnY69e/easmXLmjJlypjQ0FBz2223mZw5c5obb7zRHDp0KOg6bGpJSEgw9evXN9myZTOZMmUyhQoVMmFhYaZmzZrm1KlTdLjUcf78edOpUycTGhpqPB6PCQsLMyEhIaZNmzYmKSmJDpc6bNs5TkhIMC+//LJp3ry5adSokXn++efN77//HpQdtqy7dPizYf+QDn+27IfQ4cum5xkbtu02dQQKjzEuvGYwXDNs2DB16dJF2bJlczvFmhanOhITE7Vt2zadOnVKZcuWVebMmX1u37dvnwoUKKCQkIw9q5aO/0lKStLs2bO1bt06nTp1SjfffLNat26tqKioDHtMmztsa/nuu+98OurXr+94Ax3+9u7dq19++UWnTp1SfHy8SpUqRYfLHcePH9fYsWN9fj+6du2q/PnzO95yLZ544gkNHjxYuXLlCooOW9ZdOtIm2PZTbemwZT+EDl/X0/NMsD3HBAS3p2JwVpYsWbyvEOA2W1rooONq7r77biv+8mFLhzH2tJQrV87s2bPH7Qw6/sKWdZcOX48//rg5cuSI2xnGGHu+J7Z02LLu0uHLlt8POvzZsh9Chy9bnmds+V21peN6wIXOg4yx6MA4W1ro8EWHr2XLluns2bNuZ1jTIdnTsmvXLl24cMHtDDr+wpZ1lw5f7777bqoXmXaDLd8TWzpsWXfp8GXL7wcd/mzZD6HDly3PM7b8rtrScT1gKAUAABDg2DkGAGQknmfwTzGUAgAAAAAAgOMYSgEAAAAAAMBxDKUAS3g8HrcTJNEB4N+xZd2lAwAAwH4MpYJMjRo1XHlp99TY0mJLhy3nYdMB4N+wZd2lw15t2rRRbGys2xnWdMBOtuwf0gGkjS3bdls6rgehbgcgfbRt21Z16tRRzZo1VaJEiSveb/78+UHTYkvH4sWLVb16dUVGRl71fr/99psKFChAh0Md1+q5555Tjhw53M6wpkOyp2XSpEnKmzev2xlB07Fjxw4VL178b+/35ZdfKi4ujg6HOq6VEzvHNWvWVO3atVWrVi3dfvvtV9zOT5gwISg6rlWwbENs6khJSdG2bdt0+PBhpaSk+NxWs2ZNSc7sM9Pxz9iyH0KHLyeeZxISEvTTTz+l+rvatm1bSc5s223pCBQew5/wAkKnTp20bNkybdu2TXFxcapVq5Z3h6xUqVJB2WJLR+bMmZWUlKRbbrnFZyfZ6b820eFv69atWrJkSapPKAMGDAi6DptaFi1apEWLFqXaMXnyZDpc6AgJCVHBggV9tqUlS5Z05LHpuLJrHcA44eWXX9ayZcv0/fffKykpSVWqVPFpi46ODqoOyY51lw5fP/zwg1q1aqXdu3f7Hcno8XiUnJxMhwsdklS4cGHvulq7du2r/lGZDmddyxAmo33++edq3bq1Tp06pdjYWJ/T4z0ej44dOxZUHYGEoVSA2b9/v5YtW6Zvv/1W3377rbZs2aL8+fNr3759QdvidseFCxf0008/eR//+++/V2JioqpUqaI6dero5ZdfpsOFjrfeekuPP/64cuXKpXz58vk9oaxevTqoOmxqGTRokAYPHqwqVaoof/78ftfk+fjjj+lwoWP//v1aunSpd93dunWrChQooFq1aqlOnTrq1KkTHS502DSAuSQpKUn//e9/9e2332rp0qVavHixQkJCdO7cuaDqsGXdpcNXpUqVdMMNN2jQoEGpdmTNmpUOFzok6d1339WyZcu0dOlSnz8qXxrKOPVHZTp82TKEueGGG3T33XdryJAhrjy32dYRUAwCyunTp82CBQtM3759zW233WbCw8NNpUqVgrrFlo5Lfv31V9OuXTsTGhpqQkJC6HCpo3DhwmbYsGGOPZ7tHcbY05IvXz4zffp0tzPo+BtbtmyxYhtCx0UXLlww33//vRk6dKhp2LChCQsLMxEREY53GGPM5s2bzaRJk8xDDz1k8ufPb3LkyGGaNWsWdB22rLt0+IqOjjZbt251O4OOv/H777+b999/37Ru3drV7TsdxpQqVcr07NnTnD592rHHTE10dLTZvn27qw02dQQSrikVIJ577jktXbpUa9asUZkyZVSrVi317dtXNWvWVPbs2YOyxZaOLVu2aOnSpd6/qJ8/f141atTQq6++qtq1a9PhUseff/6p5s2bO/Z4tndI9rQkJiaqevXqbmfQ8RdnzpzRihUrvOvvmjVrVLp0aXXr1s3RdZeO1O3YsUO//PKL1q1bp/Xr1ytLlizea8E4pVWrVt7tes2aNb3PuxUqVHD0VQht6bBl3aXDV9WqVbVt2zZXTrel4+9dvm1dsmSJ1qxZo3Llyjm+XaXjf/bv368ePXq4flRQw4YN9fPPP1/T9RyDoSOQcPpegAgJCVHu3Ln11FNP6f7779cNN9wQ9C22dfTs2VP33HOPypcv78pLhNPhq2PHjrrlllvUpUsXxx/bxg6bWvr06aPMmTOrf//+dFjUER4eruzZs6t169aqXbu2atSo4fgfPejwl9oApnbt2o4PYKSL2/dcuXLpkUceUd26dXXHHXe48o8YWzpsWXfp8PXxxx/rhRdeUO/evVW+fHmFhYX53F6hQgU6XOiQpOrVq3v/mHzpNGQ3/sBOh6/7779fDz30kFq0aOHo4/7VO++8o8GDB6tDhw6p/q42bdo0qDoCCUOpALFu3TrvNROWL1+u8PBw745p7dq1HR3I2NJiS8eTTz6pZcuW6bffftPNN9/sfXynd5Lp8DV06FCNGjVKjRs3TvUJpUePHkHVYVNLz549NX36dFWoUEEVKlTw6xg1ahQdLnQ0a9ZMK1asUHh4uHe9dfr5hQ5/tgxgpItHWy5fvtx7JOzGjRtVqVIl7/emQYMGQdVhy7pLh6+QkBC/ZR6PR8YYRy/sTYe/HDlyKCQkRA0aNHBtm0qHP1uGMKn9rl7i9jrjRkcgYSgVoNatW6fXXntN7733nlJSUlxdOWxpcbsjISFBy5cv914Ud8OGDYqPj9d3331HhwsdxYoVu+JtHo9HO3bsCKoOm1rq1Klz1Y7FixfT4ULHJevXr/eut8uXL1doaKhq166t9957jw4XOmwZwKRm27Ztevnll11//nerw5Z1lw5fu3fvvurtRYoUocOFDkkyxuiXX37xbs+WLVvm/aNynTp11LlzZzpc6GAIg4zGUCpAGGO0Zs0a77UtVqxYoRMnTqhChQqqVauWXnvttaBrsaXjkj/++EPffvutlixZoqVLl+q3335T9uzZdfToUToc7jDGaM+ePcqTJ4+ioqIy/PFs77CpJTk5Wd99953Kly/vyqlQdPy9S9vWJUuWaMmSJVqwYIGMMUpKSqLDxY5L3BwEXdquX3re/e2335QtWzbvaYU9e/YMmg5b1l06fF24cEGlS5fWF198oTJlytBhSUdqjDFatWqVxo4d6+pgmw47XLhwQVFRUVq7dq3KlSsX9B2BhgudB4gcOXLo1KlTqlixomrVqqXOnTurRo0aypYtW9C22NLRo0cPn6FLzZo11blzZ9WuXVvly5enw4UOY4xKlSqlDRs2OPZyujZ32NSSKVMmNWjQQBs3bnT1Hy50+Bs1apR3wH/y5ElVrFhRNWvW1KOPPqoaNWrQ4VLHlQYwTZo0Ua1atRzrkKQ8efIoV65cqlGjhivPLzZ12LLu0uErLCxM586dc+3x6bi61atX+/wx+eTJkypfvry6d+/u6PaMjv+xZQgTFhamwoULuz6Is6Uj0DCUChDvvvuuatSoodjYWLdTrGmxpePAgQN69NFHVbt2bVc35nT8T0hIiEqVKqU//vjD1QGMLR22tZQrV047duy46umEdDjv/fffV61atbxDl6xZs9JhQYcNA5hL1q9fr5tuusmVx7axw5Z1lw5fXbt21fDhw/X2228rNNS9fwrR4e/WW29VfHy894/JNWvWdGXbSsf/2DSEef755/Xcc89pxowZypEjR9B3BBJO3wtA+/btkyQVLFjQ5RJ7WmzpgD0+//xzjRgxQhMmTHB1SGdLh00tX331lfr166eXXnpJlStXVkxMjM/tTg2a6cD1YMOGDVYMYC535MgRbd68WZJ04403Knfu3EHZYcu6S4ev++67T4sWLVLmzJlVvnx5v465c+fS4UKHJJ04ccKK5zQ6fL3zzjuaO3eu60OY+Ph4bdu2TRcuXFCRIkX8fldXr14dVB2BhKFUgEhJSdHLL7+skSNH6tSpU5KkLFmy6Omnn9bzzz9/1QvUBWqLLR2StH37do0ePVobN26UJJUtW1Y9e/ZUiRIlHGugw1f27Nl15swZJSUlKTw83O86SseOHQuqDptaLl83L39JezdfkYiOixISEvTOO+/4rLsdO3Z0/C+3dPhzewAjSadPn1b37t01ffp0paSkSLp42lbbtm31xhtvOPaqgLZ02LLu0uGrQ4cOV719ypQpdLjQcblVq1b5bFdvvvlmxxvo+B9bhjCDBg266u0DBw4Mqo5AwlAqQPTr10/vvPOOBg0apNtvv12StGLFCr344ovq3Lmz/u///i/oWmzpWLBggZo2bapKlSp5O7777jutW7dOn3/+ue688046XOiYNm3aVW9v165dUHVI9rR8++23V73dqeso0OHr559/VsOGDRUVFaVbb71VkvTf//5XZ8+e1ddff+3YTjIdvmwZwEjSY489poULF2rs2LE+z7s9evTQnXfeqQkTJgRVhy3rLh24Xhw+fFgPPvigvv32W+81YBMSElSnTh3NmjXLsWE7Hb4YwiDDGQSE/Pnzm08//dRv+SeffGIKFCgQlC22dFSqVMn06dPHb3mfPn1MfHw8HS51AEibO+64w7Rv395cuHDBu+zChQumXbt2pkaNGnS41PHoo4+a4sWLm/nz55vjx4+b48ePm3nz5pkSJUqYLl26ONZhjDE5c+Y0S5Ys8Vu+ePFikytXrqDrgL0uXLhgvvnmGzNx4kRz4sQJY4wx+/fvNydPnqTDxY4WLVqYKlWqmN9++827bMOGDaZKlSrmoYceosOlDpv8+eef5q233jJ9+/Y1f/zxhzHGmFWrVpl9+/YFZUegYCgVICIiIszmzZv9lm/atMlERkYGZYtNHVu2bPFbvnnzZhMREUGHSx3GGLNt2zbz/PPPm4ceesgcOnTIGGPM/Pnzza+//hqUHTa1LFu2zLRu3dpUq1bN+wQ/ffp0s3z5cjpc6oiMjDQbN270W75hwwYTFRVFh0sdNg1goqKifP7xdMmvv/5qoqOjg67DGDvWXTp87dq1y5QuXdpER0ebTJkyme3btxtjjOnRo4d57LHH6HCpwxhjYmNjzU8//eS3/McffzRZs2alw6UOY+wYwqxbt87kzp3blCxZ0oSGhnp/V59//nnz8MMPB11HIHHuojrIUBUrVtTYsWP9lo8dO1YVK1YMyhZbOnLnzq21a9f6LV+7dq3y5MlDh0sd3377rcqXL68ff/xRc+fO9V53bN26dY4ehmxLh00tH330kfe0qNWrV+v8+fOSpOPHj2vIkCF0uNQRGxurPXv2+C3fu3evsmTJQodLHWfOnFHevHn9lufJk0dnzpxxrEOSqlWrpoEDB/q8xPzZs2c1aNAgVatWLeg6bFl36fDVs2dPValSRX/++afPtRMvXfCbDnc6pIvXgw0LC/NbHhYW5j09mQ7nO9avX68bbrhBw4cP16uvvqqEhARJFy+C369fP8c6evXqpfbt22vr1q2KjIz0Lr/77ru1bNmyoOsIKG5PxZA+li5damJiYkyZMmXMI488Yh555BFTpkwZkzlzZrNs2bKgbLGlY9CgQSZbtmxm2LBhZtmyZWbZsmVm6NChJlu2bGbw4MF0uNRx2223mZEjRxpjjMmcObP3rxw//vijiYuLC7oOm1oqVapkpk2b5texevVqkzdvXjpc6ujevbspWLCgmTVrltmzZ4/Zs2ePef/9903BggVNz5496XCpo27duqZ58+bm7Nmz3mVnzpwxzZs3N/Xq1XOswxhjfvnlF1OgQAGTM2dOU7duXVO3bl2TM2dOExcX5+jRlrZ02LLu0uErR44cZtOmTX4dO3fudPQoRzr8NW3a1NSsWdPs37/fu2zfvn2mVq1aplmzZnS41FGvXj3Tu3dvY4zv78h3331nihQp4lhHbGys2bZtm1/Hrl27HD3bwpaOQMJQKoDs37/fPPfcc+b+++83999/v3n++ed9NmLB2GJDR0pKihk1apSJi4szHo/HeDweExcXZ0aPHm1SUlLocKkjJibG7NixwxjjvxPm5BOKLR02tURFRZmdO3f6dWzfvp0OFzvOnz9vevToYcLDw01ISIgJCQkxERER5sknnzTnzp2jw6UOWwYwl5w+fdq8+eabplevXqZXr17mrbfeMmfOnAnKDlvWXTp8ZcuWzWzYsMGvY/ny5SZPnjx0uNRhjDF79uwxlSpVMmFhYaZ48eKmePHiJiwszMTHx5u9e/fS4VKHLUOY3Llzm9WrV/t1fP3116ZgwYJB1xFIQt0+Ugvpp0CBAo6+yt7V2NJiQ4fH49FTTz2lp556SidPnpQkR0/toCN12bJl04EDB1SsWDGf5WvWrFFcXFzQddjUki9fPm3btk1Fixb1Wb5ixQoVL16cDpc6wsPDNWbMGA0dOlTbt2+XJJUoUcLRV3ejw1+5cuW0detWvffee9q0aZMkqWXLlmrdurXPqThOiY6OVufOnR1/XBs7bFl36fDVoEEDjR49Wm+++aaki/slp06d0sCBA3X33XfT4VKHJBUqVEirV6/WwoULvduzMmXKqH79+nS42BEREaETJ074Ld+yZYtjrwAoSU2bNtXgwYP1wQcfSLr4u7pnzx716dNHDzzwQNB1BBS3p2JIP8eOHTOvvPKK91S1V1991XshumBtsaXDGGMOHTrkPV3t8OHDrjTQ8T9PP/20ueOOO8yBAwdMlixZzNatW82KFStM8eLFzYsvvhh0HTa1DBkyxJQtW9b88MMPJkuWLGb58uXm3XffNblz5zavv/46HS51XO7S6Wpuo8M+mzZtMl27dvUetdW1a9dULwgfDB22rLt0+Nq7d68pW7asKVOmjAkNDTW33XabyZkzp7nxxhu9L/BBh/MdsFfHjh1Ns2bNTGJiosmcObPZsWOH2b17t4mPj3f0dPWEhARTv359ky1bNpMpUyZTqFAhExYWZmrWrGlOnToVdB2BhKFUgPj2229NbGysKVSokLnvvvvMfffdZwoXLmxiY2PNt99+G5QttnScOHHCtGnTxmTKlMl7ulpoaKhp3bq1SUhIoMOljvPnz5tOnTqZ0NBQ4/F4TFhYmAkJCTFt2rQxSUlJQddhU0tKSop5+eWXTUxMjPd3JDIy0rzwwguONdDh78KFC+aFF14wsbGx3tPVYmNjzfPPP28SExPpcKnDGDsGMMYYM2fOHO8/ap966inz1FNPmWrVqpnQ0FAzZ86coOuwZd2lw9+FCxfMjBkzTO/evc3jjz/u2mmmdPhbuHChady4sfd0tcaNG5tvvvmGDhc7bBvCLF++3IwbN84MHz7clZ+JbR2BgKFUgChXrpzp3Lmzzz8ck5KSzKOPPmrKlSsXlC22dLRo0cKUKlXKfPXVV+b48ePm+PHj5quvvjI33nijefDBB+lwqeOS3bt3m3nz5pnZs2ebLVu2OP74tnXY1HL+/HmzYcMG8+OPP5qTJ0/S4XJHly5dTJ48eczEiRPNunXrzLp168zEiRNNvnz5TJcuXehwqcOWAYwxxhQvXtz079/fb/mAAQNM8eLFg67jErfXXTpwvRg3bpwJDQ01Dz30kBkzZowZM2aMadmypQkLCzNjx46lw6WOSxjCIKMwlAoQkZGR3lfOuNymTZtMZGRkULbY0hEdHW2WL1/ut3zZsmUmOjqaDpc6LpeSkuLoRdZt7zDGnhZbToui4+KFTufPn++3fN68eSY2NpYOlzpsGsBERUWZrVu3+i3fsmWLo6/iZUvH5diG2NVhy9GFdPiKi4szb7zxht/ysWPHmgIFCtDhUodNbDhyzKaOQBHi9jWtkD5uvvlmbdy40W/5xo0bVbFixaBssaUjZ86cypo1q9/yrFmzKnv27HS41CFJ77zzjsqVK6fIyEhFRkaqXLlyevvttx1tsKnDlpakpCT1799fWbNmVdGiRVW0aFFlzZpVL7zwgi5cuECHSx0RERF+FyiWpGLFiik8PJwOlzoOHDigtm3b+i1v06aNDhw44FiHJNWuXVvLly/3W75ixQrVqFEj6DpsWXfp8PXRRx+pXLlyWrVqlSpWrKiKFStq9erVKl++vD766CM6XOqQpISEBN11111+yxs0aKDjx4/T4VKHJC1atEj33HOPSpQooRIlSuiee+7RwoULHW0YP3687rrrLmXJkkU9e/ZUz549FRsbq7vvvlvjxo0Luo6A4vZUDP/cpdMF1q1bZ2bNmmUKFy5sXnnlFbN8+XKzfPly88orr5iiRYuaWbNmBU2LLR2XmzRpkqlfv745cOCAd9mBAwdMgwYNzMSJE+lwqaN///4mJibG9O3b13z66afm008/NX379jWZM2dO9aiDQO+wqcWW06Lo8DVo0CDTsmVLc+7cOe+yc+fOmdatWzt6IXw6fDVq1MhMnjzZb/nkyZNNgwYNMvzxL20rPv30UzNhwgSTO3du07VrVzNjxgwzY8YM07VrV5MnTx4zYcKEoOi4nC3rLh2+bDm6kA5/LVu2NCNGjPBb/sorrzh6iQc6fNlyGqEtR47Z0hFIPMYY4/ZgDP9MSEiIPB6P/u5H6PF4lJycHBQttnTEx8fL4/F439+6davOnz+vwoULS5L27NmjiIgIlSpVSqtXr6bDoY7L5c6dW6+//rpatmzps/z9999X9+7ddfTo0aDqsKkla9asmjVrlho1auSzfP78+WrZsqVjfx2kQ7r//vt93l+4cKEiIiK8R5uuW7dOiYmJqlevnubOnUuHQx2fffaZ9/9///13DRgwQC1atNBtt90mSfrhhx/04YcfatCgQerSpUuGdUgXn3evhRPP/zZ0XI5tiJ0d0dHRWr9+vUqWLOmzfOvWrapYsaLOnDlDh4Mdr7/+uvf/T5w4oVdffVW33367qlWrJuni9uy7777T008/rRdeeIEOhzouV7BgQfXt21fdunXzWT5u3DgNGTJE+/fvd6Qjc+bMWrt2baq/q/Hx8Tp16lRQdQSSULcD8M/t3LnT7QQvW1ps6WjWrJnbCZLouJoLFy6oSpUqfssrV66spKSkoOuwqcWW06LokN+ptg888IDP+4UKFcrQx6cjdaltU8ePH6/x48f7LOvatWuGD6VSUlIy9PNfK1s6Lsc2xM6OS6d3/vUflG6dZhrsHa+99prP+9mzZ9dvv/2m3377zbssW7Zsmjx5coYOYei4squdRtinTx9HGiSpadOm+vjjj9W7d2+f5Z9++qnuueeeoOsIJBwpFWQaN26st99+W/nz53c7xZoWWzref/99NW3aVDExMXQ40NG9e3eFhYVp1KhRPsufeeYZnT171rFzwm3psKll8ODB2rRpk6ZMmaKIiAhJ0vnz59WxY0eVKlVKAwcOpMOFjmv13XffqUqVKt5WOuzosEn58uU1f/58x4Z4bnXYsu7SYc/RhXTgetSqVSvFx8f7DWFeffVV/fzzz5o1a1aGPbYtR47Z0hGoGEoFmSxZsmjdunUqXry42ynWtNjSERsbq7Vr19KRgR29evXy/n9SUpKmTp2qwoULe3fCfvzxR+3Zs0dt27bVG2+8kW6Pa2uHTS22nBZFx78XyNuQ67nDlkGQZM/zbkZ02LLu0uHLltM76UgftmxXA7nDliFMsWLFrul+Ho9HO3bsCPiOQMXpe4AlbJkPB3LHmjVrfN6vXLmyJGn79u2SpFy5cilXrlzasGFDuj+2jR02tdhyWhQd/14gb0P+CVs6du3a5egrnAUrW9ZdOnzZcnonHenDlu1qIHfYchqhLZdmsaUjUDGUAhA0lixZkuaP2bdvnwoUKHDNf1W8njpsapkyZUqaPyYjTouiA8C/Ycu6S8e/Z8vRhXTADdfzECaQj2ALVOn7rxsACDBly5bVrl273M6wpkOyp6VRo0aOveILHQAyii3rLh2+bDm6kA5cL2JjY604dS2Qj2ALVAylAOAqbHlCsaVDsqeFDl+2dABIG1vWXToA/Busu/inGEoBAIB04/F43E6QRAcABCpbtqt0AOmDoVSQee6555QjRw63MyTZ02JLR5EiRRQWFuZ2Bh0A/BhjtGfPHp07d+6a7kuHMx02uXDhgurVq6etW7f+7X0nTZqkvHnzBnQHgIxjy3aVDiB9cKHzALJ582a98cYb2rhxoySpTJky6t69u2688Ubvffr16xdULbZ0SNLPP//s01GlShWf23/99Vc6XOgA8PeMMSpZsqQ2bNigUqVKXfW+J0+epMOhjgsXLuiuu+7SxIkT/7YjowcwYWFhWr9+/TXdt1WrVgHfASDjfPnll4qLi3M7gw5L2XLkmC0d1wOGUgHio48+0kMPPaQqVaqoWrVqkqQffvhB5cqV06xZs/xegjcYWmzp2Ldvn1q2bKnvvvtO2bJlkyQlJCSoevXqmjVrlgoWLEiHCx3XypYnFFs6JHta6PCVkR0hISEqVaqU/vjjj78dfmQkOnzZNoBp06aN3nnnHQ0bNizDH+t66LhWwbANSQtbOuCMXr16XfN9R40aJUm644476Mjgjn/ClnXXliPHbOm4HjCUChDPPvus+vXrp8GDB/ssHzhwoJ599llHh1K2tNjS0alTJ124cEEbN270HqG1efNmdejQQZ06ddJXX31Fhwsd18qWJxRbOiR7WujwldEdw4YNU+/evTVhwgSVK1cuQx+Ljmtn0wAmKSlJkydP1sKFC1W5cmXFxMT43H7pH1DB0GGM0d69e5UnTx5FRkb+7X3pcKbDlqML6fifNWvW+Ly/evVqJSUlefcRt2zZokyZMqly5crp/th0pC9b9odsOXLMlo7rgcfY8tuDfyU6Olrr169XyZIlfZZv3bpVFStW1JkzZ4KuxZaOqKgoff/994qPj/dZvmrVKtWoUYMOlzout3fvXklSoUKFUr2tQIECypQpU9B02NJy+PBhbd68WZJ04403Kk+ePBn6eHRcXfbs2XXmzBklJSUpPDxcUVFRPrcfO3aMDhc6unfvrunTp6tUqVKuDoIkqU6dOle8zePxaPHixUHTkZKSosjIyGs6xZMOZ+XOnVvff/89HZZ1SBe3V0uXLtW0adOUPXt2SdKff/6pDh06qEaNGnr66afpcKHjWq1YsUK33HKLIiIi0u1z/pMjxzKCLR2BiiOlAkTt2rW1fPlyvwHMihUrVKNGjaBssaWjUKFCunDhgt/y5ORkFShQgA6XOpKSkjRo0CC9/vrrOnXqlCQpc+bM6t69uwYOHOi9yHpqQ5lA7LCp5eTJk3riiSc0a9YsJScnS5IyZcqkBx98UOPGjVPWrFkz9PHpSN3o0aMdeZy/Q4evX3/9VTfffLOki39Bv5zTp1IsWbLE0ce7Ehs6bDnFkw5/thxdSIe/kSNH6uuvv/YOYKSLfwB4+eWX1aBBA8eGMHTYcxqhLUeO2dIRqBhKXcc+++wz7/83bdpUffr00apVq3TbbbdJunj9pA8//FCDBg0KmhZbOi73yiuvqHv37ho3bpz3Yt4///yzevbsqVdffZUOlzq6d++uuXPnasSIEd5rjq1cuVIvvvii/vjjD02YMCGoOmxq6dSpk9asWaMvvvjCp6Nnz5567LHHNGvWLDpc6GjXrp0jj/N36PBlwwAGqbPlFE86fNlweicdqTtx4oSOHDnit/zIkSMZ+qIRdPizZQhz+XPcqFGjlCVLliseORYMHYGK0/euYyEhIdd0P4/H4/3reqC32NKRPXt2n79Qnz59WklJSQoNvTgHvvT/MTExGXqKBx1XljVrVs2aNUuNGjXyWT5//ny1bNlSx48fD6oOm1piYmK0YMECv7+4LV++XHfddZdOnz5Nh0MdJ06cuOb7xsbG0uFQh03uv//+a77v3LlzA77jcrac4kmHLxtO76QjdW3bttXy5cs1cuRI3XrrrZKkH3/8Ub1791aNGjU0bdo0OlzosOU0wri4OH399de66aabfJb/+uuvatCggX7//feg6ggkHCl1HUtJSXE7wcuWFls6bDmtg44ri4iIUNGiRf2WFytWTOHh4UHXYVNLzpw5Uz0lLWvWrD6HsNOR8R3ZsmX721PAjDEZPuinw5dNAxinTh/9O7Z0XM6W5z46fNlydCEd/iZOnKhnnnlGrVq18l7qITQ0VB07dtQrr7xCh0sdtpzOyBFsgYsjpQAEpcGDB2vTpk2aMmWK94KM58+fV8eOHVWqVCkNHDgwqDpsannzzTf14YcfasaMGcqXL58k6eDBg2rXrp3uv/9+PfbYY3Q41PHtt99e831r1apFh0MdHTp0uOb7TpkyJcM6/qnvvvtOVapUSdeL4V7PHQB8nT59Wtu3b5cklShRwu+UQjqc7ciSJYs+//xz1a5d22f5kiVL1LRpU8cGMbYcOWZLRyBhKBVAFi1apEWLFunw4cN+RwxNnjw5KFts6UhJSdG2bdtS7ahZsyYdDnX89eiChQsXKiIiQhUrVpQkrVu3TomJiapXr56jp5m41WFbyyXx8fHatm2bzp8/r8KFC0uS9uzZo4iICL8L5K5evZoOhzqu1RNPPKHBgwcrV65cdFjUYdMAJjY2VmvXrlXx4sUDrsOWUzzp8GXL0YV04HpkyxDmzJkzeuaZZzR58uRUjxxzalhnS0cg4fS9ADFo0CANHjxYVapUUf78+R1/xR0bW2zp+OGHH9SqVSvt3r1bf50BO3G9Lzr+56+ndzzwwAM+7zvxCnc2ddjWckmzZs0cf8zU0PHPvPvuu3rmmWdcH8LQ4atRo0ZWDIIk+W373ZIRHbac4kmHL1tO76Tj6u67775Uf188Ho8iIyNVsmRJtWrVynuxbTqc6bDlNMLo6GiNHz9er7zyiqtHjtnSEUg4UipA5M+fXyNGjNDDDz/sdoo1LbZ0VKpUSTfccIMGDRqU6nDMqR0DOgA4IUuWLFq3bp3rww867OywqSUjOmw5xZOOf8+WowuDqaN9+/b65JNPlC1bNu+ruq1evVoJCQlq0KCB1q1bp127dmnRokW6/fbb6XCo4xK3TyNE4OJIqQCRmJio6tWru50hyZ4WWzq2bt2qOXPmqGTJknRY1AG7JSQkaM6cOdq+fbt69+6tHDlyaPXq1cqbN6/i4uLocKkDwNX9k8FKRpziSce/Z8vRhcHUkS9fPrVq1Upjx471vqJ2SkqKevbsqSxZsmjWrFnq0qWL+vTpoxUrVtDhUMclMTExqlChQoY/zpXYcuSYLR2BJMTtAKSPTp06aebMmW5nSLKnxZaOqlWratu2bW5n0PEXhw4d0sMPP6wCBQooNDRUmTJl8nkLtg6bWtavX68bbrhBw4cP16uvvqqEhARJF69r0a9fPzpc6gCQMd599900XXuJDmfYcjJJMHW88847evLJJ70DGEkKCQlR9+7d9eabb8rj8ahbt2769ddf6XCw47777tP999/v9/bAAw+odevWGjhwoDZv3pyhDdLFsykWL16s1atXy+PxyOPxaM2aNVq8eLGSkpI0e/ZsVaxYUd99911QdAQSjpQKEOfOndObb76phQsXqkKFCgoLC/O5fdSoUUHXYktH9+7d9fTTT+vgwYMqX768X4dTf3Ggw1f79u21Z88e9e/f39VrjtnSYVNLr1691L59e40YMUJZsmTxLr/77rvVqlUrOlzqANKLm9u5y9nSEUxDh2thSwecl5SUpE2bNumGG27wWb5p0ybvNcciIyMzfN2lw1fWrFmvehrh7NmzNXz48Aw/jdCWI8ds6QgkDKUCxPr161WpUiVJ8puWO73TZUuLLR2XLhz9yCOP+Dy+Exf2pOPKVqxYoeXLl3t/R9xiS4dNLf/97381adIkv+VxcXE6ePAgHS514PpmywBGsmfoYEsHgIsefvhhdezYUc8995xuueUWSRefA4cMGaK2bdtKunidsptuuokOBztsGcK88847+u6771I9cqx69eoaMmSIunXrpho1amRYg00dgYShVIBYsmSJ2wletrTY0rFz5063EyTR8VeFChWy4h8ktnRI9rRERESkeurGli1blDt3bjpc6rhWbdq0ydCXdqfjn3Fy3T58+LD3VI4bb7xRefLk8bn95MmTQdUB4Nq89tpryps3r0aMGKFDhw5JujgQeeqpp9SnTx9JUoMGDXTXXXfR4WCHLUMYW44cs6UjkDCUAjJYkSJF3E6QRMdfjR49Wn379tWkSZNUtGjRoO+wqaVp06YaPHiwPvjgA0kXj/DYs2eP+vTp4z3Sjg7nO6SLF1z/6aefdPjwYaWkpPjcdumvthMmTKDD4Q7JjgHMyZMn9cQTT2jWrFneHfNMmTLpwQcf1Lhx4xx7dVVbOnB9s+UflMHUkZiYqKeeekrPP/+8Tpw44X1lubJly3qvbVm4cGE6HO6wZQhjy5FjtnQEEo+x4c/i+NfOnTunN954Q0uWLEl153j16tVB12JLhyT9/vvvWrFiRaodPXr0oMOFjuzZs+vMmTNKSkpSdHS037Wtjh07FlQdNrUcP35c//nPf/Tzzz/r5MmTKlCggA4ePKhq1app/vz5jr0EMR2+Pv/8c7Vu3VqnTp1SbGysz86nx+Nx7PeDDl82DWAefPBBrVmzRm+88YaqVasmSVq5cqV69uypSpUqadasWUHVca2yZMmidevWuf7qanTQ4XZHgwYNdP/996tLly5KSEhQ6dKlFRYWpqNHj2rUqFF6/PHHM+yx6biyHj166P333091CNOqVSuNGTNGb7/9tqZOnZqhp+8lJydr2LBhGjt2rM+RY926dVOfPn2UKVMm7dmzRyEhISpYsGDAdwQShlIBonXr1vr666/1n//8R3nz5vWbVA8cODDoWmzpmDp1qh577DGFh4crZ86cfv9w2bFjBx0udEybNu2qt7dr1y6oOiS7WiTpu+++07p163Tq1CndfPPNql+/vqOPT4evG264QXfffbeGDBmi6OhoRx+bjiuzaQATExOjBQsW6I477vBZvnz5ct111106ffp0UHVcq8cff1wvvfSScuXKRYeDHX93dKFT6PifXLlyeY8wefvtt/XGG29ozZo1+uijjzRgwABt3LiRDhc6bBnCnD17VsYYRUdH+x051rBhwwx7XFs7AopBQIiNjTUrVqxwO8MYY0+LLR0FCxY0L7/8sklOTqbDog7Ya9q0aebcuXN+y8+fP2+mTZtGh0sd0dHRZvv27Y49Hh3X3rF8+XK/5cuWLTPR0dGOthQqVMisX7/eb/m6detMXFxc0HUYY8yff/5pFixYYGbMmGGmTZvm80aHOx0nTpwwbdq0MaGhocbj8RiPx2NCQ0NN69atTUJCAh0udRhjTFRUlNm9e7cxxpjmzZubF1980RhjzJ49e0xUVBQdLnWcOXPGnD592hhjzPHjx826devMqFGjzFdffeVYgzHG3HnnnWbChAnGmIvbkrx585qCBQuayMhIM378+KDrCCQMpQJEmTJlzLp169zOMMbY02JLR44cOcy2bdvczqAjFUlJSebDDz80gwcPNoMHDzZz5swxFy5cCNoOW1pCQkLMoUOH/JYfPXrUhISE0OFSx3333Wdmz57t2OPRcW1sGsBMmjTJ1K9f3xw4cMC77MCBA6ZBgwZm4sSJQdfx2Wef/b/27j2sqjLv//hnb05y2BtUwGOEiAmeCDMDtTJRMSsTNQ9YPirqYyUaHqbpZ4paZuZ4LPOEDyMdNPOAlY1SZlqKkQmYpWkmIAWKJiUPymH7/f3BsMcdas487rVu9/68rotr5Ia51vsPh3F9ue+1xGQyicFgEF9fX/Hz87N+1K9fnx06dQwePFhatWolO3bskN9++01+++032bFjh7Ru3VqGDBnCDp06RETat28vS5culYKCAjGbzbJ//34RETl48KA0atSIHTp1qDKEadiwoRw5ckRERNasWSMdOnQQi8UiGzdulLCwMKfrcCQcSjmIjz/+WPr06SN5eXl6pyjTokrHtGnTZN68ebo2sKOuI0eOSEhIiHh5eUlkZKRERkaKt7e3BAcHy7fffut0HSq1GAwGOXv2bJ31nJwcTW9c2GErJSVFgoKCJDk5WTZt2iTbtm2z+WCHPh2qDGBERO6++27x8fERNzc3admypbRs2VLc3NzEx8fH+jOl9sMZOlq1aiWTJk2y7jDQCztsqbK7kB11vf/+++Lm5iZGo1F69eplXX/llVekT58+7NCpQ5UhjCo7x1TpcCR8ppSDKCkpweDBg7F3717dH5asSosqHRaLBY8++iguXbqE9u3b1+lYtGgRO3ToiI6ORkBAANatW4f69esDAC5cuICRI0eipKQE+/fvd6oOFVoiIyNhMBiQm5uLtm3bwtX1Xy+ItVgsOHXqFPr06WN9Cx07tOmodfWroP/IYDBYH7LNDm07IiMj8eOPP6KiosL6FqaCggJ4eHigVatWNt9r7xd8zJ49+6a/157PdVSlw9vbG99++63uD6hmh62goCBs374d7du3t1k/fPgw+vbti8LCQnbo0FGruLgYRUVFiIiIsP6czcrKgtlsRlhYGDt06PDy8sKxY8cQFBSEwYMHo23btkhOTsbp06fRunVrlJeXa9LRoUMHjBkzBnFxcWjXrh127NiB6OhofPPNN3jkkUdQXFzsVB2OxPXPv4VuB8OGDcPPP/+MV1555ZoP9XbGFlU65s2bh507d6J169YAUOfB3uzQpyMnJwcHDx60Dl+AmrfPzZ071/pmEWfqUKGlf//+1o7Y2Fj4+PhYv+bu7o7g4GAMHDiQHRp31PrjmzL1wg5btX9PVKDlS1VuRJWO2NhYHDx4UPchDDtsvfjii5g8eTLeeustNG7cGEDNjf+0adMwY8YMdujUUatx48bWjlqdO3dmh44doaGhSE9PR1xcHHbu3ImkpCQANQ/HN5vNmnXMnDkT8fHxSEpKQkxMjPXlHhkZGYiMjHS6DkfCnVIOwsvLC5mZmYiIiNA7RZkWVTrq16+PxYsXY+TIkexQqCMiIgKLFy9Gjx49bNY/++wzTJo0Cd9++61TdajUsm7dOgwdOhQeHh6aXI8df66qqgqenp7IyclBu3bt2KFIh4pKS0uxadMmnDx5EtOmTUODBg1w6NAhNGrUCM2aNXOqjrVr12LOnDkYNWrUNXcG9+vXjx06dKiyu5AddLvYtGkT4uPjYbFYEBMTg4yMDAA1v2jeu3cv/vGPf2jWosLOMZU6HAV3SjmIsLAwXLp0Se8MAOq0qNLh4eGBrl276p3BDgC///679c/z5s3DxIkTMWvWLERFRQEADhw4gDlz5mD+/PlO0aFaS60ePXqgpKTE+lrhrKwsvPvuu2jTpg3GjRvHDh063NzcEBQUpNmRNHb8e1QYwAA1x3169uwJX19f5OXlYezYsWjQoAG2bNmCgoICpKWlOVXH2LFjAQBz5syp8zUtj3iyw5YquwvZQbeLQYMGoVu3btYhTK2YmBjExcVp2qLCzjGVOhyGvo+0oltl586d0qVLF9m9e7ecO3fO+vaM2g9nbFGl45VXXpHExETNrseO6zMYDGI0Gq0fta8+vtbnztChWkutbt26SVpamojUPLDZZDJJdHS0+Pv7y+zZs9mhU0dKSor07dtXzp8/r9k12fHncnNzJSAgQEJDQ8XV1VVOnjwpIiLTp0+Xp556StOWmJgYmTZtmoiI+Pj4WFv27dsnd955p9N1EBER0Z/j8T0HUbtt8I/P5BERTX/7pFKLKh1xcXH47LPP0LBhQ7Rt27bOdvUtW7awQ6OOPXv23PT3Pvjggw7fAajVUqt+/fo4cOAAWrdujWXLluG9997Dvn37kJGRgfHjx+Onn35ihw4dtUc8qqqqcOedd8Lb29vm61od62CHrZ49e6Jjx4547bXXYDKZkJubi5CQEOzfvx/x8fHIy8vTpAMAfH19cejQIbRs2dKmJT8/H61bt8bly5edpkOVI57suDZVdheyg4iIx/ccxu7du/VOsFKlRZUOPz8/DBgwQO8MduA/G6o888wzmDNnDvz9/R2uQ7WWWlVVVdbnJ3366afW54yEhYWhqKjILtdkx59T5YgHO2x9/fXXWLVqVZ31Zs2aaf4GIA8PD5sjwbWOHz+OgIAAp+pQ5YgnO+pS5XgnO4iI/knXfVqkuaefflpKSkr0zhARdVpU6fjyyy/l8uXLemew4w9MJpP16Ac7ati7pXPnzvL888/L3r17pV69epKTkyMiIpmZmdKsWTO7XZcddDsKCAiQQ4cOiYjtUbWMjAxp3ry5pi0JCQnSv39/qaysFB8fH/npp58kPz9fIiMjZdKkSU7XocoRT3bYUuV4JzuIiGpwKOVknOnGlh3suBWu/gcaO2rYu2X37t3i5+cnRqNRRo0aZV1/4YUXJC4uzm7XZQfdjlQZwIiIlJaWSs+ePcXPz09cXFzkjjvuEDc3N3nggQekrKzM6Truvvtu8fHxEQ8PD7nrrrskMjLS5oMd+nSYzWb58ccfRcT2/8/y8vLEw8ODHTp1EJHz4vE9JyMKPUJMlRZ22GIH6a179+44d+4cfv/9d9SvX9+6Pm7cOHh5eVk/37dvHzp16mQ92sYO+3ZYLBYsXrwYGzduREFBASorK22+/uuvv9rluuy4sYULF2LQoEEIDAzEpUuX8OCDD6K4uBjR0dGYO3euJg21fH198cknn2Dfvn3Izc1FWVkZOnbsiJ49ezplhypHPNlhS4XjnewgIrqKnhMx0p4z7bZgBzvYYR+qtKiym85ZOmbMmCFNmjSRv/3tb1KvXj156aWXJCEhQRo2bChLly6123XZcXO+/PJLWb58ucyfP18++eQTza8vIrJu3bprHruuqKiQdevWOV0HqUmV3YXsICKqwaGUk1HlZlJEnRZ2sIMd/x5VWtihbUdISIh89NFH1mvVHvdYunSpDBs2zG7XZceNqTSAMRqNcubMmTrr586dE6PR6HQdpCZVjneyg4ioBo/vERER0Z8qLi5G+/btAQA+Pj747bffAACPPvooZsyYwQ6dOkaNGoU+ffogMDDQZv3ixYsYNWoURowYoVmLiMBgMNRZLywshK+vr9N1qHLEkx22VDneyQ4iohocShEp4lr/gNYDO2w9+eSTMJvNemco0wGo1ULaad68OYqKihAUFISWLVsiIyMDHTt2xNdff22351ix48+pMICJjIyEwWCAwWBATEwMXF3/9c9Li8WCU6dOoU+fPk7TUWv27NlISUnBlClT8OKLL2L69OnIy8tDeno6Zs6cyQ6dOtLS0jBkyBB07doVXbt2ta5XVlZiw4YNmg1y2UFEVINDKSej0s2kKi2qdIgiD/Z2po7S0lJkZWXh7NmzuHLlis3Xav8RtmLFCqfpUK2F1BIXF4ddu3bhvvvuQ2JiIp588kmsXbsWBQUFSEpKYofGHSoNYGofYJ2Tk4PY2Fj4+PhYv+bu7o7g4GAMHDjQaTpqvfPOO1izZg0eeeQRzJo1C8OGDUPLli3RoUMHHDhwABMnTmSHDh2q7C5kBxFRDYOocgdK/2c3czPpbC2qdADA2bNn8cMPPwAAWrduXef//NmhbceHH36I4cOHo6ysDGaz2WangcFg0OwYgSodqrXcDLPZjJycHISEhLBDh47MzExkZmaiVatWeOyxxzS5Jjv+Zfbs2db/nDJlynUHMO7u7nZvqbVu3ToMHTpU051iKnd4e3vj6NGjCAoKQpMmTbB9+3Z07NgRP/30EyIjI61HPtmhbYfRaMSZM2fqvFkuNzcXDz30kGb/X8cOIqIa3CnlIP7sZlLLAYwqLap0XLx4Ec888ww2bNgAi8UCAHBxccGQIUOwfPlyzY5XsMPWlClTMHr0aLzyyivw8vLS5Joqd6jWcjNU+Z2Ks3ZER0cjOjpa02uy41+Sk5MBAMHBwUoMYACgR48eKCkpQfPmzQEAWVlZePfdd9GmTRuMGzfO6TpUOeLJjhqq7C5kBxGRLaPeAXRr1N5MlpWVobS0FBcuXLB+aP0bDlVaVOkYM2YMvvrqK3z00UcoLS1FaWkpPvroIxw8eBD//d//zQ6dOn7++WdMnDhR9+GLKh0qtSQnJyM/P/9Pv+/ixYt23RXEjrreeustdO3aFU2bNrU2LVmyBNu2bbPrddlxfbUDmFpZWVl47rnnsHr1as0aasXHx2P37t0Aah4E37NnT2RlZWH69OmYM2eO03XUHvEEgMTERMyYMQOtWrXCiBEjMHr0aHZo3NG/f388/vjjEBHExsbi8ccft34MHToUq1atwttvv80OjTuIiKDHK//o1vPy8lLileQi6rSo1PHFF1/UWd+7d694eXmxQ6eOuLg4ee+99zS7nuodIuq0REREiIuLi/To0UPeeeeda77unh3ae/PNN8Xf319efvll8fT0tP58TU1Nle7du7NDp45u3bpJWlqaiIgUFRWJyWSS6Oho8ff3l9mzZ2vWISLi5+cnx44dExGRpUuXSpcuXUREZOfOndKiRQun6/ij/fv3y8KFC+WDDz7QrYEdIn//+991+znKDiKiunh8z0HExsbi4MGDuj/TRKUWVToaNmx4zSNpvr6+qF+/Pjt06njkkUcwbdo0fP/992jfvj3c3Nxsvt6vXz+n6lCpJScnB9nZ2UhNTcWkSZPw7LPPYujQoRg9ejTuvfdeTRrYUdfrr7+ONWvWoH///nj11Vet6506dcLUqVPZoVPHkSNH0LlzZwDAxo0b0b59e+zbtw8ZGRkYP368pm81q6qqsh7D+vTTT60/M8LCwlBUVOR0HX/krEdNVetQ5XgnO4iI/knvqRjdGikpKRIUFCTJycmyadMm2bZtm82HM7ao0rFq1Srp2bOnFBUVWdeKioqkd+/esnLlSnbo1GEwGK77YTQana5DtZZalZWVsnnzZnn00UfFzc1N2rdvL0uWLJHS0lJ2aNxRr149ycvLExERHx8f686g48ePS7169ex+fXZcm7e3t5w6dUpERB577DF59dVXRUQkPz9f0w4Rkc6dO8vzzz8ve/fulXr16klOTo6IiGRmZkqzZs2crkNEJC0tTbp06SJNmjSx/n1ZvHixpKens0OnDlV2F7KDiKgGh1IOQqWbSVVaVOm4++67xcfHR9zc3KRly5bSsmVLcXNzEx8fH4mMjLT5YId2HXR7qKiokA0bNkjv3r3F1dVVHnjgAQkNDRWTySQbNmxgh4Yd4eHh1hvHq4cwy5Yt0/R/r+ywpdIAZvfu3eLn5ydGo1FGjRplXX/hhRckLi7O6TpUOeLJDluqHO9kBxFRDR7fcxBXrlzRO8FKlRZVOvr37693AgB2XK2qqgqenp7IyclBu3btnL5DtRYA+Oabb5Camor169fDw8MDI0aMwPLlyxEaGgqg5ujUxIkTMWTIEHZo1DF58mQ8++yzuHz5MkQEWVlZWL9+PebNm4eUlBS7XZcdNzZ//nzExcVhwYIF+K//+i9EREQAAD744APrsT6tdO/eHefOncPvv/9ucxx73LhxNi9Q2LdvHzp16mS3N66p0qHKEU922FLleCc7iIj+SeehGN0ClZWV4uLiIt9++63eKcq0qNJB6mrRooV1RwE7aqjS0q5dO3F1dZW+ffvK1q1bpbq6us73lJSUiMFgYIeGHSIib7/9toSGhlp3nTZr1kxSUlLsfl123Fh1dbX8+uuvNmunTp2SM2fOWD//8ssvlXmYsclkUuJFJPbuUOWIJztsqbK7kB1ERDWMeg/F6P/Ozc0NQUFBsFgseqco06JKR63S0lKkpKTghRdewK+//goAOHToEH7++Wd26NQxffp0/L//9/+s19eLKh0qtQwePBh5eXnYvn07+vfvDxcXlzrf4+/vb/fdkOyoa/jw4Thx4gTKyspQXFyMwsJCJCQk2P267LgxFxeXOi+KCA4ORmBgoPXzhx9+WPOf9dcjInonALB/R4sWLZCTk1NnfceOHQgPD7frtdlxffPnz8eqVavQvXt3DBs2TLfdhewgIvonvadidGukpKRI37595fz583qnKNOiSkdubq4EBARIaGiouLq6Wn8zOH36dHnqqafYoVNH7bOtPDw85K677tLteVaqdKjWUuvKlSty5coVXa7NDnIkV+9M0ZsqLfbuWLNmjTRr1kw2bNgg3t7esn79enn55Zetf9YKO+pSZXchO4iI+Ewph/HGG2/gxx9/RNOmTXHnnXfC29vb5uuHDh1yuhZVOiZPnoyRI0fitddeg8lksq737dsX8fHxmjSwoy4Vnm0FqNMBqNWydu1aLF68GCdOnAAAtGrVCs899xzGjBnDDg07IiMjYTAYbup77fkzlR10OxozZgw8PT3x4osvory8HPHx8WjatCmWLl2KoUOHskOnDuD6uwuv9vDDDyMnJwchISHs0KiDiJwTh1IOQqWbSVVaVOn4+uuvsWrVqjrrzZo1Q3FxMTt06khOTtbsWjeiSgegTsvMmTOxaNEiJCYmIjo6GgCQmZmJpKQkFBQUYM6cOezQqEOVn6PsoNvV8OHDMXz4cJSXl6OsrMzmSCU79Ou4GeIkx0xvliodROR4OJRyEKrcTALqtKjS4eHhgd9//73O+vHjxxEQEMAOnTpIXStWrMCaNWswbNgw61q/fv3QoUMHJCYmajYMYsfN/xy1980KOxzLze42szctO7y8vGze/KcXdhARkWo4lCKys379+mHOnDnYuHEjgJp/BBcUFOD555/HwIED2aFTh8ViweLFi7Fx40YUFBSgsrLS5utaPexblQ6VWqqqqtCpU6c66/fccw+qq6s1aWBHXQsWLMC0adPqrFssFjz55JNYv349O3TouFmqDIIAdYZ29uhQ5YgnO4iI6HbBoZSDUOVmUqUWVToWLlyIQYMGITAwEJcuXcKDDz6I4uJiREdHY+7cuZo0sKOu2bNnIyUlBVOmTMGLL76I6dOnIy8vD+np6Zg5c6bTdajU8tRTT2HFihVYtGiRzfrq1asxfPhwdujUsWDBAjRo0MDm7XIWiwVDhw7FkSNH2KFTx83SYhCUnJyM0aNH484777zh9128eNFhO1Q54skOIiK6bej2iHW6pWbMmCFNmjSRv/3tb1KvXj156aWXJCEhQRo2bChLly51yhZVOmp9+eWXsnz5cpk/f7588sknml+fHbZCQkLko48+EpGaNzD9+OOPIiKydOlSGTZsmNN16N2SlJRk/UhMTBSTySRt27aVhIQESUhIkHbt2onZbJYJEyawQ8OOq2VlZYmfn5+8//77IiJSVVUlcXFxEh4eLkVFRezQqWPmzJmSl5en2fVuJCIiQlxcXKRHjx7yzjvv6PamLlU6bkSVt2iy4/pMJpMSb4lkBxE5Og6lHARvbNXtWLdu3TX/QVxRUSHr1q1jh04dXl5ekp+fLyIijRs3lm+++UZERE6ePClms9npOvRu6d69+019PPTQQ+zQsOOPdu3aJSaTSbZt2yb9+vWTNm3aSHFxsaYN7LCl2gDm0KFDkpiYKP7+/uLn5yfjx4+XrKwsp+x47bXXrrleXV0tQ4cOZYdOHTfLx8dHiSEMO4jI0XEo5SB4Y6tuh9FolDNnztRZP3funBiNRnbo1HHXXXfJgQMHRESka9euMm/ePBER2bBhgwQEBDhdh2otN+P06dNisVj0znC6jq1bt4qrq6u0b99eSkpK7H49dvw5FQYwf1RZWSmbN2+WRx99VNzc3KR9+/ayZMkSKS0tdZqOgIAASUlJsVmrrq6WQYMGSVhYmN2vz45rU2V3ITuIiGrwmVIOonnz5igqKkJQUBBatmyJjIwMdOzYEV9//TU8PDycskWVDhG55kM+CwsL4evryw6dOuLi4rBr1y7cd999SExMxJNPPom1a9eioKAASUlJTtehWsvNaNOmDXJychASEsIOO3UMGDDgmusBAQHw8/PDuHHjrGtbtmy5Zddlx78nMjISkZGRWLhwIT788EOkpqaia9euCAsLQ0JCAkaOHKnpz1eg5md9VVUVKisrISKoX78+3njjDcyYMQNr1qzBkCFDHL5j+/bt6N27N3x9fTFo0CBUV1dj8ODBOHbsGHbv3m2367LjxrZt24a5c+fiwQcfREJCAgYOHKj5v5XZQUT0LxxKOQiVbiZVadG7o/aNMwaDATExMXB1/df/3CwWC06dOoU+ffqwQ+OOWq+++qr1z0OGDEFQUBAyMzPRqlUrPPbYY07XoVrLzRAHfoPXf8IeHdcbZMTGxt7ya7Hj/06FQdA333yD1NRUrF+/Hh4eHhgxYgSWL1+O0NBQAMDrr7+OiRMn2r1FhY57770XmzdvRv/+/eHu7o61a9fixx9/xO7du9GoUSO7XZcdN5aTk4Ps7GykpqZi0qRJePbZZzF06FCMHj0a9957Lzt06iAi52UQVf41TbdUZmamMjeTqrRo3TF79mzrf06ZMgU+Pj7Wr7m7uyM4OBgDBw6Eu7s7OzTsIMdhMpmQm5ur+w4lZ+m4dOkSrly5Am9vbwCwvpkxPDxc06EMO+q61gBmzJgxNgOYl19+GWfOnLFrR/v27XHs2DH07t0bY8eOxWOPPQYXFxeb7zl37hwCAwNx5coVh++olZ6ejieeeALh4eH47LPP4O/vb/drsuPmVFVVWXcX7ty5U7fdhewgImfGoRSRna1btw5Dhw7VfSs0O+p66623sHLlSpw6dQqZmZm48847sWTJErRo0QKPP/6403Wo1vJnnGUYpEpH7969MWDAAIwfPx6lpaUICwuDm5sbzp07h0WLFuHpp5+2y3XZcWMqDWBeeukljB49Gs2aNbPrdVTuuN4RzwMHDiA0NNRmAKPHUVNn7bieyspKbN26Ff/zP/+Dzz77DF26dMEvv/yCM2fOaHrMlB1E5MyMegfQrfPWW2+ha9euaNq0KfLz8wEAS5YswbZt25y2RYWOHj16oKSkxPp5VlYWnnvuOaxevVqzBnbUtWLFCkyePBl9+/ZFaWkpLBYLAMDPzw9Llixxug7VWkg9hw4dwv333w8A2LRpExo1aoT8/HykpaVh2bJl7NCpY/DgwcjLy8P27dvRv3//OgMpAPD399dkR9CMGTOsgyCpeZmO3a+pWoevr+81P2JjY9GyZUubNXZo1/FH33zzDSZMmIAmTZogKSkJkZGROHr0KPbs2YMTJ05g7ty5mDhxIjs07iAiJ6Xtc9XJXt58803x9/eXl19+WTw9Pa2vbE1NTZXu3bs7ZYsqHd26dZO0tDQRESkqKhKTySTR0dHi7+8vs2fPZodOHeHh4bJ161YRsX3N8bfffisNGzZ0ug7VWm6GyWRS4vXUztLh6elpfaPpE088IbNmzRIRkYKCAvH09LTbddlx865cuSJXrlzR5dq1UlJSpG3btuLu7i7u7u7Stm1bWbNmjVN2lJeXS1lZmfXzU6dOyeLFi2XHjh3s0LGjXbt24urqKn379pWtW7dKdXV1ne8pKSkRg8HADg07iMh5cSjlIFS6mVSlRZUOPz8/OXbsmIiILF26VLp06SIiIjt37pQWLVqwQ6eOevXqWV+BfPXfj+PHj0u9evWcrkO1lptxdSM77N/Rvn17Wbp0qRQUFIjZbJb9+/eLiMjBgwelUaNGdrsuO/6cCgMYEZEZM2aIt7e3/PWvf5Vt27bJtm3b5K9//av4+PjIjBkznK6jV69esmLFChERuXDhgjRq1EiaN28u9erVkzfffJMdOnXMmTNHCgsLNbseO4iIboxDKQeh0s2kKi2qdHh7e8upU6dEROSxxx6TV199VURE8vPz2aFjR3h4uKSnp4uI7d+PZcuWSWRkpNN1qNZSq6CgQAoKCq77tWv9Rpcd9vH++++Lm5ubGI1G6dWrl3X9lVdekT59+tjtuuy4MVUGMCIi/v7+8u6779ZZf/fddzX9ZZAqHQ0bNpQjR46IiMiaNWukQ4cOYrFYZOPGjRIWFsYOnTqupsLuQnYQkbPjUMpBqHQzqUqLKh2dO3eW559/Xvbu3Sv16tWTnJwcERHJzMyUZs2asUOnjjVr1kizZs1kw4YN4u3tLevXr5eXX37Z+mdn61CppaqqSl588UUxm81iNBrFaDSK2WyW6dOnS2VlJTt06hCpOXJ76NAhsVgs1rWvvvpKjh49yg6dOlQZwIiI+Pr6yvHjx+us//DDD+Lr6+t0Haoc8WRHXarsLmQHERGHUg5DlZtJlVpU6di9e7f4+fmJ0WiUUaNGWddfeOEFiYuLY4dOHSIib7/9toSGhorBYBCDwSDNmjWTlJQUTRtU6lClZfz48RIYGCgrV66U3Nxcyc3NlZUrV0rjxo1l/Pjx7NCpg9SkygBGRGTChAmSlJRUZ33KlCnyzDPPOF2HKkc82WFLld2F7CAiqsGhlANR4WZStRZVOqqrq+XXX3+1WTt16pScOXPG+vmXX34ply9fZoeGHbX+93//1+baelGlQ0TfFrPZLB9//HGd9e3bt4vZbGaHTh2kJr0HMElJSdaPxMREMZlM0rZtW0lISJCEhARp166dmM1mmTBhglN0XE2VI57ssKXK7kJ2EBHVMIjo9L5espvy8nKUlZUhMDBQ7xRlWlTpuBGz2YycnByEhISwQ6EO0l5gYCD27NmD8PBwm/WjR4/igQceQElJCTt06CB1TJ482frn6upq/P3vf0dQUBCioqIAAF999RUKCgowYsQIvP7663Zteeihh27q+wwGAz777DOH7/ij4uJiFBUVISIiAkajEQCQlZUFs9mMsLAwdujQ4efnh6+//hqtWrWyWT9+/Dg6d+6M0tJSdujQQUTOi0MpIkWYTCbk5ubqPoRx5I7IyEgYDIab+t5Dhw7dsuuq2qFaS605c+bg2LFjSE1NhYeHBwCgoqICCQkJaNWqFZKTk9mhQwepQ9UBzM0qLCxE06ZNrUMJZ+8gbSUmJsLNzQ2LFi2yWZ86dSouXbqE5cuXs0OHDiJyXq56B9B/TqWbSVVaVOkgNfXv31/vBADqdABqtdTKzs7Grl270Lx5c0RERAAAcnNzUVlZiZiYGAwYMMD6vVu2bGGHRh2kjt27d//b/x2VBjBt2rRRYiesKh1kf1fvLjQYDEhJSUFGRsY1dxeyQ7sOIiKAQ6nbmko3k6q0qNJBarrZHSX23kCqSgegVkstPz8/DBw40Gbtjjvu0Oz67CBHpNIARpVN+qp0kP1lZ2fbfH7PPfcAAE6ePAkA8Pf3h7+/P7777jt2aNhBRATw+J5TEJGb3j1kb6q0qNJxNUc+Nqdix4IFCzBt2rQ66xaLBU8++STWr19vl+uq2qFaCxHdWqr8bFepRZUOUpMquwvZQUSOjj9VHMSCBQuuuW6xWBAfH++ULap03CxVhmTO0rFgwQKsXbvWZs1isWDo0KHIycmx67VV7FCtpbq6Gp9++ilWrVqFixcvAgB++eUXlJWVsUPHDiIiZ9GmTRvk5eXpncEOInJ4PL7nIBYsWIAGDRogISHBulZ7M3nkyBGnbFGl42apsmnRWTq2b9+O3r17w9fXF4MGDUJ1dTUGDx6MY8eO/UfPa7ndO1Rqyc/PR58+fVBQUICKigr06tULJpMJ8+fPR0VFBVauXMkOHTqIiJyJs/x76Gap0kFEjoc7pRzE9u3bMXXqVGzatAlAzW/Vn3jiCXz33Xe63Niq0KJKR3JyMvLz8//0+y5evGjXIwTssHXvvfdi8+bNGD16ND744AMMHDgQP/zwA3bv3o3GjRvb7bqqdqjUMmnSJHTq1AkXLlyAp6endT0uLg67du1ih04dRLeKs+zIJSIiopsg5DB27dolJpNJtm3bJv369ZM2bdpIcXGxU7eo0BERESEuLi7So0cPeeedd+Ty5cuaXp8dN7Z161ZxdXWV9u3bS0lJidN3qNDSoEEDOXbsmIiI+Pj4yMmTJ0VE5NSpU+Lp6ckOnTro9mYymax/d/R29d9jdpCqVPn7wQ4icnQ8vudAevTogbS0NAwcOBDh4eHYs2cP/P39nbpFhY6cnBxkZ2cjNTUVkyZNwrPPPouhQ4di9OjRuPfee9mhYceAAQOuuR4QEAA/Pz+MGzfOurZlyxaH71CtpdaVK1dgsVjqrBcWFsJkMmnSwA5yNKLx0ZvTp08DuPabIr///ns0bdrUqTqIiIjo2vj2vdvY9W4mDxw4gNDQUJvhi143tlq3qNJxPVVVVfjwww+RmpqKnTt3IiwsDAkJCRg5ciR8fX3ZYeeOUaNG3fT3pqam3vLrq9YBqNVSa8iQIfD19cXq1athMplw+PBhBAQE4PHHH0dQUBA7dOog9d1oAHP69Gk0bdoULi4udrt+dXU1Zs+ejWXLllkfwu/j44PExEQkJyfDzc3NbtdWsYNub2azGTk5Obq/nZEdROTouFPqNna9m/bY2FiNS9RpUaXjekQEVVVVqKyshIigfv36eOONNzBjxgysWbMGQ4YMYYcdO66+eb906RKuXLkCb29vAEBeXh7S09MRHh5u978vqnSo1lJr4cKFiI2NRZs2bXD58mXEx8fjxIkT8Pf3x/r169mhUwep6WYHMNcaVN1qiYmJ2LJlC1577TVER0cDADIzMzFr1iycP38eK1assHuDSh10e1Pl9/bsICKHp8+pQbrVysvLpayszPr5qVOnZPHixbJjxw6nbVGlQ0Tk4MGD8uyzz0qDBg2kSZMm8vzzz8uJEyesX1+2bJkEBgayQ8OOXr16yYoVK0RE5MKFC9KoUSNp3ry51KtXT9588027XlvFDtVaqqqq5O2335Zp06bJ008/LWvWrJHy8nJNG9hBt4Px48dLYGCgrFy5UnJzcyU3N1dWrlwpjRs3lvHjx2vaYjab5eOPP66zvn37djGbzU7XQeorKCiQgoKC636turqaHTp0EJFz4VDKQah0M6lKiyod7dq1E1dXV+nbt69s3br1mv+HXlJSIgaDgR0adjRs2FCOHDkiIiJr1qyRDh06iMVikY0bN0pYWJhdr61ih0ote/bskaqqqjrrVVVVsmfPHnbo1EFqUmkAExAQIN9//32d9e+//178/f2droPUVFVVJS+++KKYzWYxGo1iNBrFbDbL9OnTpbKykh06dRCR8+JQykGocjOpUosqHXPmzJHCwkLNrseOm+Pp6Sn5+fkiIvLEE0/IrFmzRKTmN4FavtFMlQ6VWoxGo5w5c6bO+rlz58RoNLJDpw5Sk0oDmNmzZ8uwYcNs3qp6+fJlGT58uPXniTN1kJpU2V3IDiKiGnymlIMoLy+3voUpIyMDAwYMgNFoRFRUFPLz852yRZWOGTNmWP8s/zyPbzAYNLs+O64tNDQU6enpiIuLw86dO5GUlAQAOHv2LMxms9N1qNQiItf8O3H+/Hnr867YoX0HqWnChAl46aWXkJqaCg8PDwBARUUF5s6diwkTJmjakp2djV27dqF58+aIiIgAAOTm5qKyshIxMTE2LyOx58tGVOkgNb377rvYsGEDHn74Yetahw4dcMcdd2DYsGGaPXOMHURENTiUchCq3Eyq1KJKBwCsXbsWixcvxokTJwAArVq1wnPPPYcxY8awQ6eOmTNnIj4+HklJSYiJibE+DDcjIwORkZFO16FCS+2NosFgwMiRI6032ABgsVhw+PBhdOnShR0ad5DaVBrA+Pn5YeDAgTZrWjxgXdUOUpOHhweCg4PrrLdo0QLu7u7s0KmDiJwXh1IOQu+bSRVbVOpYtGgREhMTbd4ClJSUhIKCAsyZM4cdOnQMGjQI3bp1Q1FRkfVGDgBiYmIQFxenSYNKHSq01L49U0RgMpng6elp/Zq7uzuioqIwduxYdmjcQWpTaQBz9ds89aRKB6lJld2F7CAiqmEQ4fs9HUVxcbH1ZtJoNAIAsrKyYDabERYW5pQtKnQEBARg2bJlGDZsmM36+vXrkZiYiHPnzrFDhw5S11/+8hfMmjULXl5eAIC8vDykp6cjPDwcsbGx7NCpg+hmVFdX4/PPP8fJkycRHx8Pk8mEX375BWazGT4+Pk7XQeqJi4vDrl274OHhcc3dhVez5+5CdhAR1eBOKQfSuHFjNG7c2Gatc+fOTt2iQkdVVRU6depUZ/2ee+5BdXU1O3TqIHVlZ2cjLS0N48ePR2lpKaKiouDm5oZz585h0aJFePrpp9mhQwepS5UBTH5+Pvr06YOCggJUVFSgV69eMJlMmD9/PioqKrBy5Uqn6iA1qbK7kB1ERDW4U4rIzhITE+Hm5oZFixbZrE+dOhWXLl3C8uXL2aFDB6nL398fe/bsQdu2bZGSkoLXX38d2dnZ2Lx5M2bOnImjR4+yQ4cOUtMfBzDHjx9HSEgIJk2apPkApn///jCZTFi7di0aNmyI3NxchISE4PPPP8fYsWOtzxF0lg4iIiL6c9wpRWQHkydPtv7ZYDAgJSUFGRkZiIqKAgB89dVXKCgowIgRI9ihYQfdHlR5cyY76HYwadIkdOrUCbm5uWjYsKF1PS4uTvNnjn3xxRfYv39/nYcjBwcH4+eff3a6DlKXKrsL2UFExKEUkV1kZ2fbfH7PPfcAAE6ePAmgZueDv78/vvvuO3Zo2EG3B1XenMkOuh2oNIC5cuUKLBZLnfXCwkLrYNWZOkhNqhzvZAcRUQ0OpYjsYPfu3f/2f6ewsBBNmza1PpCdHbe+g24PKr05kx2kOpUGML1798aSJUuwevVqADU7Y8vKypCcnIy+ffs6XQepSZXdhewgIqrBZ0oRKcJsNiMnJwchISHsUKiD9KHCmzPZQbeDIUOGwNfXF6tXr4bJZMLhw4cREBCAxx9/HEFBQUhNTdWspbCwELGxsRARnDhxAp06dcKJEyfg7++PvXv3IjAw0Kk6SE0NGzbE/v370bp1a5hMJuszx/Ly8tCmTRuUl5ezQ4cOInJe3ClFpAhV5sPsIBWo8OZMdtDtYOHChYiNjUWbNm1w+fJlxMfHWwcw69ev17SlefPmyM3NxXvvvYfc3FyUlZUhISEBw4cPh6enp9N1kJpU2V3IDiKiGtwpRaSIq387xQ51OoiIVFddXW0zgOnYsaMuA5i9e/eiS5cucHW1/Z1ndXU19u/fjwceeMCpOkhNquwuZAcRUQ0OpYgUocoQhh1ERLcPlQYwLi4uKCoqqnM87vz58wgMDLzmbgxH7iA1qXK8kx1ERDV4fI+IiIjoNvXQQw9dcwDz22+/4aGHHtJ0ACMiMBgMddbPnz8Pb29vp+sgNalyvJMdREQ1OJQiUsS1/gGtB3YQEd0+VBjADBgwAEDNz+2RI0fCw8PD+jWLxYLDhw+jS5cuTtNBaqvdXTh8+HAMHz7cul5dXY29e/dqfsyUHUTk7DiUIlKEKidp2UFEpD6VBjC+vr4Aan5um0wmm90V7u7uiIqK0uTV8qp0kNpU2V3IDiKiGhxKEWno9OnTAIA77rijzte+//57NG3alB06dBAR3W5UGsDUPgg5ICAAs2bNgpeXFwAgLy8P6enpCA8Ph7+/v9N0kNpU2F3IDiKif+FQisjOqqurMXv2bCxbtgxlZWUAAB8fHyQmJiI5ORlubm4Arj2YYQcREV2LigOY7OxspKWlYfz48SgtLUVUVBTc3Nxw7tw5LFq0CE8//bRTdZBaVNldyA4iIlscShHZWWJiIrZs2YLXXnsN0dHRAIDMzEzMmjUL58+fx4oVK9ihQwcRkSNQaQCTnZ2NJUuWAAA2bdqERo0aITs7G5s3b8bMmTM1HUqp0EFqUWV3ITuIiGwZhA9uIbIrX19fbNiwAQ8//LDN+scff4xhw4bht99+Y4cOHUREjsDf3x979uxB27ZtkZKSgtdff91mAHP06FHNWry8vHDs2DEEBQVh8ODBaNu2LZKTk3H69Gm0bt0a5eXlTtVBavrLX/5y3d2FsbGx7NCpg4icl1HvACJH5+HhgeDg4DrrLVq0gLu7Ozt06iAicgTl5eUwmUwAgIyMDAwYMABGoxFRUVHIz8/XtCU0NBTp6ek4ffo0du7cid69ewMAzp49C7PZ7HQdpKba3YUArLsLFy5ciP79+2u6W5sdREQ1OJQisrMJEybgpZdeQkVFhXWtoqICc+fOxYQJE9ihUwcRkSNQaQAzc+ZMTJ06FcHBwbjvvvusR7QzMjIQGRnpdB2kpuzsbNx///0A/nW8Mz8/H2lpaVi2bBk7dOogIufF43tEdhYXF4ddu3bBw8MDERERAIDc3FxUVlYiJibG5nu3bNnCDo06iIgcwaZNmxAfHw+LxYKYmBhkZGQAAObNm4e9e/fiH//4h6Y9xcXFKCoqQkREBIzGmt99ZmVlwWw2IywszOk6SD2qHO9kBxFRDT7onMjO/Pz8MHDgQJs1Pd4sxw4iIsczaNAgdOvWzTqAqRUTE4O4uDjNexo3bozGjRvbrHXu3NlpO0g9tbsL4+LisHPnTiQlJQHQ75gpO4jI2XGnFBEREREROQVVdheyg4ioBodSRBqorq7G559/jpMnTyI+Ph4mkwm//PILzGYzfHx82KFTBxERETkfVY53soOIiEMpIrvLz89Hnz59UFBQgIqKChw/fhwhISGYNGkSKioqsHLlSnbo0EFERERERET64tv3iOxs0qRJ6NSpEy5cuABPT0/reu0Dv9mhTwcRERERERHpiw86J7KzL774Avv374e7u7vNenBwMH7++Wd26NRBRERERERE+uJOKSI7u3LlCiwWS531wsJCmEwmdujUQURERERERPriUIrIznr37o0lS5ZYPzcYDCgrK0NycjL69u3LDp06iIiIiIiISF980DmRnRUWFiI2NhYighMnTqBTp044ceIE/P39sXfvXgQGBrJDhw4iIiIiIiLSF4dSRBqorq7Ge++9h9zcXJSVlaFjx44YPny4zYO+2aF9BxEREREREemHQykiO9u7dy+6dOkCV1fb9wpUV1dj//79eOCBB9ihQwcRERERERHpi0MpIjtzcXFBUVFRnWNp58+fR2Bg4DUf+s0OIiIiIiIicnR80DmRnYkIDAZDnfXz58/D29ubHTp1EBERERERkb5c//xbiOg/MWDAAAA1b5cbOXIkPDw8rF+zWCw4fPgwunTpwg6NO4iIiIiIiEgNHEoR2Ymvry+Amp1BJpPJ5iHe7u7uiIqKwtixY9mhcQcRERERERGpgUMpIjtJTU0FAAQEBGDWrFnw8vICAOTl5SE9PR3h4eHw9/dnh8YdREREREREpAY+U4rIzrKzs5GWlgYAKC0tRVRUFBYuXIj+/ftjxYoV7NCpg4iIiIiIiPTFoRSRnWVnZ+P+++8HAGzatAmNGjVCfn4+0tLSsGzZMnbo1EFERERERET64lCKyM7Ky8thMpkAABkZGRgwYACMRiOioqKQn5/PDp06iIiIiIiISF8cShHZWWhoKNLT03H69Gns3LkTvXv3BgCcPXsWZrOZHTp1EBERERERkb44lCKys5kzZ2Lq1KkIDg7Gfffdh+joaAA1u4QiIyPZoVMHERERERER6csgIqJ3BJGjKy4uRlFRESIiImA01syCs7KyYDabERYWxg6dOoiIiIiIiEg/HEoREREREREREZHmeHyPiIiIiIiIiIg0x6EUERERERERERFpjkMpIiIiIiIiIiLSHIdSRERERERERESkOQ6liIiIiIiIiIhIcxxKERERERERERGR5jiUIiIiIiIiIiIizXEoRUREREREREREmvv/a2IOsPpmXEYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(all_results_recall.keys(), all_results_recall.values(), color='skyblue')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model Recall Scores')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6DdIMkLfZaps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "f5c8b0c6-5cd7-4e5b-c705-08c5278f1786"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(all_results_precision.keys(), all_results_precision.values(), color='skyblue')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model Precision Scores')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Koz98jVGyxuV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "629a2ecb-0424-4313-b5df-bfbce536f50b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    0\n",
              "sklearn_mnb_bow_enron1       0.940789\n",
              "sklearn_mnb_bow_enron2       0.943515\n",
              "sklearn_mnb_bow_enron4       0.974217\n",
              "step_by_step_mnb_bow_enron1  0.938596\n",
              "step_by_step_mnb_bow_enron2  0.943515\n",
              "step_by_step_mnb_bow_enron4  0.953959\n",
              "sklearn_bnb_bern_enron1      0.730263\n",
              "sklearn_bnb_bern_enron2      0.774059\n",
              "sklearn_bnb_bern_enron4      0.917127\n",
              "step_by_step_bern_enron1     0.734649\n",
              "step_by_step_bern_enron2     0.778243\n",
              "step_by_step_bern_enron4     0.917127\n",
              "sklearn_lr_bow_enron1        0.964912\n",
              "sklearn_lr_bow_enron2        0.951883\n",
              "sklearn_lr_bow_enron4        0.946593\n",
              "step_by_step_lr_bow_enron1   0.949561\n",
              "step_by_step_lr_bow_enron2   0.903766\n",
              "step_by_step_lr_bow_enron4   0.944751\n",
              "sklearn_lr_bern_enron1       0.964912\n",
              "sklearn_lr_bern_enron2       0.949791\n",
              "sklearn_lr_bern_enron4       0.955801\n",
              "step_by_step_lr_bern_enron1  0.927632\n",
              "step_by_step_lr_bern_enron2  0.884937\n",
              "step_by_step_lr_bern_enron4  0.941068\n",
              "sgd_bow_enron1               0.958333\n",
              "sgd_bow_enron2               0.949791\n",
              "sgd_bow_enron4               0.942910\n",
              "sgd_bern_enron1              0.962719\n",
              "sgd_bern_enron2              0.956067\n",
              "sgd_bern_enron4              0.942910"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcde7717-c69c-4dc2-9a99-2046d4cfcca3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sklearn_mnb_bow_enron1</th>\n",
              "      <td>0.940789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_mnb_bow_enron2</th>\n",
              "      <td>0.943515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_mnb_bow_enron4</th>\n",
              "      <td>0.974217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_mnb_bow_enron1</th>\n",
              "      <td>0.938596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_mnb_bow_enron2</th>\n",
              "      <td>0.943515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_mnb_bow_enron4</th>\n",
              "      <td>0.953959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_bnb_bern_enron1</th>\n",
              "      <td>0.730263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_bnb_bern_enron2</th>\n",
              "      <td>0.774059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_bnb_bern_enron4</th>\n",
              "      <td>0.917127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_bern_enron1</th>\n",
              "      <td>0.734649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_bern_enron2</th>\n",
              "      <td>0.778243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_bern_enron4</th>\n",
              "      <td>0.917127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_lr_bow_enron1</th>\n",
              "      <td>0.964912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_lr_bow_enron2</th>\n",
              "      <td>0.951883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_lr_bow_enron4</th>\n",
              "      <td>0.946593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_lr_bow_enron1</th>\n",
              "      <td>0.949561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_lr_bow_enron2</th>\n",
              "      <td>0.903766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_lr_bow_enron4</th>\n",
              "      <td>0.944751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_lr_bern_enron1</th>\n",
              "      <td>0.964912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_lr_bern_enron2</th>\n",
              "      <td>0.949791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sklearn_lr_bern_enron4</th>\n",
              "      <td>0.955801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_lr_bern_enron1</th>\n",
              "      <td>0.927632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_lr_bern_enron2</th>\n",
              "      <td>0.884937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>step_by_step_lr_bern_enron4</th>\n",
              "      <td>0.941068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sgd_bow_enron1</th>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sgd_bow_enron2</th>\n",
              "      <td>0.949791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sgd_bow_enron4</th>\n",
              "      <td>0.942910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sgd_bern_enron1</th>\n",
              "      <td>0.962719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sgd_bern_enron2</th>\n",
              "      <td>0.956067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sgd_bern_enron4</th>\n",
              "      <td>0.942910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcde7717-c69c-4dc2-9a99-2046d4cfcca3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dcde7717-c69c-4dc2-9a99-2046d4cfcca3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dcde7717-c69c-4dc2-9a99-2046d4cfcca3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec084108-2cfd-4718-8b81-1b072f31b062\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec084108-2cfd-4718-8b81-1b072f31b062')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec084108-2cfd-4718-8b81-1b072f31b062 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06837530731175473,\n        \"min\": 0.7302631578947368,\n        \"max\": 0.9742173112338858,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.7346491228070176,\n          0.9497907949790795,\n          0.9407894736842105\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "pd.DataFrame(all_results_accuracy, index=[0], columns=all_results_accuracy.keys()).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "iFMpoZ3mchl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3717323-8e1f-4721-ad45-045190b11b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Recall Combination: sklearn_bnb_bern_enron4\n",
            "Best F1 Combination: sklearn_mnb_bow_enron4\n",
            "Best Accuracy Combination: sklearn_mnb_bow_enron4\n",
            "Best Precision Combination: sklearn_mnb_bow_enron4\n"
          ]
        }
      ],
      "source": [
        "best_recall_combination = max(all_results_recall, key=all_results_recall.get)\n",
        "best_f1_combination = max(all_results_f1, key=all_results_f1.get)\n",
        "best_accuracy_combination = max(all_results_accuracy, key=all_results_accuracy.get)\n",
        "best_precision_combination = max(all_results_precision, key=all_results_precision.get)\n",
        "\n",
        "print(f\"Best Recall Combination: {best_recall_combination}\")\n",
        "print(f\"Best F1 Combination: {best_f1_combination}\")\n",
        "print(f\"Best Accuracy Combination: {best_accuracy_combination}\")\n",
        "print(f\"Best Precision Combination: {best_precision_combination}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXbYdlSaj0GN"
      },
      "source": [
        "## Extracting Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "0CeCQrwInlUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "a7a13e66-b68e-433a-cb85-699e863ca8f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Email  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } > loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or   \n",
              "2                                                                                                                                                                         Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 & 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg & e   \n",
              "4  Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy ' ; graham , travis e .\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> i now understand that the transport agreement between hpl and\\r\\n> midcon has been extended for delivery for day 8 .\\r\\n>\\r\\n> 28 midcon\\r\\n> 13 hpl\\r\\n>\\r\\n> swing it down on hpl .\\r\\n>\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> flow returning to the normal 10 spot purchase for day 8 .\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : monday , march 06 , 2000 4 : 09 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n> graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n> roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n> w . ; ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> increase in flow to a total of 28 for day 7 . this is a\\r\\n> total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n> outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n> be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n>\\r\\n>\\r\\n>   \n",
              "\n",
              "   Type  \n",
              "0   ham  \n",
              "1  spam  \n",
              "2   ham  \n",
              "3   ham  \n",
              "4   ham  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07d66478-67d7-41fd-b804-494e92a8912f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Email</th>\n",
              "      <th>Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } &gt; loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 &amp; 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg &amp; e</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy ' ; graham , travis e .\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; i now understand that the transport agreement between hpl and\\r\\n&gt; midcon has been extended for delivery for day 8 .\\r\\n&gt;\\r\\n&gt; 28 midcon\\r\\n&gt; 13 hpl\\r\\n&gt;\\r\\n&gt; swing it down on hpl .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; flow returning to the normal 10 spot purchase for day 8 .\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : monday , march 06 , 2000 4 : 09 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n&gt; graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n&gt; roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n&gt; w . ; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; increase in flow to a total of 28 for day 7 . this is a\\r\\n&gt; total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n&gt; outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n&gt; be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt;</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07d66478-67d7-41fd-b804-494e92a8912f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-07d66478-67d7-41fd-b804-494e92a8912f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-07d66478-67d7-41fd-b804-494e92a8912f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b754e475-bccc-480d-9507-36374ada9297\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b754e475-bccc-480d-9507-36374ada9297')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b754e475-bccc-480d-9507-36374ada9297 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "copy_of_enron1_train_df",
              "summary": "{\n  \"name\": \"copy_of_enron1_train_df\",\n  \"rows\": 450,\n  \"fields\": [\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 448,\n        \"samples\": [\n          \"Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : continuous _ gr = f 4 wth _ with _ the _ pill ? =\\r\\nour naturalgain + penis enlargement pills will expand ,\\r\\nlengthen and enlarge your penis 3 + inches .\\r\\n100 % satisfaction guaranteed ! or your money back !\\r\\n* gain 3 + full\\r\\ninches in length\\r\\n* increase\\r\\nyour penis width ( girth ) by 20 %\\r\\n* stop premature\\r\\nejaculation !\\r\\n* produce stronger ,\\r\\nrock hard erections\\r\\n* 100 % safe\\r\\nto take , with no side effects\\r\\n* fast priority\\r\\nfed - ex shipping worldwide\\r\\n* fedex tracking\\r\\nnumbers with all orders\\r\\n* doctor approved\\r\\nand recommended\\r\\n* no pumps !\\r\\nno surgery ! no exercises !\\r\\n* 100 % money\\r\\nback guarantee\\r\\n* free bottle\\r\\nof naturalgain + worth over $ 50\\r\\n* free \\\" male\\r\\nhelp e - book \\\" worth over $ 50\\r\\nclick here to enlarge your penis\\r\\n\",\n          \"Subject: hpl meter # 989648 tram / hpl - transtexas thompson\\r\\ndaren :\\r\\non 9 / 24 / 99 , the above meter recorded flow of 437 mmbtus . there was no deal\\r\\nat this meter the month prior or after . logistics needs either a deal to\\r\\nrecord these volumes which have flowed into hpl ' s pipeline or logistics needs\\r\\napproval to writeoff these volumes to unaccounted for gas . ( please print ,\\r\\nsign , and return the original to clem cernosek ) .\\r\\napproval to writeoff the volumes to unaccounted for gas loss\\r\\nor\\r\\ndeal / deal ticket # / customer ( buyer / seller )\\r\\nthanks , clem\",\n          \"Subject: february o & m reports\\r\\nfinal february budget numbers attached .\\r\\ny\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by yvette g connevey / corp / enron on\\r\\n03 / 24 / 2000 10 : 52 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nenron north america corp .\\r\\nfrom : jennifer blay @ ect 03 / 21 / 2000 04 : 15 pm\\r\\nto : brenda f herod / hou / ect @ ect\\r\\ncc : yvette g connevey / corp / enron @ enron\\r\\nsubject : february o & m reports\\r\\nattached are your final february o & m reports .\\r\\nplease call me with any questions .\\r\\nthank you\\r\\njennifer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "copy_of_enron1_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "28aGsx1PerFF"
      },
      "outputs": [],
      "source": [
        "# can add a regex for identifying wrong url\n",
        "def extract_features(email):\n",
        "\n",
        "    words = word_tokenize(email)\n",
        "\n",
        "    # number of words\n",
        "    word_count = len(email.split(\" \"))\n",
        "\n",
        "    # average word length\n",
        "    avg_word_length = np.mean([len(word) for word in words]) if word_count > 0 else 0\n",
        "\n",
        "    # part of speech tags\n",
        "    pos_tags = pos_tag(words)\n",
        "    num_nouns = len([word for word, pos in pos_tags if pos.startswith('NN')])\n",
        "    num_verbs = len([word for word, pos in pos_tags if pos.startswith('VB')])\n",
        "    num_adjectives = len([word for word, pos in pos_tags if pos.startswith('JJ')])\n",
        "\n",
        "    # number of special characters\n",
        "    sp_char = 0\n",
        "    for char in email:\n",
        "        if char in \"!@#$%^&*()[]\":\n",
        "            sp_char+=1\n",
        "\n",
        "    # number of uppercase characters\n",
        "    nums = 0\n",
        "    for char in email:\n",
        "        if char.isnumeric():\n",
        "            nums+=1\n",
        "\n",
        "    return [int(word_count), avg_word_length, int(num_nouns), int(num_verbs), int(num_adjectives), int(sp_char), int(nums)]\n",
        "\n",
        "\n",
        "feature_columns = ['word_count', 'avg_word_length', 'num_nouns', 'num_verbs', 'num_adjectives', 'sp_char', 'nums']\n",
        "\n",
        "# Apply the function and create a new dataframe with the extracted features\n",
        "copy_of_enron1_train_df[feature_columns] = copy_of_enron1_train_df['Email'].apply(lambda email: pd.Series(extract_features(email)))\n",
        "copy_of_enron2_train_df[feature_columns] = copy_of_enron2_train_df['Email'].apply(lambda email: pd.Series(extract_features(email)))\n",
        "copy_of_enron4_train_df[feature_columns] = copy_of_enron4_train_df['Email'].apply(lambda email: pd.Series(extract_features(email)))\n",
        "\n",
        "copy_of_enron1_test_df[feature_columns] = copy_of_enron1_test_df['Email'].apply(lambda email: pd.Series(extract_features(email)))\n",
        "copy_of_enron2_test_df[feature_columns] = copy_of_enron2_test_df['Email'].apply(lambda email: pd.Series(extract_features(email)))\n",
        "copy_of_enron4_test_df[feature_columns] = copy_of_enron4_test_df['Email'].apply(lambda email: pd.Series(extract_features(email)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "NPQ7mhkGdOzw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53ad7179-66c8-4007-b1ee-739515683826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Email  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } > loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or   \n",
              "2                                                                                                                                                                         Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 & 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg & e   \n",
              "4  Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy ' ; graham , travis e .\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> i now understand that the transport agreement between hpl and\\r\\n> midcon has been extended for delivery for day 8 .\\r\\n>\\r\\n> 28 midcon\\r\\n> 13 hpl\\r\\n>\\r\\n> swing it down on hpl .\\r\\n>\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> flow returning to the normal 10 spot purchase for day 8 .\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : monday , march 06 , 2000 4 : 09 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n> graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n> roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n> w . ; ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> increase in flow to a total of 28 for day 7 . this is a\\r\\n> total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n> outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n> be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n>\\r\\n>\\r\\n>   \n",
              "\n",
              "   Type  word_count  avg_word_length  num_nouns  num_verbs  num_adjectives  \\\n",
              "0   ham        50.0         4.103448       12.0       11.0             3.0   \n",
              "1  spam       147.0         2.327273       65.0       10.0            16.0   \n",
              "2   ham       674.0         2.725762      239.0       46.0            40.0   \n",
              "3   ham        45.0         3.660000       13.0        5.0             6.0   \n",
              "4   ham       697.0         2.662304      225.0       41.0            59.0   \n",
              "\n",
              "   sp_char   nums  \n",
              "0      0.0   37.0  \n",
              "1      9.0   43.0  \n",
              "2     17.0  126.0  \n",
              "3      1.0   23.0  \n",
              "4     11.0   96.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b945dd3-2b5a-40c3-a16f-c0dd5f646abc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Email</th>\n",
              "      <th>Type</th>\n",
              "      <th>word_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>num_nouns</th>\n",
              "      <th>num_verbs</th>\n",
              "      <th>num_adjectives</th>\n",
              "      <th>sp_char</th>\n",
              "      <th>nums</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren</td>\n",
              "      <td>ham</td>\n",
              "      <td>50.0</td>\n",
              "      <td>4.103448</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } &gt; loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or</td>\n",
              "      <td>spam</td>\n",
              "      <td>147.0</td>\n",
              "      <td>2.327273</td>\n",
              "      <td>65.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 &amp; 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .</td>\n",
              "      <td>ham</td>\n",
              "      <td>674.0</td>\n",
              "      <td>2.725762</td>\n",
              "      <td>239.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>126.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg &amp; e</td>\n",
              "      <td>ham</td>\n",
              "      <td>45.0</td>\n",
              "      <td>3.660000</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy ' ; graham , travis e .\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; i now understand that the transport agreement between hpl and\\r\\n&gt; midcon has been extended for delivery for day 8 .\\r\\n&gt;\\r\\n&gt; 28 midcon\\r\\n&gt; 13 hpl\\r\\n&gt;\\r\\n&gt; swing it down on hpl .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; flow returning to the normal 10 spot purchase for day 8 .\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : monday , march 06 , 2000 4 : 09 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n&gt; graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n&gt; roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n&gt; w . ; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; increase in flow to a total of 28 for day 7 . this is a\\r\\n&gt; total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n&gt; outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n&gt; be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt;</td>\n",
              "      <td>ham</td>\n",
              "      <td>697.0</td>\n",
              "      <td>2.662304</td>\n",
              "      <td>225.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>96.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b945dd3-2b5a-40c3-a16f-c0dd5f646abc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b945dd3-2b5a-40c3-a16f-c0dd5f646abc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b945dd3-2b5a-40c3-a16f-c0dd5f646abc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-81289af4-54bc-4721-a02c-ba5331860106\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-81289af4-54bc-4721-a02c-ba5331860106')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-81289af4-54bc-4721-a02c-ba5331860106 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "copy_of_enron1_train_df",
              "summary": "{\n  \"name\": \"copy_of_enron1_train_df\",\n  \"rows\": 450,\n  \"fields\": [\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 448,\n        \"samples\": [\n          \"Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : continuous _ gr = f 4 wth _ with _ the _ pill ? =\\r\\nour naturalgain + penis enlargement pills will expand ,\\r\\nlengthen and enlarge your penis 3 + inches .\\r\\n100 % satisfaction guaranteed ! or your money back !\\r\\n* gain 3 + full\\r\\ninches in length\\r\\n* increase\\r\\nyour penis width ( girth ) by 20 %\\r\\n* stop premature\\r\\nejaculation !\\r\\n* produce stronger ,\\r\\nrock hard erections\\r\\n* 100 % safe\\r\\nto take , with no side effects\\r\\n* fast priority\\r\\nfed - ex shipping worldwide\\r\\n* fedex tracking\\r\\nnumbers with all orders\\r\\n* doctor approved\\r\\nand recommended\\r\\n* no pumps !\\r\\nno surgery ! no exercises !\\r\\n* 100 % money\\r\\nback guarantee\\r\\n* free bottle\\r\\nof naturalgain + worth over $ 50\\r\\n* free \\\" male\\r\\nhelp e - book \\\" worth over $ 50\\r\\nclick here to enlarge your penis\\r\\n\",\n          \"Subject: hpl meter # 989648 tram / hpl - transtexas thompson\\r\\ndaren :\\r\\non 9 / 24 / 99 , the above meter recorded flow of 437 mmbtus . there was no deal\\r\\nat this meter the month prior or after . logistics needs either a deal to\\r\\nrecord these volumes which have flowed into hpl ' s pipeline or logistics needs\\r\\napproval to writeoff these volumes to unaccounted for gas . ( please print ,\\r\\nsign , and return the original to clem cernosek ) .\\r\\napproval to writeoff the volumes to unaccounted for gas loss\\r\\nor\\r\\ndeal / deal ticket # / customer ( buyer / seller )\\r\\nthanks , clem\",\n          \"Subject: february o & m reports\\r\\nfinal february budget numbers attached .\\r\\ny\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by yvette g connevey / corp / enron on\\r\\n03 / 24 / 2000 10 : 52 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nenron north america corp .\\r\\nfrom : jennifer blay @ ect 03 / 21 / 2000 04 : 15 pm\\r\\nto : brenda f herod / hou / ect @ ect\\r\\ncc : yvette g connevey / corp / enron @ enron\\r\\nsubject : february o & m reports\\r\\nattached are your final february o & m reports .\\r\\nplease call me with any questions .\\r\\nthank you\\r\\njennifer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 318.1948228379448,\n        \"min\": 5.0,\n        \"max\": 4422.0,\n        \"num_unique_values\": 271,\n        \"samples\": [\n          695.0,\n          354.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_word_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7491967312497194,\n        \"min\": 1.106207928197457,\n        \"max\": 7.26530612244898,\n        \"num_unique_values\": 410,\n        \"samples\": [\n          3.56,\n          3.56140350877193\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_nouns\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 139.63355104374048,\n        \"min\": 1.0,\n        \"max\": 2391.0,\n        \"num_unique_values\": 151,\n        \"samples\": [\n          100.0,\n          417.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_verbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33.56906519527514,\n        \"min\": 0.0,\n        \"max\": 447.0,\n        \"num_unique_values\": 78,\n        \"samples\": [\n          30.0,\n          11.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_adjectives\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.672861261513248,\n        \"min\": 0.0,\n        \"max\": 257.0,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          19.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sp_char\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.554699155531953,\n        \"min\": 0.0,\n        \"max\": 188.0,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          13.0,\n          21.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nums\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.93334740794448,\n        \"min\": 0.0,\n        \"max\": 1069.0,\n        \"num_unique_values\": 117,\n        \"samples\": [\n          17.0,\n          96.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "copy_of_enron1_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4f-c4EvSke9Q"
      },
      "outputs": [],
      "source": [
        "quantiles = [0,0.25,0.5,0.75,1.0]\n",
        "dfs = [copy_of_enron1_train_df, copy_of_enron1_test_df, copy_of_enron2_train_df, copy_of_enron2_test_df, copy_of_enron4_train_df, copy_of_enron4_test_df]\n",
        "def bin(col, df, quantiles):\n",
        "    # Apply qcut and drop duplicate bin edges, without labels for now\n",
        "    binned_feature = pd.qcut(df[col], q=quantiles, duplicates='drop')\n",
        "\n",
        "    # Now generate dynamic labels based on the actual number of bins created\n",
        "    n_bins = binned_feature.cat.categories.size  # Get the number of bins\n",
        "    labels = ['Low', 'Medium', 'High', 'Very High'][:n_bins]  # Adjust labels accordingly\n",
        "\n",
        "    # Reapply qcut with labels\n",
        "    df[col] = pd.qcut(df[col], q=quantiles, labels=labels, duplicates='drop')\n",
        "\n",
        "for df in dfs:\n",
        "    for feature in feature_columns:\n",
        "        bin(feature, df, quantiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Vzda14YJl253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0aa689a0-155a-4f37-e1ca-ee097c1e9804"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Email  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } > loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or   \n",
              "2                                                                                                                                                                           Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 & 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg & e   \n",
              "4    Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy ' ; graham , travis e .\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> i now understand that the transport agreement between hpl and\\r\\n> midcon has been extended for delivery for day 8 .\\r\\n>\\r\\n> 28 midcon\\r\\n> 13 hpl\\r\\n>\\r\\n> swing it down on hpl .\\r\\n>\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> flow returning to the normal 10 spot purchase for day 8 .\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : monday , march 06 , 2000 4 : 09 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n> graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n> roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n> w . ; ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> increase in flow to a total of 28 for day 7 . this is a\\r\\n> total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n> outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n> be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n>\\r\\n>\\r\\n>   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
              "445                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Subject: tenaska iv 4 / 01\\r\\nplease change the demand fee on deal 384258 for april from $ 2 , 862 , 531 . 95 to $ 2 , 855 , 089 . 73 . we need to return some money to tenaska iv . the transport expenses are less than what was etimated on the spreadsheet .\\r\\nmegan   \n",
              "446                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Subject: credit watch list - - week of 11 / 5 / 01\\r\\nattached is a revised credit watch listing for the week of 11 / 05 / 01 . there are no updates from last week ' s list .\\r\\nif there are any personnel in your group that were not included in this distribution , please insure that they receive a copy of this report .\\r\\nto add additional people to this distribution , or if this report has been sent to you in error , please contact veronica espinoza at x 6 - 6002 .\\r\\nfor other questions , please contact jason r . williams at x 5 - 3923 , veronica espinoza at x 6 - 6002 or darren vanek at x 3 - 1436 .   \n",
              "447                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Subject: proccess credit cards online , hispanic\\r\\nknapp , {\\r\\nneed affordable but reliable web hosting for only $ 5 / month ?\\r\\n- 800 mb disk space\\r\\n- unlimited email accounts\\r\\n- free shopping cart . . . and much more !\\r\\nplease contact us to take advantage of this new offer at\\r\\nhttp : / / viewhostdeals . com\\r\\nbelvidere\\r\\n   \n",
              "448                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Subject: hpl nom for march 28 , 2000\\r\\n( see attached file : hplo 328 . xls )\\r\\n- hplo 328 . xls   \n",
              "449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Subject: ? ? ? , ? > ? ? ? ? ? ?\\r\\n? ? ?? ? ?  ? ? r    ????? ? ? ? r ? ? . ? ? ? r ? ? . . . . . . .\\r\\n ????? ? ? ? ? 100 ? ? ?? ? y ? ? ? r ? ? www . 7 english 7 . co . kr ? ??  ?? ? ? ? ?   ?    ?? ??? ? ?? ? ??y ? r?  ?? ? ???  ??? ? ? ? ? .\\r\\n ? 1   ???? ? ? ? ? ? ? ? r ? ? 80 % 25   ? ? -  ? ? ? ? ? ? ?  ? ??  ? ? ?   ?   ? ?? 98 % ?? ????  ?  ?? $ ? ?  ?? ? ?? ? ? ? ? .  ? 2    ????? ? ? ? ? ? ? ? ? , ? r ? ??? ?? ? ? - ? ? ? ? ??  ?\\r\\n? ??? ? r ? r ? ? ?? ? ??? ?  ? ? ? ??? ? ? ? ? ? ?  ? ?? .\\r\\n? ? ? ?   ?  ? ?  ? ?  ? ? ? r  ?? ? ?? ? ? ? r ? ??? 1 . ? ? ? ???  ??  ?  ??   ??  ? 2 . ? ??     ?    ? ?? ? ??  ??????  ? 3 . ??   ?? ? ? ???? ? ? ? ? ? ?  ??  ? ? ??  ??  ? ? .\\r\\n? ? ? ? ?   ?? $ ?? ? ?  ?   ? ?   ? ? ?   ? ? ???? ??  ?  ? ? ? .  ? ? r  ? ? ? ? ?   ?    ?? ?  ? ? ???? ?  ??  ?  ? ? ? ? ? ? ? ? ? . - ? ? ? ?   ?    ??  ? ? ???? ? www . 7 english 7 . co . kr -\\r\\n?? ? ? ? ???  ??  ? ?  ? ? ? ? ?? ? ?? ? ??? ? ? ? ? . ? ?  ??   ?????  ? ?  ?  ? ? yoonsuk 37 @ netian . com ? ?  ? ? ?  ?? ?? ? ? ?   ????? ? ? ?  ? ?  ?  ??? ??  ? ?  ? ?   ??  ? ? ? ? ??  ???? ? ? ?? ? ?  ? ? ? ? r?? ? ?  ?  ? ? ? ? ? . ? ?  ?  ?  ? ?? ? ?  ? ?  ? ? ? ?? ? ?? ? ??? ? ? ? ?\\r\\n[ ? ?  ?  ??? ]\\r\\n[ rejection ]\\r\\n? ????? ? ? ? ? ? ????? ? ? ? ? ? ? ??  ?? ?  ?  ?  ? ? ? ?  ? ? ? ? . [ ? ?  ?\\r\\n ??? ] ??\\r\\n??  ? ? ? ?? ?? ? ?  ? ? ?   ?? ? ?  ?  ? ? ? ? ? .\\r\\n( ?  ? ????? ? ?  ??? ? ??? ? ? ? ? ? r??  ?? ? ?? ? ??? ?  ?y  ? ? ? ?  ? ??   ? ? r?? ? ? ? ? ? ? ? ? . )\\r\\nplease click\\r\\n[ ? ??? ???  ]\\r\\n ????? ?? ? ? ? ? bbs , community ,  ?  ??? ? ? ????  ?  ???  ?   ? ? ?? ? ? ?  ?   ? ? ??? ? ? ? ? ? ?\\r\\n? ??? ?? ? ? ?? ? ???   ??? $ ? ? ? ? ??? ? ? ? ? ? ??  ? ???? ? ?  ? ? ? ? ? .\\r\\n? ????  ? : http : / / www . kdnuggets . com / news / 97 / nl 9 . html ? ?  ?   ? ? ?? ? ?\\r\\n? ???  ?   : 2004 - 08 - 30 ? ??? 7 : 40 : 42 , ? ? ? ?  ?   : 2004 - 08 - 30 ? ??? 1 : 25 : 52\\r\\n   \n",
              "\n",
              "     Type word_count avg_word_length  num_nouns  num_verbs num_adjectives  \\\n",
              "0     ham     Medium       Very High        Low     Medium            Low   \n",
              "1    spam       High             Low       High     Medium           High   \n",
              "2     ham  Very High             Low  Very High  Very High      Very High   \n",
              "3     ham     Medium            High        Low        Low         Medium   \n",
              "4     ham  Very High             Low  Very High  Very High      Very High   \n",
              "..    ...        ...             ...        ...        ...            ...   \n",
              "445   ham     Medium          Medium        Low     Medium            Low   \n",
              "446   ham       High            High     Medium       High         Medium   \n",
              "447  spam     Medium       Very High     Medium        Low         Medium   \n",
              "448   ham        Low             Low        Low        Low            Low   \n",
              "449  spam  Very High             Low       High     Medium         Medium   \n",
              "\n",
              "       sp_char       nums  \n",
              "0          Low       High  \n",
              "1    Very High       High  \n",
              "2    Very High  Very High  \n",
              "3          Low       High  \n",
              "4    Very High  Very High  \n",
              "..         ...        ...  \n",
              "445     Medium       High  \n",
              "446        Low       High  \n",
              "447     Medium        Low  \n",
              "448     Medium     Medium  \n",
              "449  Very High  Very High  \n",
              "\n",
              "[450 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56b34c4f-4900-4e1c-a2da-496a8ffd04c3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Email</th>\n",
              "      <th>Type</th>\n",
              "      <th>word_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>num_nouns</th>\n",
              "      <th>num_verbs</th>\n",
              "      <th>num_adjectives</th>\n",
              "      <th>sp_char</th>\n",
              "      <th>nums</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren</td>\n",
              "      <td>ham</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } &gt; loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or</td>\n",
              "      <td>spam</td>\n",
              "      <td>High</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 &amp; 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .</td>\n",
              "      <td>ham</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg &amp; e</td>\n",
              "      <td>ham</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy ' ; graham , travis e .\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; i now understand that the transport agreement between hpl and\\r\\n&gt; midcon has been extended for delivery for day 8 .\\r\\n&gt;\\r\\n&gt; 28 midcon\\r\\n&gt; 13 hpl\\r\\n&gt;\\r\\n&gt; swing it down on hpl .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; flow returning to the normal 10 spot purchase for day 8 .\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : monday , march 06 , 2000 4 : 09 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n&gt; graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n&gt; roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n&gt; w . ; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; increase in flow to a total of 28 for day 7 . this is a\\r\\n&gt; total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n&gt; outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n&gt; be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt;</td>\n",
              "      <td>ham</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>Subject: tenaska iv 4 / 01\\r\\nplease change the demand fee on deal 384258 for april from $ 2 , 862 , 531 . 95 to $ 2 , 855 , 089 . 73 . we need to return some money to tenaska iv . the transport expenses are less than what was etimated on the spreadsheet .\\r\\nmegan</td>\n",
              "      <td>ham</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>Subject: credit watch list - - week of 11 / 5 / 01\\r\\nattached is a revised credit watch listing for the week of 11 / 05 / 01 . there are no updates from last week ' s list .\\r\\nif there are any personnel in your group that were not included in this distribution , please insure that they receive a copy of this report .\\r\\nto add additional people to this distribution , or if this report has been sent to you in error , please contact veronica espinoza at x 6 - 6002 .\\r\\nfor other questions , please contact jason r . williams at x 5 - 3923 , veronica espinoza at x 6 - 6002 or darren vanek at x 3 - 1436 .</td>\n",
              "      <td>ham</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>Subject: proccess credit cards online , hispanic\\r\\nknapp , {\\r\\nneed affordable but reliable web hosting for only $ 5 / month ?\\r\\n- 800 mb disk space\\r\\n- unlimited email accounts\\r\\n- free shopping cart . . . and much more !\\r\\nplease contact us to take advantage of this new offer at\\r\\nhttp : / / viewhostdeals . com\\r\\nbelvidere\\r\\n</td>\n",
              "      <td>spam</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>Subject: hpl nom for march 28 , 2000\\r\\n( see attached file : hplo 328 . xls )\\r\\n- hplo 328 . xls</td>\n",
              "      <td>ham</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>Subject: ? ? ? , ? &gt; ? ? ? ? ? ?\\r\\n? ? ?? ? ?  ? ? r    ????? ? ? ? r ? ? . ? ? ? r ? ? . . . . . . .\\r\\n ????? ? ? ? ? 100 ? ? ?? ? y ? ? ? r ? ? www . 7 english 7 . co . kr ? ??  ?? ? ? ? ?   ?    ?? ??? ? ?? ? ??y ? r?  ?? ? ???  ??? ? ? ? ? .\\r\\n ? 1   ???? ? ? ? ? ? ? ? r ? ? 80 % 25   ? ? -  ? ? ? ? ? ? ?  ? ??  ? ? ?   ?   ? ?? 98 % ?? ????  ?  ?? $ ? ?  ?? ? ?? ? ? ? ? .  ? 2    ????? ? ? ? ? ? ? ? ? , ? r ? ??? ?? ? ? - ? ? ? ? ??  ?\\r\\n? ??? ? r ? r ? ? ?? ? ??? ?  ? ? ? ??? ? ? ? ? ? ?  ? ?? .\\r\\n? ? ? ?   ?  ? ?  ? ?  ? ? ? r  ?? ? ?? ? ? ? r ? ??? 1 . ? ? ? ???  ??  ?  ??   ??  ? 2 . ? ??     ?    ? ?? ? ??  ??????  ? 3 . ??   ?? ? ? ???? ? ? ? ? ? ?  ??  ? ? ??  ??  ? ? .\\r\\n? ? ? ? ?   ?? $ ?? ? ?  ?   ? ?   ? ? ?   ? ? ???? ??  ?  ? ? ? .  ? ? r  ? ? ? ? ?   ?    ?? ?  ? ? ???? ?  ??  ?  ? ? ? ? ? ? ? ? ? . - ? ? ? ?   ?    ??  ? ? ???? ? www . 7 english 7 . co . kr -\\r\\n?? ? ? ? ???  ??  ? ?  ? ? ? ? ?? ? ?? ? ??? ? ? ? ? . ? ?  ??   ?????  ? ?  ?  ? ? yoonsuk 37 @ netian . com ? ?  ? ? ?  ?? ?? ? ? ?   ????? ? ? ?  ? ?  ?  ??? ??  ? ?  ? ?   ??  ? ? ? ? ??  ???? ? ? ?? ? ?  ? ? ? ? r?? ? ?  ?  ? ? ? ? ? . ? ?  ?  ?  ? ?? ? ?  ? ?  ? ? ? ?? ? ?? ? ??? ? ? ? ?\\r\\n[ ? ?  ?  ??? ]\\r\\n[ rejection ]\\r\\n? ????? ? ? ? ? ? ????? ? ? ? ? ? ? ??  ?? ?  ?  ?  ? ? ? ?  ? ? ? ? . [ ? ?  ?\\r\\n ??? ] ??\\r\\n??  ? ? ? ?? ?? ? ?  ? ? ?   ?? ? ?  ?  ? ? ? ? ? .\\r\\n( ?  ? ????? ? ?  ??? ? ??? ? ? ? ? ? r??  ?? ? ?? ? ??? ?  ?y  ? ? ? ?  ? ??   ? ? r?? ? ? ? ? ? ? ? ? . )\\r\\nplease click\\r\\n[ ? ??? ???  ]\\r\\n ????? ?? ? ? ? ? bbs , community ,  ?  ??? ? ? ????  ?  ???  ?   ? ? ?? ? ? ?  ?   ? ? ??? ? ? ? ? ? ?\\r\\n? ??? ?? ? ? ?? ? ???   ??? $ ? ? ? ? ??? ? ? ? ? ? ??  ? ???? ? ?  ? ? ? ? ? .\\r\\n? ????  ? : http : / / www . kdnuggets . com / news / 97 / nl 9 . html ? ?  ?   ? ? ?? ? ?\\r\\n? ???  ?   : 2004 - 08 - 30 ? ??? 7 : 40 : 42 , ? ? ? ?  ?   : 2004 - 08 - 30 ? ??? 1 : 25 : 52\\r\\n</td>\n",
              "      <td>spam</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>450 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56b34c4f-4900-4e1c-a2da-496a8ffd04c3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56b34c4f-4900-4e1c-a2da-496a8ffd04c3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56b34c4f-4900-4e1c-a2da-496a8ffd04c3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8702c153-59be-4643-82ce-f23de6f3481b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8702c153-59be-4643-82ce-f23de6f3481b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8702c153-59be-4643-82ce-f23de6f3481b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0c48d6e2-a631-45b2-a6a8-90ac3a177fd9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('copy_of_enron1_train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0c48d6e2-a631-45b2-a6a8-90ac3a177fd9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('copy_of_enron1_train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "copy_of_enron1_train_df",
              "summary": "{\n  \"name\": \"copy_of_enron1_train_df\",\n  \"rows\": 450,\n  \"fields\": [\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 448,\n        \"samples\": [\n          \"Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : continuous _ gr = f 4 wth _ with _ the _ pill ? =\\r\\nour naturalgain + penis enlargement pills will expand ,\\r\\nlengthen and enlarge your penis 3 + inches .\\r\\n100 % satisfaction guaranteed ! or your money back !\\r\\n* gain 3 + full\\r\\ninches in length\\r\\n* increase\\r\\nyour penis width ( girth ) by 20 %\\r\\n* stop premature\\r\\nejaculation !\\r\\n* produce stronger ,\\r\\nrock hard erections\\r\\n* 100 % safe\\r\\nto take , with no side effects\\r\\n* fast priority\\r\\nfed - ex shipping worldwide\\r\\n* fedex tracking\\r\\nnumbers with all orders\\r\\n* doctor approved\\r\\nand recommended\\r\\n* no pumps !\\r\\nno surgery ! no exercises !\\r\\n* 100 % money\\r\\nback guarantee\\r\\n* free bottle\\r\\nof naturalgain + worth over $ 50\\r\\n* free \\\" male\\r\\nhelp e - book \\\" worth over $ 50\\r\\nclick here to enlarge your penis\\r\\n\",\n          \"Subject: hpl meter # 989648 tram / hpl - transtexas thompson\\r\\ndaren :\\r\\non 9 / 24 / 99 , the above meter recorded flow of 437 mmbtus . there was no deal\\r\\nat this meter the month prior or after . logistics needs either a deal to\\r\\nrecord these volumes which have flowed into hpl ' s pipeline or logistics needs\\r\\napproval to writeoff these volumes to unaccounted for gas . ( please print ,\\r\\nsign , and return the original to clem cernosek ) .\\r\\napproval to writeoff the volumes to unaccounted for gas loss\\r\\nor\\r\\ndeal / deal ticket # / customer ( buyer / seller )\\r\\nthanks , clem\",\n          \"Subject: february o & m reports\\r\\nfinal february budget numbers attached .\\r\\ny\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by yvette g connevey / corp / enron on\\r\\n03 / 24 / 2000 10 : 52 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nenron north america corp .\\r\\nfrom : jennifer blay @ ect 03 / 21 / 2000 04 : 15 pm\\r\\nto : brenda f herod / hou / ect @ ect\\r\\ncc : yvette g connevey / corp / enron @ enron\\r\\nsubject : february o & m reports\\r\\nattached are your final february o & m reports .\\r\\nplease call me with any questions .\\r\\nthank you\\r\\njennifer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"High\",\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_word_length\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Low\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_nouns\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_verbs\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Very High\",\n          \"High\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_adjectives\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sp_char\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Very High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nums\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Very High\",\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "copy_of_enron1_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hq_tkNybnaUQ"
      },
      "outputs": [],
      "source": [
        "copy_of_enron1_train_df['Email_tok'] = copy_of_enron1_train_df['Email'].apply(preprocess)\n",
        "copy_of_enron2_train_df['Email_tok'] = copy_of_enron2_train_df['Email'].apply(preprocess)\n",
        "copy_of_enron4_train_df['Email_tok'] = copy_of_enron4_train_df['Email'].apply(preprocess)\n",
        "\n",
        "copy_of_enron1_test_df['Email_tok'] = copy_of_enron1_test_df['Email'].apply(preprocess)\n",
        "copy_of_enron2_test_df['Email_tok'] = copy_of_enron2_test_df['Email'].apply(preprocess)\n",
        "copy_of_enron4_test_df['Email_tok'] = copy_of_enron4_test_df['Email'].apply(preprocess)\n",
        "\n",
        "copy_of_enron1_train_df['Email_str'] = copy_of_enron1_train_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "copy_of_enron2_train_df['Email_str'] = copy_of_enron2_train_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "copy_of_enron4_train_df['Email_str'] = copy_of_enron4_train_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "copy_of_enron1_test_df['Email_str'] = copy_of_enron1_test_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "copy_of_enron2_test_df['Email_str'] = copy_of_enron2_test_df['Email_tok'].apply(lambda x: ' '.join(x))\n",
        "copy_of_enron4_test_df['Email_str'] = copy_of_enron4_test_df['Email_tok'].apply(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "_3omFPBCoTaK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8eece08-09fb-45b5-9cdf-a3e295fe1171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Email  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } > loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or   \n",
              "2                                                                                                                                                                         Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 & 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg & e   \n",
              "4  Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy ' ; graham , travis e .\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> i now understand that the transport agreement between hpl and\\r\\n> midcon has been extended for delivery for day 8 .\\r\\n>\\r\\n> 28 midcon\\r\\n> 13 hpl\\r\\n>\\r\\n> swing it down on hpl .\\r\\n>\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> flow returning to the normal 10 spot purchase for day 8 .\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : monday , march 06 , 2000 4 : 09 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n> graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n> roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n> w . ; ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> increase in flow to a total of 28 for day 7 . this is a\\r\\n> total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n> outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n> be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n>\\r\\n>\\r\\n>   \n",
              "\n",
              "   Type word_count avg_word_length  num_nouns  num_verbs num_adjectives  \\\n",
              "0   ham     Medium       Very High        Low     Medium            Low   \n",
              "1  spam       High             Low       High     Medium           High   \n",
              "2   ham  Very High             Low  Very High  Very High      Very High   \n",
              "3   ham     Medium            High        Low        Low         Medium   \n",
              "4   ham  Very High             Low  Very High  Very High      Very High   \n",
              "\n",
              "     sp_char       nums  \\\n",
              "0        Low       High   \n",
              "1  Very High       High   \n",
              "2  Very High  Very High   \n",
              "3        Low       High   \n",
              "4  Very High  Very High   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Email_tok  \\\n",
              "0                                                                                                                                                                                                                                                                                [subject, deals, to, be, extended, on, meter, 985097, 12, 00, according, to, the, meter, statement, there, was, overflow, from, november, on, meter, 985097, and, the, following, deals, need, to, be, extended, for, 12, 1, only, 118532, 101473, 138017, thanks, and, if, you, need, further, information, please, let, me, know, karen]   \n",
              "1                                                                                                [subject, re, 13, driving, at, in, 1876, dogs, and, cats, that, s, a, call, for, you, glrls, 9, ho, 7, squlrt, when, they, 7, cu, as, far, as, i, know, creai, femaie, ejacuiation, clever, 9, thef, ettest, pussles, you, d, better, not, to, s, of, ldeos, phot, 0, s, ll, e, 609, f, 8, ufc, 5, kblbsho, s, engine, 30, d, ys, for, a, 1, doll, r, lt, slre, l, in, 1893, good, night, e, ter, don, t, look, very, fit, and, i, can, in, 1968, date, of, birth, in, 1927, eye, one, loook, at, yes, it, s, me, in, ...]   \n",
              "2           [subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, it, s, been, created, sitara, 343421, forwarded, by, lee, l, papayoti, hou, ect, on, 07, 25, 2000, 10, 47, am, from, robert, e, lloyd, 07, 25, 2000, 10, 47, am, to, lee, l, papayoti, hou, ect, ect, cc, subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, fyi, forwarded, by, robert, e, lloyd, hou, ect, on, 07, 25, 2000, 10, 46, am, from, robert, e, lloyd, 07, 25, 2000, 10, 29, am, to, lucy, ortiz, hou, ect, ect, craig, breslau, hou, ect, ect, cc, anita, ...]   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                   [subject, eastrans, nomination, effective, february, 1, 2001, effective, 2, 1, 01, the, deliveries, into, eastrans, will, be, 25, 000, mmbtu, dy, the, redeliveries, will, be, 7300, mmbtu, dy, from, fuels, cotton, valley, 17, 700, mmbtu, to, pg, e]   \n",
              "4  [subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, this, is, message, 2, thanks, jackie, forwarded, by, jackie, young, hou, ect, on, 03, 08, 2000, 08, 47, am, allen, john, s, on, 03, 08, 2000, 07, 24, 29, am, to, mason, greg, cc, nachlinger, ken, young, jackie, maake, roger, w, subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, i, m, not, buying, any, spot, gas, for, day, 9, we, will, be, in, a, partial, non, take, situation, for, at, less, part, of, the, day, until, we, get, a, second, stage, compressor, up, and, get, some, ...]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Email_str  \n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    subject deals to be extended on meter 985097 12 00 according to the meter statement there was overflow from november on meter 985097 and the following deals need to be extended for 12 1 only 118532 101473 138017 thanks and if you need further information please let me know karen  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or  \n",
              "2                                    subject settelement request for an equistar buyback ticket with ena as shipper it s been created sitara 343421 forwarded by lee l papayoti hou ect on 07 25 2000 10 47 am from robert e lloyd 07 25 2000 10 47 am to lee l papayoti hou ect ect cc subject settelement request for an equistar buyback ticket with ena as shipper fyi forwarded by robert e lloyd hou ect on 07 25 2000 10 46 am from robert e lloyd 07 25 2000 10 29 am to lucy ortiz hou ect ect craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect pat clynes corp enron enron subject settelement request for an equistar buyback ticket with ena as shipper i spoke to anita this morning and she suggested i forward you this request to set up a sitara ticket for an ena deal ticket to handle the buyback on meters 981373 981384 that pretain to equistar for june and july 2000 equistar invoice can not be drafted without this deal ticket this is an urgent request for settlement to finalize equistar invoice for june 2000 activity forwarded by robert e lloyd hou ect on 07 25 2000 10 23 am from robert e lloyd 07 14 2000 10 27 am to craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect howard b camp hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper lee ask me to forward this note mail to you settlement is trying to close today thus it is urgent that i get this resolved asap forwarded by robert e lloyd hou ect on 07 14 2000 10 24 am from robert e lloyd 07 14 2000 09 29 am to lee l papayoti hou ect ect cc anita luong hou ect ect daren j farmer hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper daren wanted me to make this request to you for an equistar buyback ticket on enron north america ena equistar has nominated activity on ena for june and july 2000 production settlement seems to think a buyback ticket is necessary to properly account for equistar s monthly activity  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           subject eastrans nomination effective february 1 2001 effective 2 1 01 the deliveries into eastrans will be 25 000 mmbtu dy the redeliveries will be 7300 mmbtu dy from fuels cotton valley 17 700 mmbtu to pg e  \n",
              "4  subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce89eac4-2a6f-4dfe-aad2-690323a9c0d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Email</th>\n",
              "      <th>Type</th>\n",
              "      <th>word_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>num_nouns</th>\n",
              "      <th>num_verbs</th>\n",
              "      <th>num_adjectives</th>\n",
              "      <th>sp_char</th>\n",
              "      <th>nums</th>\n",
              "      <th>Email_tok</th>\n",
              "      <th>Email_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren</td>\n",
              "      <td>ham</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "      <td>[subject, deals, to, be, extended, on, meter, 985097, 12, 00, according, to, the, meter, statement, there, was, overflow, from, november, on, meter, 985097, and, the, following, deals, need, to, be, extended, for, 12, 1, only, 118532, 101473, 138017, thanks, and, if, you, need, further, information, please, let, me, know, karen]</td>\n",
              "      <td>subject deals to be extended on meter 985097 12 00 according to the meter statement there was overflow from november on meter 985097 and the following deals need to be extended for 12 1 only 118532 101473 138017 thanks and if you need further information please let me know karen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } &gt; loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or</td>\n",
              "      <td>spam</td>\n",
              "      <td>High</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>High</td>\n",
              "      <td>[subject, re, 13, driving, at, in, 1876, dogs, and, cats, that, s, a, call, for, you, glrls, 9, ho, 7, squlrt, when, they, 7, cu, as, far, as, i, know, creai, femaie, ejacuiation, clever, 9, thef, ettest, pussles, you, d, better, not, to, s, of, ldeos, phot, 0, s, ll, e, 609, f, 8, ufc, 5, kblbsho, s, engine, 30, d, ys, for, a, 1, doll, r, lt, slre, l, in, 1893, good, night, e, ter, don, t, look, very, fit, and, i, can, in, 1968, date, of, birth, in, 1927, eye, one, loook, at, yes, it, s, me, in, ...]</td>\n",
              "      <td>subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 &amp; 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .</td>\n",
              "      <td>ham</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>[subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, it, s, been, created, sitara, 343421, forwarded, by, lee, l, papayoti, hou, ect, on, 07, 25, 2000, 10, 47, am, from, robert, e, lloyd, 07, 25, 2000, 10, 47, am, to, lee, l, papayoti, hou, ect, ect, cc, subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, fyi, forwarded, by, robert, e, lloyd, hou, ect, on, 07, 25, 2000, 10, 46, am, from, robert, e, lloyd, 07, 25, 2000, 10, 29, am, to, lucy, ortiz, hou, ect, ect, craig, breslau, hou, ect, ect, cc, anita, ...]</td>\n",
              "      <td>subject settelement request for an equistar buyback ticket with ena as shipper it s been created sitara 343421 forwarded by lee l papayoti hou ect on 07 25 2000 10 47 am from robert e lloyd 07 25 2000 10 47 am to lee l papayoti hou ect ect cc subject settelement request for an equistar buyback ticket with ena as shipper fyi forwarded by robert e lloyd hou ect on 07 25 2000 10 46 am from robert e lloyd 07 25 2000 10 29 am to lucy ortiz hou ect ect craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect pat clynes corp enron enron subject settelement request for an equistar buyback ticket with ena as shipper i spoke to anita this morning and she suggested i forward you this request to set up a sitara ticket for an ena deal ticket to handle the buyback on meters 981373 981384 that pretain to equistar for june and july 2000 equistar invoice can not be drafted without this deal ticket this is an urgent request for settlement to finalize equistar invoice for june 2000 activity forwarded by robert e lloyd hou ect on 07 25 2000 10 23 am from robert e lloyd 07 14 2000 10 27 am to craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect howard b camp hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper lee ask me to forward this note mail to you settlement is trying to close today thus it is urgent that i get this resolved asap forwarded by robert e lloyd hou ect on 07 14 2000 10 24 am from robert e lloyd 07 14 2000 09 29 am to lee l papayoti hou ect ect cc anita luong hou ect ect daren j farmer hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper daren wanted me to make this request to you for an equistar buyback ticket on enron north america ena equistar has nominated activity on ena for june and july 2000 production settlement seems to think a buyback ticket is necessary to properly account for equistar s monthly activity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg &amp; e</td>\n",
              "      <td>ham</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Low</td>\n",
              "      <td>High</td>\n",
              "      <td>[subject, eastrans, nomination, effective, february, 1, 2001, effective, 2, 1, 01, the, deliveries, into, eastrans, will, be, 25, 000, mmbtu, dy, the, redeliveries, will, be, 7300, mmbtu, dy, from, fuels, cotton, valley, 17, 700, mmbtu, to, pg, e]</td>\n",
              "      <td>subject eastrans nomination effective february 1 2001 effective 2 1 01 the deliveries into eastrans will be 25 000 mmbtu dy the redeliveries will be 7300 mmbtu dy from fuels cotton valley 17 700 mmbtu to pg e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy ' ; graham , travis e .\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; i now understand that the transport agreement between hpl and\\r\\n&gt; midcon has been extended for delivery for day 8 .\\r\\n&gt;\\r\\n&gt; 28 midcon\\r\\n&gt; 13 hpl\\r\\n&gt;\\r\\n&gt; swing it down on hpl .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; flow returning to the normal 10 spot purchase for day 8 .\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : monday , march 06 , 2000 4 : 09 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n&gt; graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n&gt; roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n&gt; w . ; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; increase in flow to a total of 28 for day 7 . this is a\\r\\n&gt; total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n&gt; outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n&gt; be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt;</td>\n",
              "      <td>ham</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>Very High</td>\n",
              "      <td>[subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, this, is, message, 2, thanks, jackie, forwarded, by, jackie, young, hou, ect, on, 03, 08, 2000, 08, 47, am, allen, john, s, on, 03, 08, 2000, 07, 24, 29, am, to, mason, greg, cc, nachlinger, ken, young, jackie, maake, roger, w, subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, i, m, not, buying, any, spot, gas, for, day, 9, we, will, be, in, a, partial, non, take, situation, for, at, less, part, of, the, day, until, we, get, a, second, stage, compressor, up, and, get, some, ...]</td>\n",
              "      <td>subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce89eac4-2a6f-4dfe-aad2-690323a9c0d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce89eac4-2a6f-4dfe-aad2-690323a9c0d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce89eac4-2a6f-4dfe-aad2-690323a9c0d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-248bd174-58a0-4908-937a-7977a2c40943\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-248bd174-58a0-4908-937a-7977a2c40943')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-248bd174-58a0-4908-937a-7977a2c40943 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "copy_of_enron1_train_df",
              "summary": "{\n  \"name\": \"copy_of_enron1_train_df\",\n  \"rows\": 450,\n  \"fields\": [\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 448,\n        \"samples\": [\n          \"Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : continuous _ gr = f 4 wth _ with _ the _ pill ? =\\r\\nour naturalgain + penis enlargement pills will expand ,\\r\\nlengthen and enlarge your penis 3 + inches .\\r\\n100 % satisfaction guaranteed ! or your money back !\\r\\n* gain 3 + full\\r\\ninches in length\\r\\n* increase\\r\\nyour penis width ( girth ) by 20 %\\r\\n* stop premature\\r\\nejaculation !\\r\\n* produce stronger ,\\r\\nrock hard erections\\r\\n* 100 % safe\\r\\nto take , with no side effects\\r\\n* fast priority\\r\\nfed - ex shipping worldwide\\r\\n* fedex tracking\\r\\nnumbers with all orders\\r\\n* doctor approved\\r\\nand recommended\\r\\n* no pumps !\\r\\nno surgery ! no exercises !\\r\\n* 100 % money\\r\\nback guarantee\\r\\n* free bottle\\r\\nof naturalgain + worth over $ 50\\r\\n* free \\\" male\\r\\nhelp e - book \\\" worth over $ 50\\r\\nclick here to enlarge your penis\\r\\n\",\n          \"Subject: hpl meter # 989648 tram / hpl - transtexas thompson\\r\\ndaren :\\r\\non 9 / 24 / 99 , the above meter recorded flow of 437 mmbtus . there was no deal\\r\\nat this meter the month prior or after . logistics needs either a deal to\\r\\nrecord these volumes which have flowed into hpl ' s pipeline or logistics needs\\r\\napproval to writeoff these volumes to unaccounted for gas . ( please print ,\\r\\nsign , and return the original to clem cernosek ) .\\r\\napproval to writeoff the volumes to unaccounted for gas loss\\r\\nor\\r\\ndeal / deal ticket # / customer ( buyer / seller )\\r\\nthanks , clem\",\n          \"Subject: february o & m reports\\r\\nfinal february budget numbers attached .\\r\\ny\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by yvette g connevey / corp / enron on\\r\\n03 / 24 / 2000 10 : 52 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nenron north america corp .\\r\\nfrom : jennifer blay @ ect 03 / 21 / 2000 04 : 15 pm\\r\\nto : brenda f herod / hou / ect @ ect\\r\\ncc : yvette g connevey / corp / enron @ enron\\r\\nsubject : february o & m reports\\r\\nattached are your final february o & m reports .\\r\\nplease call me with any questions .\\r\\nthank you\\r\\njennifer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"High\",\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_word_length\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Low\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_nouns\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_verbs\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Very High\",\n          \"High\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_adjectives\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sp_char\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Very High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nums\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Very High\",\n          \"Low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_tok\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_str\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 447,\n        \"samples\": [\n          \"subject re fast shipping rnd alt rnd alt rnd alt fast ups shipping on low price drugs from canada take off 6 rnd alt rnd alt rnd alt\",\n          \"subject photoshop windows office cheap main trending abasements darer prudently fortuitous undergone lighthearted charm orinoco taster railroad affluent pornographic cuvier irvin parkhouse blameworthy chlorophyll robed diagrammatic fogarty clears bayda inconveniencing managing represented smartness hashish academies shareholders unload badness danielson pure caffein spaniard chargeable levin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "copy_of_enron1_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lgQXns8qobs3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e5d8125-9af2-42ed-e837-7f829b48dffe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Email  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } > loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or   \n",
              "2                                                                                                                                                                         Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 & 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg & e   \n",
              "4  Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy ' ; graham , travis e .\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> i now understand that the transport agreement between hpl and\\r\\n> midcon has been extended for delivery for day 8 .\\r\\n>\\r\\n> 28 midcon\\r\\n> 13 hpl\\r\\n>\\r\\n> swing it down on hpl .\\r\\n>\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n> travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n> richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n> ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> flow returning to the normal 10 spot purchase for day 8 .\\r\\n>\\r\\n> - - - - - original message - - - - -\\r\\n> from : allen , john s .\\r\\n> sent : monday , march 06 , 2000 4 : 09 pm\\r\\n> to : ' mason , greg '\\r\\n> cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n> graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n> roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n> w . ; ' shimek , patsy '\\r\\n> subject : re : midcon 9401 ( permanent march file - expanded\\r\\n> distribution list )\\r\\n>\\r\\n> increase in flow to a total of 28 for day 7 . this is a\\r\\n> total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n> outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n> be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n>\\r\\n>\\r\\n>   \n",
              "\n",
              "   Type  word_count  avg_word_length  num_nouns  num_verbs  num_adjectives  \\\n",
              "0   ham           2                3          1          2               1   \n",
              "1  spam           0                1          0          2               0   \n",
              "2   ham           3                1          3          3               3   \n",
              "3   ham           2                0          1          1               2   \n",
              "4   ham           3                1          3          3               3   \n",
              "\n",
              "   sp_char  nums  \\\n",
              "0        1     0   \n",
              "1        3     0   \n",
              "2        3     3   \n",
              "3        1     0   \n",
              "4        3     3   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Email_tok  \\\n",
              "0                                                                                                                                                                                                                                                                                [subject, deals, to, be, extended, on, meter, 985097, 12, 00, according, to, the, meter, statement, there, was, overflow, from, november, on, meter, 985097, and, the, following, deals, need, to, be, extended, for, 12, 1, only, 118532, 101473, 138017, thanks, and, if, you, need, further, information, please, let, me, know, karen]   \n",
              "1                                                                                                [subject, re, 13, driving, at, in, 1876, dogs, and, cats, that, s, a, call, for, you, glrls, 9, ho, 7, squlrt, when, they, 7, cu, as, far, as, i, know, creai, femaie, ejacuiation, clever, 9, thef, ettest, pussles, you, d, better, not, to, s, of, ldeos, phot, 0, s, ll, e, 609, f, 8, ufc, 5, kblbsho, s, engine, 30, d, ys, for, a, 1, doll, r, lt, slre, l, in, 1893, good, night, e, ter, don, t, look, very, fit, and, i, can, in, 1968, date, of, birth, in, 1927, eye, one, loook, at, yes, it, s, me, in, ...]   \n",
              "2           [subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, it, s, been, created, sitara, 343421, forwarded, by, lee, l, papayoti, hou, ect, on, 07, 25, 2000, 10, 47, am, from, robert, e, lloyd, 07, 25, 2000, 10, 47, am, to, lee, l, papayoti, hou, ect, ect, cc, subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, fyi, forwarded, by, robert, e, lloyd, hou, ect, on, 07, 25, 2000, 10, 46, am, from, robert, e, lloyd, 07, 25, 2000, 10, 29, am, to, lucy, ortiz, hou, ect, ect, craig, breslau, hou, ect, ect, cc, anita, ...]   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                   [subject, eastrans, nomination, effective, february, 1, 2001, effective, 2, 1, 01, the, deliveries, into, eastrans, will, be, 25, 000, mmbtu, dy, the, redeliveries, will, be, 7300, mmbtu, dy, from, fuels, cotton, valley, 17, 700, mmbtu, to, pg, e]   \n",
              "4  [subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, this, is, message, 2, thanks, jackie, forwarded, by, jackie, young, hou, ect, on, 03, 08, 2000, 08, 47, am, allen, john, s, on, 03, 08, 2000, 07, 24, 29, am, to, mason, greg, cc, nachlinger, ken, young, jackie, maake, roger, w, subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, i, m, not, buying, any, spot, gas, for, day, 9, we, will, be, in, a, partial, non, take, situation, for, at, less, part, of, the, day, until, we, get, a, second, stage, compressor, up, and, get, some, ...]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Email_str  \n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    subject deals to be extended on meter 985097 12 00 according to the meter statement there was overflow from november on meter 985097 and the following deals need to be extended for 12 1 only 118532 101473 138017 thanks and if you need further information please let me know karen  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or  \n",
              "2                                    subject settelement request for an equistar buyback ticket with ena as shipper it s been created sitara 343421 forwarded by lee l papayoti hou ect on 07 25 2000 10 47 am from robert e lloyd 07 25 2000 10 47 am to lee l papayoti hou ect ect cc subject settelement request for an equistar buyback ticket with ena as shipper fyi forwarded by robert e lloyd hou ect on 07 25 2000 10 46 am from robert e lloyd 07 25 2000 10 29 am to lucy ortiz hou ect ect craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect pat clynes corp enron enron subject settelement request for an equistar buyback ticket with ena as shipper i spoke to anita this morning and she suggested i forward you this request to set up a sitara ticket for an ena deal ticket to handle the buyback on meters 981373 981384 that pretain to equistar for june and july 2000 equistar invoice can not be drafted without this deal ticket this is an urgent request for settlement to finalize equistar invoice for june 2000 activity forwarded by robert e lloyd hou ect on 07 25 2000 10 23 am from robert e lloyd 07 14 2000 10 27 am to craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect howard b camp hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper lee ask me to forward this note mail to you settlement is trying to close today thus it is urgent that i get this resolved asap forwarded by robert e lloyd hou ect on 07 14 2000 10 24 am from robert e lloyd 07 14 2000 09 29 am to lee l papayoti hou ect ect cc anita luong hou ect ect daren j farmer hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper daren wanted me to make this request to you for an equistar buyback ticket on enron north america ena equistar has nominated activity on ena for june and july 2000 production settlement seems to think a buyback ticket is necessary to properly account for equistar s monthly activity  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           subject eastrans nomination effective february 1 2001 effective 2 1 01 the deliveries into eastrans will be 25 000 mmbtu dy the redeliveries will be 7300 mmbtu dy from fuels cotton valley 17 700 mmbtu to pg e  \n",
              "4  subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e11024ae-934d-4eae-98af-07c1d996e424\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Email</th>\n",
              "      <th>Type</th>\n",
              "      <th>word_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>num_nouns</th>\n",
              "      <th>num_verbs</th>\n",
              "      <th>num_adjectives</th>\n",
              "      <th>sp_char</th>\n",
              "      <th>nums</th>\n",
              "      <th>Email_tok</th>\n",
              "      <th>Email_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: deals to be extended on meter 985097 - 12 / 00\\r\\naccording to the meter statement , there was overflow from november on meter\\r\\n985097 and the following deals need to be extended for 12 / 1 only .\\r\\n118532\\r\\n101473\\r\\n138017\\r\\nthanks and if you need further information , please let me know .\\r\\nkaren</td>\n",
              "      <td>ham</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject, deals, to, be, extended, on, meter, 985097, 12, 00, according, to, the, meter, statement, there, was, overflow, from, november, on, meter, 985097, and, the, following, deals, need, to, be, extended, for, 12, 1, only, 118532, 101473, 138017, thanks, and, if, you, need, further, information, please, let, me, know, karen]</td>\n",
              "      <td>subject deals to be extended on meter 985097 12 00 according to the meter statement there was overflow from november on meter 985097 and the following deals need to be extended for 12 1 only 118532 101473 138017 thanks and if you need further information please let me know karen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: re [ 13 ]\\r\\ndriving at ? in 1876\\r\\ndogs and cats that ' s a call for you\\r\\nglrls 9 \\ / \\ / ho 7 squlrt when they 7 cu | \\ / | ! as far as i know\\r\\ncreai femaie ejacuiation ! clever\\r\\n9 thef \\ / \\ / ettest pussles ! you ' d better not . .\\r\\nto | \\ | s of \\ / ldeos , phot 0 s , ll \\ / e 609 f 8 ufc 5 kblbsho \\ / \\ / s !\\r\\nengine\\r\\n30 d / \\ ys for a 1 doll / \\ r - lt ' slre / \\ l !\\r\\nin 1893\\r\\ngood night ! e | \\ | ter ! don ' t look very fit\\r\\nand i can\\r\\nin 1968 date of birth\\r\\nin 1927\\r\\neye one } &gt; loook at\\r\\nyes , it ' s me .\\r\\nin 1800 in 1867\\r\\nin 1842 city name or</td>\n",
              "      <td>spam</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject, re, 13, driving, at, in, 1876, dogs, and, cats, that, s, a, call, for, you, glrls, 9, ho, 7, squlrt, when, they, 7, cu, as, far, as, i, know, creai, femaie, ejacuiation, clever, 9, thef, ettest, pussles, you, d, better, not, to, s, of, ldeos, phot, 0, s, ll, e, 609, f, 8, ufc, 5, kblbsho, s, engine, 30, d, ys, for, a, 1, doll, r, lt, slre, l, in, 1893, good, night, e, ter, don, t, look, very, fit, and, i, can, in, 1968, date, of, birth, in, 1927, eye, one, loook, at, yes, it, s, me, in, ...]</td>\n",
              "      <td>subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nit ' s been created - sitara # 343421\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lee l papayoti / hou / ect on 07 / 25 / 2000\\r\\n10 : 47 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 47 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc :\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nfyi\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 46 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 25 / 2000 10 : 29 am\\r\\nto : lucy ortiz / hou / ect @ ect , craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , pat clynes / corp / enron @ enron\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ni spoke to anita this morning and she suggested i forward you this request\\r\\nto set up a sitara ticket for an \" ena \" deal\\r\\nticket to handle the buyback on meters 981373 &amp; 981384 that pretain to\\r\\nequistar for june and july 2000 .\\r\\nequistar invoice can not be drafted without this deal ticket .\\r\\nthis is an urgent request for settlement to finalize equistar invoice for\\r\\njune 2000 activity .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 25 / 2000\\r\\n10 : 23 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 10 : 27 am\\r\\nto : craig breslau / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , lee l papayoti / hou / ect @ ect , daren j\\r\\nfarmer / hou / ect @ ect , howard b camp / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\nlee ask me to forward this note - mail to you . settlement is trying to close\\r\\ntoday thus , it is urgent that i get this resolved asap .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by robert e lloyd / hou / ect on 07 / 14 / 2000\\r\\n10 : 24 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nfrom : robert e lloyd 07 / 14 / 2000 09 : 29 am\\r\\nto : lee l papayoti / hou / ect @ ect\\r\\ncc : anita luong / hou / ect @ ect , daren j farmer / hou / ect @ ect\\r\\nsubject : settelement request for an equistar \" buyback \" ticket with ena as\\r\\nshipper\\r\\ndaren wanted me to make this request to you for an equistar buyback ticket\\r\\non enron north america { ena } .\\r\\nequistar has nominated activity on ena for june and july 2000 production .\\r\\nsettlement seems to think a buyback ticket is necessary to properly account\\r\\nfor equistar ' s monthly activity .</td>\n",
              "      <td>ham</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>[subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, it, s, been, created, sitara, 343421, forwarded, by, lee, l, papayoti, hou, ect, on, 07, 25, 2000, 10, 47, am, from, robert, e, lloyd, 07, 25, 2000, 10, 47, am, to, lee, l, papayoti, hou, ect, ect, cc, subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, fyi, forwarded, by, robert, e, lloyd, hou, ect, on, 07, 25, 2000, 10, 46, am, from, robert, e, lloyd, 07, 25, 2000, 10, 29, am, to, lucy, ortiz, hou, ect, ect, craig, breslau, hou, ect, ect, cc, anita, ...]</td>\n",
              "      <td>subject settelement request for an equistar buyback ticket with ena as shipper it s been created sitara 343421 forwarded by lee l papayoti hou ect on 07 25 2000 10 47 am from robert e lloyd 07 25 2000 10 47 am to lee l papayoti hou ect ect cc subject settelement request for an equistar buyback ticket with ena as shipper fyi forwarded by robert e lloyd hou ect on 07 25 2000 10 46 am from robert e lloyd 07 25 2000 10 29 am to lucy ortiz hou ect ect craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect pat clynes corp enron enron subject settelement request for an equistar buyback ticket with ena as shipper i spoke to anita this morning and she suggested i forward you this request to set up a sitara ticket for an ena deal ticket to handle the buyback on meters 981373 981384 that pretain to equistar for june and july 2000 equistar invoice can not be drafted without this deal ticket this is an urgent request for settlement to finalize equistar invoice for june 2000 activity forwarded by robert e lloyd hou ect on 07 25 2000 10 23 am from robert e lloyd 07 14 2000 10 27 am to craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect howard b camp hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper lee ask me to forward this note mail to you settlement is trying to close today thus it is urgent that i get this resolved asap forwarded by robert e lloyd hou ect on 07 14 2000 10 24 am from robert e lloyd 07 14 2000 09 29 am to lee l papayoti hou ect ect cc anita luong hou ect ect daren j farmer hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper daren wanted me to make this request to you for an equistar buyback ticket on enron north america ena equistar has nominated activity on ena for june and july 2000 production settlement seems to think a buyback ticket is necessary to properly account for equistar s monthly activity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: eastrans nomination effective february 1 , 2001\\r\\neffective 2 / 1 / 01 , the deliveries into eastrans will be 25 , 000 mmbtu / dy .\\r\\nthe redeliveries will be :\\r\\n7300 mmbtu / dy from fuels cotton valley\\r\\n17 , 700 mmbtu to pg &amp; e</td>\n",
              "      <td>ham</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject, eastrans, nomination, effective, february, 1, 2001, effective, 2, 1, 01, the, deliveries, into, eastrans, will, be, 25, 000, mmbtu, dy, the, redeliveries, will, be, 7300, mmbtu, dy, from, fuels, cotton, valley, 17, 700, mmbtu, to, pg, e]</td>\n",
              "      <td>subject eastrans nomination effective february 1 2001 effective 2 1 01 the deliveries into eastrans will be 25 000 mmbtu dy the redeliveries will be 7300 mmbtu dy from fuels cotton valley 17 700 mmbtu to pg e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: fw : midcon 9401 ( permanent march file - expanded distribution li\\r\\nst )\\r\\nthis is message # 2 .\\r\\nthanks\\r\\n- jackie -\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by jackie young / hou / ect on 03 / 08 / 2000 08 : 47\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" allen , john s . \" on 03 / 08 / 2000 07 : 24 : 29 am\\r\\nto : \" ' mason , greg ' \"\\r\\ncc : \" ' nachlinger , ken ' \" , \" ' young , jackie ' \"\\r\\n, \" maake , roger w . \"\\r\\nsubject : fw : midcon 9401 ( permanent march file - expanded distribution li st )\\r\\ni ' m not buying any spot gas for day 9 . we will be in a partial\\r\\nnon - take situation for at less part of the day , until we get a second stage\\r\\ncompressor up and get some of our fuel gas out of the flare . please bear\\r\\nwith us .\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 6 : 53 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy ' ; graham , travis e .\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; i now understand that the transport agreement between hpl and\\r\\n&gt; midcon has been extended for delivery for day 8 .\\r\\n&gt;\\r\\n&gt; 28 midcon\\r\\n&gt; 13 hpl\\r\\n&gt;\\r\\n&gt; swing it down on hpl .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : tuesday , march 07 , 2000 8 : 25 am\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ; graham ,\\r\\n&gt; travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ; roose ,\\r\\n&gt; richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory w . ;\\r\\n&gt; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; flow returning to the normal 10 spot purchase for day 8 .\\r\\n&gt;\\r\\n&gt; - - - - - original message - - - - -\\r\\n&gt; from : allen , john s .\\r\\n&gt; sent : monday , march 06 , 2000 4 : 09 pm\\r\\n&gt; to : ' mason , greg '\\r\\n&gt; cc : ' nachlinger , ken ' ; abdul - raheem , herman m . ; dudley , l . o . ;\\r\\n&gt; graham , travis e . ; ledet , eugene i . ; maake , roger w . ; moore , kathy d . ;\\r\\n&gt; roose , richard e . ; smith , ronald k . ; summers , r . bruce ; wilson , gregory\\r\\n&gt; w . ; ' shimek , patsy '\\r\\n&gt; subject : re : midcon 9401 ( permanent march file - expanded\\r\\n&gt; distribution list )\\r\\n&gt;\\r\\n&gt; increase in flow to a total of 28 for day 7 . this is a\\r\\n&gt; total of the 10 spot purchase and a transport of 18 to accommodate at\\r\\n&gt; outage on hpl . this will be occurring at 09 : 00 in the morning . you will\\r\\n&gt; be seeing a corresponding decrease of 18 on hpl during the same period .\\r\\n&gt;\\r\\n&gt;\\r\\n&gt;</td>\n",
              "      <td>ham</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>[subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, this, is, message, 2, thanks, jackie, forwarded, by, jackie, young, hou, ect, on, 03, 08, 2000, 08, 47, am, allen, john, s, on, 03, 08, 2000, 07, 24, 29, am, to, mason, greg, cc, nachlinger, ken, young, jackie, maake, roger, w, subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, i, m, not, buying, any, spot, gas, for, day, 9, we, will, be, in, a, partial, non, take, situation, for, at, less, part, of, the, day, until, we, get, a, second, stage, compressor, up, and, get, some, ...]</td>\n",
              "      <td>subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e11024ae-934d-4eae-98af-07c1d996e424')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e11024ae-934d-4eae-98af-07c1d996e424 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e11024ae-934d-4eae-98af-07c1d996e424');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-35a95950-7d41-49ff-91c6-b7eccbcabe74\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35a95950-7d41-49ff-91c6-b7eccbcabe74')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-35a95950-7d41-49ff-91c6-b7eccbcabe74 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "copy_of_enron1_train_df",
              "summary": "{\n  \"name\": \"copy_of_enron1_train_df\",\n  \"rows\": 450,\n  \"fields\": [\n    {\n      \"column\": \"Email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 448,\n        \"samples\": [\n          \"Subject: = ? iso - 8859 - 7 ? q ? = 5 b = 3 f = 5 d _ fwd : continuous _ gr = f 4 wth _ with _ the _ pill ? =\\r\\nour naturalgain + penis enlargement pills will expand ,\\r\\nlengthen and enlarge your penis 3 + inches .\\r\\n100 % satisfaction guaranteed ! or your money back !\\r\\n* gain 3 + full\\r\\ninches in length\\r\\n* increase\\r\\nyour penis width ( girth ) by 20 %\\r\\n* stop premature\\r\\nejaculation !\\r\\n* produce stronger ,\\r\\nrock hard erections\\r\\n* 100 % safe\\r\\nto take , with no side effects\\r\\n* fast priority\\r\\nfed - ex shipping worldwide\\r\\n* fedex tracking\\r\\nnumbers with all orders\\r\\n* doctor approved\\r\\nand recommended\\r\\n* no pumps !\\r\\nno surgery ! no exercises !\\r\\n* 100 % money\\r\\nback guarantee\\r\\n* free bottle\\r\\nof naturalgain + worth over $ 50\\r\\n* free \\\" male\\r\\nhelp e - book \\\" worth over $ 50\\r\\nclick here to enlarge your penis\\r\\n\",\n          \"Subject: hpl meter # 989648 tram / hpl - transtexas thompson\\r\\ndaren :\\r\\non 9 / 24 / 99 , the above meter recorded flow of 437 mmbtus . there was no deal\\r\\nat this meter the month prior or after . logistics needs either a deal to\\r\\nrecord these volumes which have flowed into hpl ' s pipeline or logistics needs\\r\\napproval to writeoff these volumes to unaccounted for gas . ( please print ,\\r\\nsign , and return the original to clem cernosek ) .\\r\\napproval to writeoff the volumes to unaccounted for gas loss\\r\\nor\\r\\ndeal / deal ticket # / customer ( buyer / seller )\\r\\nthanks , clem\",\n          \"Subject: february o & m reports\\r\\nfinal february budget numbers attached .\\r\\ny\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by yvette g connevey / corp / enron on\\r\\n03 / 24 / 2000 10 : 52 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nenron north america corp .\\r\\nfrom : jennifer blay @ ect 03 / 21 / 2000 04 : 15 pm\\r\\nto : brenda f herod / hou / ect @ ect\\r\\ncc : yvette g connevey / corp / enron @ enron\\r\\nsubject : february o & m reports\\r\\nattached are your final february o & m reports .\\r\\nplease call me with any questions .\\r\\nthank you\\r\\njennifer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_word_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_nouns\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_verbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_adjectives\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sp_char\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nums\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_tok\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_str\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 447,\n        \"samples\": [\n          \"subject re fast shipping rnd alt rnd alt rnd alt fast ups shipping on low price drugs from canada take off 6 rnd alt rnd alt rnd alt\",\n          \"subject photoshop windows office cheap main trending abasements darer prudently fortuitous undergone lighthearted charm orinoco taster railroad affluent pornographic cuvier irvin parkhouse blameworthy chlorophyll robed diagrammatic fogarty clears bayda inconveniencing managing represented smartness hashish academies shareholders unload badness danielson pure caffein spaniard chargeable levin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "le = LabelEncoder()\n",
        "for col in feature_columns:\n",
        "    copy_of_enron1_train_df[col] = le.fit_transform(copy_of_enron1_train_df[col])\n",
        "    copy_of_enron2_train_df[col] = le.fit_transform(copy_of_enron2_train_df[col])\n",
        "    copy_of_enron4_train_df[col] = le.fit_transform(copy_of_enron4_train_df[col])\n",
        "\n",
        "    copy_of_enron1_test_df[col] = le.fit_transform(copy_of_enron1_test_df[col])\n",
        "    copy_of_enron2_test_df[col] = le.fit_transform(copy_of_enron2_test_df[col])\n",
        "    copy_of_enron4_test_df[col] = le.fit_transform(copy_of_enron4_test_df[col])\n",
        "\n",
        "copy_of_enron1_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "67UvfhpspZ1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b58068d-ef78-4094-81ad-e5748b749d59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   word_count  avg_word_length  num_nouns  num_verbs  num_adjectives  sp_char  \\\n",
              "0           2                3          1          2               1        1   \n",
              "1           0                1          0          2               0        3   \n",
              "2           3                1          3          3               3        3   \n",
              "3           2                0          1          1               2        1   \n",
              "4           3                1          3          3               3        3   \n",
              "\n",
              "   nums  \\\n",
              "0     0   \n",
              "1     0   \n",
              "2     3   \n",
              "3     0   \n",
              "4     3   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Email_str  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    subject deals to be extended on meter 985097 12 00 according to the meter statement there was overflow from november on meter 985097 and the following deals need to be extended for 12 1 only 118532 101473 138017 thanks and if you need further information please let me know karen   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or   \n",
              "2                                    subject settelement request for an equistar buyback ticket with ena as shipper it s been created sitara 343421 forwarded by lee l papayoti hou ect on 07 25 2000 10 47 am from robert e lloyd 07 25 2000 10 47 am to lee l papayoti hou ect ect cc subject settelement request for an equistar buyback ticket with ena as shipper fyi forwarded by robert e lloyd hou ect on 07 25 2000 10 46 am from robert e lloyd 07 25 2000 10 29 am to lucy ortiz hou ect ect craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect pat clynes corp enron enron subject settelement request for an equistar buyback ticket with ena as shipper i spoke to anita this morning and she suggested i forward you this request to set up a sitara ticket for an ena deal ticket to handle the buyback on meters 981373 981384 that pretain to equistar for june and july 2000 equistar invoice can not be drafted without this deal ticket this is an urgent request for settlement to finalize equistar invoice for june 2000 activity forwarded by robert e lloyd hou ect on 07 25 2000 10 23 am from robert e lloyd 07 14 2000 10 27 am to craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect howard b camp hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper lee ask me to forward this note mail to you settlement is trying to close today thus it is urgent that i get this resolved asap forwarded by robert e lloyd hou ect on 07 14 2000 10 24 am from robert e lloyd 07 14 2000 09 29 am to lee l papayoti hou ect ect cc anita luong hou ect ect daren j farmer hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper daren wanted me to make this request to you for an equistar buyback ticket on enron north america ena equistar has nominated activity on ena for june and july 2000 production settlement seems to think a buyback ticket is necessary to properly account for equistar s monthly activity   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           subject eastrans nomination effective february 1 2001 effective 2 1 01 the deliveries into eastrans will be 25 000 mmbtu dy the redeliveries will be 7300 mmbtu dy from fuels cotton valley 17 700 mmbtu to pg e   \n",
              "4  subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Email_tok  \n",
              "0                                                                                                                                                                                                                                                                                [subject, deals, to, be, extended, on, meter, 985097, 12, 00, according, to, the, meter, statement, there, was, overflow, from, november, on, meter, 985097, and, the, following, deals, need, to, be, extended, for, 12, 1, only, 118532, 101473, 138017, thanks, and, if, you, need, further, information, please, let, me, know, karen]  \n",
              "1                                                                                                [subject, re, 13, driving, at, in, 1876, dogs, and, cats, that, s, a, call, for, you, glrls, 9, ho, 7, squlrt, when, they, 7, cu, as, far, as, i, know, creai, femaie, ejacuiation, clever, 9, thef, ettest, pussles, you, d, better, not, to, s, of, ldeos, phot, 0, s, ll, e, 609, f, 8, ufc, 5, kblbsho, s, engine, 30, d, ys, for, a, 1, doll, r, lt, slre, l, in, 1893, good, night, e, ter, don, t, look, very, fit, and, i, can, in, 1968, date, of, birth, in, 1927, eye, one, loook, at, yes, it, s, me, in, ...]  \n",
              "2           [subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, it, s, been, created, sitara, 343421, forwarded, by, lee, l, papayoti, hou, ect, on, 07, 25, 2000, 10, 47, am, from, robert, e, lloyd, 07, 25, 2000, 10, 47, am, to, lee, l, papayoti, hou, ect, ect, cc, subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, fyi, forwarded, by, robert, e, lloyd, hou, ect, on, 07, 25, 2000, 10, 46, am, from, robert, e, lloyd, 07, 25, 2000, 10, 29, am, to, lucy, ortiz, hou, ect, ect, craig, breslau, hou, ect, ect, cc, anita, ...]  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                   [subject, eastrans, nomination, effective, february, 1, 2001, effective, 2, 1, 01, the, deliveries, into, eastrans, will, be, 25, 000, mmbtu, dy, the, redeliveries, will, be, 7300, mmbtu, dy, from, fuels, cotton, valley, 17, 700, mmbtu, to, pg, e]  \n",
              "4  [subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, this, is, message, 2, thanks, jackie, forwarded, by, jackie, young, hou, ect, on, 03, 08, 2000, 08, 47, am, allen, john, s, on, 03, 08, 2000, 07, 24, 29, am, to, mason, greg, cc, nachlinger, ken, young, jackie, maake, roger, w, subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, i, m, not, buying, any, spot, gas, for, day, 9, we, will, be, in, a, partial, non, take, situation, for, at, less, part, of, the, day, until, we, get, a, second, stage, compressor, up, and, get, some, ...]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-edaaef2b-9b61-410b-90e3-b6f1e3243213\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>num_nouns</th>\n",
              "      <th>num_verbs</th>\n",
              "      <th>num_adjectives</th>\n",
              "      <th>sp_char</th>\n",
              "      <th>nums</th>\n",
              "      <th>Email_str</th>\n",
              "      <th>Email_tok</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>subject deals to be extended on meter 985097 12 00 according to the meter statement there was overflow from november on meter 985097 and the following deals need to be extended for 12 1 only 118532 101473 138017 thanks and if you need further information please let me know karen</td>\n",
              "      <td>[subject, deals, to, be, extended, on, meter, 985097, 12, 00, according, to, the, meter, statement, there, was, overflow, from, november, on, meter, 985097, and, the, following, deals, need, to, be, extended, for, 12, 1, only, 118532, 101473, 138017, thanks, and, if, you, need, further, information, please, let, me, know, karen]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or</td>\n",
              "      <td>[subject, re, 13, driving, at, in, 1876, dogs, and, cats, that, s, a, call, for, you, glrls, 9, ho, 7, squlrt, when, they, 7, cu, as, far, as, i, know, creai, femaie, ejacuiation, clever, 9, thef, ettest, pussles, you, d, better, not, to, s, of, ldeos, phot, 0, s, ll, e, 609, f, 8, ufc, 5, kblbsho, s, engine, 30, d, ys, for, a, 1, doll, r, lt, slre, l, in, 1893, good, night, e, ter, don, t, look, very, fit, and, i, can, in, 1968, date, of, birth, in, 1927, eye, one, loook, at, yes, it, s, me, in, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>subject settelement request for an equistar buyback ticket with ena as shipper it s been created sitara 343421 forwarded by lee l papayoti hou ect on 07 25 2000 10 47 am from robert e lloyd 07 25 2000 10 47 am to lee l papayoti hou ect ect cc subject settelement request for an equistar buyback ticket with ena as shipper fyi forwarded by robert e lloyd hou ect on 07 25 2000 10 46 am from robert e lloyd 07 25 2000 10 29 am to lucy ortiz hou ect ect craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect pat clynes corp enron enron subject settelement request for an equistar buyback ticket with ena as shipper i spoke to anita this morning and she suggested i forward you this request to set up a sitara ticket for an ena deal ticket to handle the buyback on meters 981373 981384 that pretain to equistar for june and july 2000 equistar invoice can not be drafted without this deal ticket this is an urgent request for settlement to finalize equistar invoice for june 2000 activity forwarded by robert e lloyd hou ect on 07 25 2000 10 23 am from robert e lloyd 07 14 2000 10 27 am to craig breslau hou ect ect cc anita luong hou ect ect lee l papayoti hou ect ect daren j farmer hou ect ect howard b camp hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper lee ask me to forward this note mail to you settlement is trying to close today thus it is urgent that i get this resolved asap forwarded by robert e lloyd hou ect on 07 14 2000 10 24 am from robert e lloyd 07 14 2000 09 29 am to lee l papayoti hou ect ect cc anita luong hou ect ect daren j farmer hou ect ect subject settelement request for an equistar buyback ticket with ena as shipper daren wanted me to make this request to you for an equistar buyback ticket on enron north america ena equistar has nominated activity on ena for june and july 2000 production settlement seems to think a buyback ticket is necessary to properly account for equistar s monthly activity</td>\n",
              "      <td>[subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, it, s, been, created, sitara, 343421, forwarded, by, lee, l, papayoti, hou, ect, on, 07, 25, 2000, 10, 47, am, from, robert, e, lloyd, 07, 25, 2000, 10, 47, am, to, lee, l, papayoti, hou, ect, ect, cc, subject, settelement, request, for, an, equistar, buyback, ticket, with, ena, as, shipper, fyi, forwarded, by, robert, e, lloyd, hou, ect, on, 07, 25, 2000, 10, 46, am, from, robert, e, lloyd, 07, 25, 2000, 10, 29, am, to, lucy, ortiz, hou, ect, ect, craig, breslau, hou, ect, ect, cc, anita, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>subject eastrans nomination effective february 1 2001 effective 2 1 01 the deliveries into eastrans will be 25 000 mmbtu dy the redeliveries will be 7300 mmbtu dy from fuels cotton valley 17 700 mmbtu to pg e</td>\n",
              "      <td>[subject, eastrans, nomination, effective, february, 1, 2001, effective, 2, 1, 01, the, deliveries, into, eastrans, will, be, 25, 000, mmbtu, dy, the, redeliveries, will, be, 7300, mmbtu, dy, from, fuels, cotton, valley, 17, 700, mmbtu, to, pg, e]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period</td>\n",
              "      <td>[subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, this, is, message, 2, thanks, jackie, forwarded, by, jackie, young, hou, ect, on, 03, 08, 2000, 08, 47, am, allen, john, s, on, 03, 08, 2000, 07, 24, 29, am, to, mason, greg, cc, nachlinger, ken, young, jackie, maake, roger, w, subject, fw, midcon, 9401, permanent, march, file, expanded, distribution, li, st, i, m, not, buying, any, spot, gas, for, day, 9, we, will, be, in, a, partial, non, take, situation, for, at, less, part, of, the, day, until, we, get, a, second, stage, compressor, up, and, get, some, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edaaef2b-9b61-410b-90e3-b6f1e3243213')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-edaaef2b-9b61-410b-90e3-b6f1e3243213 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-edaaef2b-9b61-410b-90e3-b6f1e3243213');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-011e32fe-639e-4a6e-9d35-4fe0e2c25e40\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-011e32fe-639e-4a6e-9d35-4fe0e2c25e40')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-011e32fe-639e-4a6e-9d35-4fe0e2c25e40 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"copy_of_enron1_train_df[X]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_word_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_nouns\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_verbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_adjectives\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sp_char\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nums\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_str\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"subject re 13 driving at in 1876 dogs and cats that s a call for you glrls 9 ho 7 squlrt when they 7 cu as far as i know creai femaie ejacuiation clever 9 thef ettest pussles you d better not to s of ldeos phot 0 s ll e 609 f 8 ufc 5 kblbsho s engine 30 d ys for a 1 doll r lt slre l in 1893 good night e ter don t look very fit and i can in 1968 date of birth in 1927 eye one loook at yes it s me in 1800 in 1867 in 1842 city name or\",\n          \"subject fw midcon 9401 permanent march file expanded distribution li st this is message 2 thanks jackie forwarded by jackie young hou ect on 03 08 2000 08 47 am allen john s on 03 08 2000 07 24 29 am to mason greg cc nachlinger ken young jackie maake roger w subject fw midcon 9401 permanent march file expanded distribution li st i m not buying any spot gas for day 9 we will be in a partial non take situation for at less part of the day until we get a second stage compressor up and get some of our fuel gas out of the flare please bear with us original message from allen john s sent tuesday march 07 2000 6 53 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy graham travis e subject re midcon 9401 permanent march file expanded distribution list i now understand that the transport agreement between hpl and midcon has been extended for delivery for day 8 28 midcon 13 hpl swing it down on hpl original message from allen john s sent tuesday march 07 2000 8 25 am to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list flow returning to the normal 10 spot purchase for day 8 original message from allen john s sent monday march 06 2000 4 09 pm to mason greg cc nachlinger ken abdul raheem herman m dudley l o graham travis e ledet eugene i maake roger w moore kathy d roose richard e smith ronald k summers r bruce wilson gregory w shimek patsy subject re midcon 9401 permanent march file expanded distribution list increase in flow to a total of 28 for day 7 this is a total of the 10 spot purchase and a transport of 18 to accommodate at outage on hpl this will be occurring at 09 00 in the morning you will be seeing a corresponding decrease of 18 on hpl during the same period\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Email_tok\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "X = feature_columns+['Email_str', 'Email_tok']\n",
        "y = ['Type']\n",
        "\n",
        "copy_of_enron1_train_df[X].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tYHuRwO6pxQt"
      },
      "outputs": [],
      "source": [
        "# new_results_acc = {}\n",
        "# new_results_f1 = {}\n",
        "# new_results_recall = {}\n",
        "# new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1ijBza04iU7"
      },
      "source": [
        "### Multinomial NB BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "TFNtjZzrpo_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7126520a-a5d6-4cb5-afb8-64aa5c980cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 10021) (456, 10021)\n",
            "(450, 10028) (456, 10028)\n",
            "enron1\n",
            "accuracy - 0.9407894736842105\n",
            "recall - 0.8657718120805369\n",
            "f1 score - 0.9052631578947368\n",
            "precision - 0.9485294117647058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.94      0.98      0.96       307\n",
            "        spam       0.95      0.87      0.91       149\n",
            "\n",
            "    accuracy                           0.94       456\n",
            "   macro avg       0.94      0.92      0.93       456\n",
            "weighted avg       0.94      0.94      0.94       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(463, 10375) (478, 10375)\n",
            "(463, 10382) (478, 10382)\n",
            "enron2\n",
            "accuracy - 0.9309623430962343\n",
            "recall - 0.8076923076923077\n",
            "f1 score - 0.8641975308641975\n",
            "precision - 0.9292035398230089\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.93      0.98      0.95       348\n",
            "        spam       0.93      0.81      0.86       130\n",
            "\n",
            "    accuracy                           0.93       478\n",
            "   macro avg       0.93      0.89      0.91       478\n",
            "weighted avg       0.93      0.93      0.93       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(535, 17792) (543, 17792)\n",
            "(535, 17799) (543, 17799)\n",
            "enron4\n",
            "accuracy - 0.9521178637200737\n",
            "recall - 0.979539641943734\n",
            "f1 score - 0.9671717171717172\n",
            "precision - 0.9551122194513716\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.94      0.88      0.91       152\n",
            "        spam       0.96      0.98      0.97       391\n",
            "\n",
            "    accuracy                           0.95       543\n",
            "   macro avg       0.95      0.93      0.94       543\n",
            "weighted avg       0.95      0.95      0.95       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import hstack\n",
        "def predict_type(X_train, X_test, name, feature_columns):\n",
        "    X_train_nums = X_train[feature_columns]\n",
        "    X_test_nums = X_test[feature_columns]\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    X_train_nums = X_train_nums.values\n",
        "    X_test_nums = X_test_nums.values\n",
        "\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_nums = scaler.fit_transform(X_train_nums)\n",
        "    X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "    X_train_combined = hstack([X_train, X_train_nums])\n",
        "    X_test_combined = hstack([X_test, X_test_nums])\n",
        "\n",
        "    print(X_train_combined.shape, X_test_combined.shape)\n",
        "\n",
        "    model = MultinomialNB(alpha=1.0)\n",
        "    model.fit(X_train_combined, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "predict_type(copy_of_enron1_train_df, copy_of_enron1_test_df, 'enron1', feature_columns)\n",
        "predict_type(copy_of_enron2_train_df, copy_of_enron2_test_df, 'enron2', feature_columns)\n",
        "predict_type(copy_of_enron4_train_df, copy_of_enron4_test_df, 'enron4', feature_columns)\n",
        "\n",
        "# enron1 - 0.9298245614035088 enron2 - 0.9435146443514645 enron4 - 0.9742173112338858"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsYxS6014bTt"
      },
      "source": [
        "### Bernoulli NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "pcRmfKSyqUmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1611b4e-75d1-4d2e-e1e9-d8109a5c8aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 10021) (456, 10021)\n",
            "(450, 10028) (456, 10028)\n",
            "enron1\n",
            "accuracy - 0.7346491228070176\n",
            "recall - 0.20134228187919462\n",
            "f1 score - 0.3314917127071823\n",
            "precision - 0.9375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.72      0.99      0.83       307\n",
            "        spam       0.94      0.20      0.33       149\n",
            "\n",
            "    accuracy                           0.73       456\n",
            "   macro avg       0.83      0.60      0.58       456\n",
            "weighted avg       0.79      0.73      0.67       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(463, 10375) (478, 10375)\n",
            "(463, 10382) (478, 10382)\n",
            "enron2\n",
            "accuracy - 0.7740585774058577\n",
            "recall - 0.19230769230769232\n",
            "f1 score - 0.31645569620253167\n",
            "precision - 0.8928571428571429\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.77      0.99      0.86       348\n",
            "        spam       0.89      0.19      0.32       130\n",
            "\n",
            "    accuracy                           0.77       478\n",
            "   macro avg       0.83      0.59      0.59       478\n",
            "weighted avg       0.80      0.77      0.72       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(535, 17792) (543, 17792)\n",
            "(535, 17799) (543, 17799)\n",
            "enron4\n",
            "accuracy - 0.9189686924493554\n",
            "recall - 1.0\n",
            "f1 score - 0.9467312348668281\n",
            "precision - 0.8988505747126436\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.71      0.83       152\n",
            "        spam       0.90      1.00      0.95       391\n",
            "\n",
            "    accuracy                           0.92       543\n",
            "   macro avg       0.95      0.86      0.89       543\n",
            "weighted avg       0.93      0.92      0.91       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import hstack\n",
        "def predict_type(X_train, X_test, name, feature_columns):\n",
        "    X_train_nums = X_train[feature_columns]\n",
        "    X_test_nums = X_test[feature_columns]\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    X_train_nums = X_train_nums.values\n",
        "    X_test_nums = X_test_nums.values\n",
        "\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_nums = scaler.fit_transform(X_train_nums)\n",
        "    X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "    X_train_combined = hstack([X_train, X_train_nums])\n",
        "    X_test_combined = hstack([X_test, X_test_nums])\n",
        "\n",
        "    print(X_train_combined.shape, X_test_combined.shape)\n",
        "\n",
        "    model = BernoulliNB(alpha=1.0)\n",
        "    model.fit(X_train_combined, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "predict_type(copy_of_enron1_train_df, copy_of_enron1_test_df, 'enron1', feature_columns)\n",
        "predict_type(copy_of_enron2_train_df, copy_of_enron2_test_df, 'enron2', feature_columns)\n",
        "predict_type(copy_of_enron4_train_df, copy_of_enron4_test_df, 'enron4', feature_columns)\n",
        "\n",
        "# enron1 - 0.9298245614035088 enron2 - 0.9435146443514645 enron4 - 0.9742173112338858"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U86ys3iF4xq1"
      },
      "source": [
        "### LR BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "bS2WP2um4m3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe3aa94-348a-4fee-f7eb-ad808477188b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 10021) (456, 10021)\n",
            "(450, 10028) (456, 10028)\n",
            "enron1\n",
            "accuracy - 0.9385964912280702\n",
            "recall - 0.8993288590604027\n",
            "f1 score - 0.9054054054054055\n",
            "precision - 0.9115646258503401\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      0.96      0.95       307\n",
            "        spam       0.91      0.90      0.91       149\n",
            "\n",
            "    accuracy                           0.94       456\n",
            "   macro avg       0.93      0.93      0.93       456\n",
            "weighted avg       0.94      0.94      0.94       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(463, 10375) (478, 10375)\n",
            "(463, 10382) (478, 10382)\n",
            "enron2\n",
            "accuracy - 0.9393305439330544\n",
            "recall - 0.9076923076923077\n",
            "f1 score - 0.8905660377358491\n",
            "precision - 0.8740740740740741\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.95      0.96       348\n",
            "        spam       0.87      0.91      0.89       130\n",
            "\n",
            "    accuracy                           0.94       478\n",
            "   macro avg       0.92      0.93      0.92       478\n",
            "weighted avg       0.94      0.94      0.94       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(535, 17792) (543, 17792)\n",
            "(535, 17799) (543, 17799)\n",
            "enron4\n",
            "accuracy - 0.9650092081031307\n",
            "recall - 1.0\n",
            "f1 score - 0.9762796504369537\n",
            "precision - 0.9536585365853658\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.88      0.93       152\n",
            "        spam       0.95      1.00      0.98       391\n",
            "\n",
            "    accuracy                           0.97       543\n",
            "   macro avg       0.98      0.94      0.95       543\n",
            "weighted avg       0.97      0.97      0.96       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    X_train_nums = X_train[feature_columns]\n",
        "    X_test_nums = X_test[feature_columns]\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    X_train_nums = X_train_nums.values\n",
        "    X_test_nums = X_test_nums.values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_nums = scaler.fit_transform(X_train_nums)\n",
        "    X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "    X_train_combined = hstack([X_train, X_train_nums])\n",
        "    X_test_combined = hstack([X_test, X_test_nums])\n",
        "\n",
        "    print(X_train_combined.shape, X_test_combined.shape)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_combined, y_train, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    lambda_values = {'C': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "    grid_search = GridSearchCV(model, lambda_values, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_C = grid_search.best_params_['C']\n",
        "    new_model = LogisticRegression(C=best_C)\n",
        "    new_model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_pred = new_model.predict(X_test_combined)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "predict_type(copy_of_enron1_train_df, copy_of_enron1_test_df, 'enron1')\n",
        "predict_type(copy_of_enron2_train_df, copy_of_enron2_test_df, 'enron2')\n",
        "predict_type(copy_of_enron4_train_df, copy_of_enron4_test_df, 'enron4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2TGdYq_6Cf0"
      },
      "source": [
        "### LR Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "GpbgiEqo5xXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbe9223-d36f-4cc6-baba-e827bbcf8d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 10021) (456, 10021)\n",
            "(450, 10028) (456, 10028)\n",
            "enron1\n",
            "accuracy - 0.9473684210526315\n",
            "recall - 0.9328859060402684\n",
            "f1 score - 0.9205298013245033\n",
            "precision - 0.9084967320261438\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.95      0.96       307\n",
            "        spam       0.91      0.93      0.92       149\n",
            "\n",
            "    accuracy                           0.95       456\n",
            "   macro avg       0.94      0.94      0.94       456\n",
            "weighted avg       0.95      0.95      0.95       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(463, 10375) (478, 10375)\n",
            "(463, 10382) (478, 10382)\n",
            "enron2\n",
            "accuracy - 0.9518828451882845\n",
            "recall - 0.9153846153846154\n",
            "f1 score - 0.9118773946360154\n",
            "precision - 0.9083969465648855\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.97      0.97       348\n",
            "        spam       0.91      0.92      0.91       130\n",
            "\n",
            "    accuracy                           0.95       478\n",
            "   macro avg       0.94      0.94      0.94       478\n",
            "weighted avg       0.95      0.95      0.95       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(535, 17792) (543, 17792)\n",
            "(535, 17799) (543, 17799)\n",
            "enron4\n",
            "accuracy - 0.9558011049723757\n",
            "recall - 1.0\n",
            "f1 score - 0.9702233250620347\n",
            "precision - 0.9421686746987952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.84      0.91       152\n",
            "        spam       0.94      1.00      0.97       391\n",
            "\n",
            "    accuracy                           0.96       543\n",
            "   macro avg       0.97      0.92      0.94       543\n",
            "weighted avg       0.96      0.96      0.95       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def predict_type(X_train, X_test, name):\n",
        "    X_train_nums = X_train[feature_columns]\n",
        "    X_test_nums = X_test[feature_columns]\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    X_train_nums = X_train_nums.values\n",
        "    X_test_nums = X_test_nums.values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_nums = scaler.fit_transform(X_train_nums)\n",
        "    X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "    X_train_combined = hstack([X_train, X_train_nums])\n",
        "    X_test_combined = hstack([X_test, X_test_nums])\n",
        "\n",
        "    print(X_train_combined.shape, X_test_combined.shape)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_combined, y_train, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    lambda_values = {'C': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "    grid_search = GridSearchCV(model, lambda_values, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_C = grid_search.best_params_['C']\n",
        "    new_model = LogisticRegression(C=best_C)\n",
        "    new_model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_pred = new_model.predict(X_test_combined)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "predict_type(copy_of_enron1_train_df, copy_of_enron1_test_df, 'enron1')\n",
        "predict_type(copy_of_enron2_train_df, copy_of_enron2_test_df, 'enron2')\n",
        "predict_type(copy_of_enron4_train_df, copy_of_enron4_test_df, 'enron4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1w-vBdg7qJZ"
      },
      "source": [
        "### SGDClassifier BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "oteIjTbi6FcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57743922-9e8d-4d6c-ad54-a5c3648ed6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 10021) (456, 10021)\n",
            "(450, 10028) (456, 10028)\n",
            "enron1\n",
            "accuracy - 0.9298245614035088\n",
            "recall - 0.8859060402684564\n",
            "f1 score - 0.8918918918918919\n",
            "precision - 0.8979591836734694\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.94      0.95      0.95       307\n",
            "        spam       0.90      0.89      0.89       149\n",
            "\n",
            "    accuracy                           0.93       456\n",
            "   macro avg       0.92      0.92      0.92       456\n",
            "weighted avg       0.93      0.93      0.93       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(463, 10375) (478, 10375)\n",
            "(463, 10382) (478, 10382)\n",
            "enron2\n",
            "accuracy - 0.9288702928870293\n",
            "recall - 0.8461538461538461\n",
            "f1 score - 0.8661417322834646\n",
            "precision - 0.8870967741935484\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.94      0.96      0.95       348\n",
            "        spam       0.89      0.85      0.87       130\n",
            "\n",
            "    accuracy                           0.93       478\n",
            "   macro avg       0.92      0.90      0.91       478\n",
            "weighted avg       0.93      0.93      0.93       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(535, 17792) (543, 17792)\n",
            "(535, 17799) (543, 17799)\n",
            "enron4\n",
            "accuracy - 0.9668508287292817\n",
            "recall - 0.9769820971867008\n",
            "f1 score - 0.9769820971867008\n",
            "precision - 0.9769820971867008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.94      0.94      0.94       152\n",
            "        spam       0.98      0.98      0.98       391\n",
            "\n",
            "    accuracy                           0.97       543\n",
            "   macro avg       0.96      0.96      0.96       543\n",
            "weighted avg       0.97      0.97      0.97       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "enron1_best_params = {'alpha': 0.001, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': None, 'validation_fraction': 0.2, 'warm_start': False}\n",
        "enron2_best_params = {'alpha': 0.01, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 800, 'penalty': 'l2', 'validation_fraction': 0.3, 'warm_start': True}\n",
        "enron4_best_params = {'alpha': 0.0001, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': None, 'validation_fraction': 0.3, 'warm_start': True}\n",
        "\n",
        "def predict_type(X_train, X_test, name, best_params):\n",
        "    X_train_nums = X_train[feature_columns]\n",
        "    X_test_nums = X_test[feature_columns]\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    X_train_nums = X_train_nums.values\n",
        "    X_test_nums = X_test_nums.values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_nums = scaler.fit_transform(X_train_nums)\n",
        "    X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "    X_train_combined = hstack([X_train, X_train_nums])\n",
        "    X_test_combined = hstack([X_test, X_test_nums])\n",
        "\n",
        "    print(X_train_combined.shape, X_test_combined.shape)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_combined, y_train, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = SGDClassifier(**best_params)\n",
        "\n",
        "    model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "predict_type(copy_of_enron1_train_df, copy_of_enron1_test_df, 'enron1', enron1_best_params)\n",
        "predict_type(copy_of_enron2_train_df, copy_of_enron2_test_df, 'enron2', enron2_best_params)\n",
        "predict_type(copy_of_enron4_train_df, copy_of_enron4_test_df, 'enron4', enron4_best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53gOR_1U9wnn"
      },
      "source": [
        "### SGDClassifier Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "nM2tFiO78P2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc81f157-67ff-4193-a185-263ba6ad98a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(450, 10021) (456, 10021)\n",
            "(450, 10028) (456, 10028)\n",
            "enron1\n",
            "accuracy - 0.9298245614035088\n",
            "recall - 0.912751677852349\n",
            "f1 score - 0.8947368421052632\n",
            "precision - 0.8774193548387097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      0.94      0.95       307\n",
            "        spam       0.88      0.91      0.89       149\n",
            "\n",
            "    accuracy                           0.93       456\n",
            "   macro avg       0.92      0.93      0.92       456\n",
            "weighted avg       0.93      0.93      0.93       456\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(463, 10375) (478, 10375)\n",
            "(463, 10382) (478, 10382)\n",
            "enron2\n",
            "accuracy - 0.9623430962343096\n",
            "recall - 0.9461538461538461\n",
            "f1 score - 0.9318181818181818\n",
            "precision - 0.917910447761194\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.97      0.97       348\n",
            "        spam       0.92      0.95      0.93       130\n",
            "\n",
            "    accuracy                           0.96       478\n",
            "   macro avg       0.95      0.96      0.95       478\n",
            "weighted avg       0.96      0.96      0.96       478\n",
            "\n",
            "----------------------------------------------------------------------------------\n",
            "(535, 17792) (543, 17792)\n",
            "(535, 17799) (543, 17799)\n",
            "enron4\n",
            "accuracy - 0.9631675874769797\n",
            "recall - 0.989769820971867\n",
            "f1 score - 0.9748110831234257\n",
            "precision - 0.9602977667493796\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      0.89      0.93       152\n",
            "        spam       0.96      0.99      0.97       391\n",
            "\n",
            "    accuracy                           0.96       543\n",
            "   macro avg       0.97      0.94      0.95       543\n",
            "weighted avg       0.96      0.96      0.96       543\n",
            "\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "enron1_best_params = {'alpha': 0.001, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'max_iter': 1000, 'penalty': None, 'validation_fraction': 0.2, 'warm_start': False}\n",
        "enron2_best_params = {'alpha': 0.01, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 800, 'penalty': 'l2', 'validation_fraction': 0.3, 'warm_start': True}\n",
        "enron4_best_params = {'alpha': 0.0001, 'early_stopping': False, 'learning_rate': 'optimal', 'loss': 'perceptron', 'max_iter': 1000, 'penalty': None, 'validation_fraction': 0.3, 'warm_start': True}\n",
        "\n",
        "def predict_type(X_train, X_test, name, best_params):\n",
        "    X_train_nums = X_train[feature_columns]\n",
        "    X_test_nums = X_test[feature_columns]\n",
        "    y_train = X_train['Type']\n",
        "    y_test = X_test['Type']\n",
        "    # X_train['Email_str'] = X_train['Email'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train['Email_str'])\n",
        "    X_test = vectorizer.transform(X_test['Email_str'])\n",
        "\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    X_train_nums = X_train_nums.values\n",
        "    X_test_nums = X_test_nums.values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_nums = scaler.fit_transform(X_train_nums)\n",
        "    X_test_nums = scaler.transform(X_test_nums)\n",
        "\n",
        "    X_train_combined = hstack([X_train, X_train_nums])\n",
        "    X_test_combined = hstack([X_test, X_test_nums])\n",
        "\n",
        "    print(X_train_combined.shape, X_test_combined.shape)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_combined, y_train, random_state=42, test_size=0.3)\n",
        "\n",
        "    model = SGDClassifier(**best_params)\n",
        "\n",
        "    model.fit(np.vstack([X_train.toarray(), X_val.toarray()]), np.hstack([y_train, y_val]))\n",
        "\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "\n",
        "    print(name)\n",
        "    print(f\"accuracy - {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"recall - {recall_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"f1 score - {f1_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(f\"precision - {precision_score(y_test, y_pred, pos_label='spam')}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "predict_type(copy_of_enron1_train_df, copy_of_enron1_test_df, 'enron1', enron1_best_params)\n",
        "predict_type(copy_of_enron2_train_df, copy_of_enron2_test_df, 'enron2', enron2_best_params)\n",
        "predict_type(copy_of_enron4_train_df, copy_of_enron4_test_df, 'enron4', enron4_best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "WDnV6pW790s0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}